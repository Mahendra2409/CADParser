{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOc_YwOFIPDh",
        "outputId": "c86e6609-96ff-464d-d1de-49baa8ae5f81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels/cu121/repo.html\n",
            "Requirement already satisfied: dgl in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.66.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: torchdata>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (0.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2024.2.2)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata>=0.5.0->dgl) (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata>=0.5.0->dgl) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->torchdata>=0.5.0->dgl) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->torchdata>=0.5.0->dgl) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install dgl -f https://data.dgl.ai/wheels/cu121/repo.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from scipy.spatial.transform import Rotation\n",
        "\n",
        "\n",
        "def bounding_box_uvgrid(inp: torch.Tensor):\n",
        "    pts = inp[..., :3].reshape((-1, 3))\n",
        "    mask = inp[..., 6].reshape(-1)\n",
        "    point_indices_inside_faces = mask == 1\n",
        "    pts = pts[point_indices_inside_faces, :]\n",
        "    return bounding_box_pointcloud(pts)\n",
        "\n",
        "\n",
        "def bounding_box_pointcloud(pts: torch.Tensor):\n",
        "    x = pts[:, 0]\n",
        "    y = pts[:, 1]\n",
        "    z = pts[:, 2]\n",
        "    box = [[x.min(), y.min(), z.min()], [x.max(), y.max(), z.max()]]\n",
        "    return torch.tensor(box)\n",
        "\n",
        "\n",
        "def center_and_scale_uvgrid(inp: torch.Tensor, return_center_scale=False):\n",
        "    bbox = bounding_box_uvgrid(inp)\n",
        "    diag = bbox[1] - bbox[0]\n",
        "    scale = 2.0 / max(diag[0], diag[1], diag[2])\n",
        "    center = 0.5 * (bbox[0] + bbox[1])\n",
        "    inp[..., :3] -= center\n",
        "    inp[..., :3] *= scale\n",
        "    if return_center_scale:\n",
        "        return inp, center, scale\n",
        "    return inp\n",
        "\n",
        "\n",
        "def get_random_rotation():\n",
        "    \"\"\"Get a random rotation in 90 degree increments along the canonical axes\"\"\"\n",
        "    axes = [\n",
        "        np.array([1, 0, 0]),\n",
        "        np.array([0, 1, 0]),\n",
        "        np.array([0, 0, 1]),\n",
        "    ]\n",
        "    angles = [0.0, 90.0, 180.0, 270.0]\n",
        "    axis = random.choice(axes)\n",
        "    angle_radians = np.radians(random.choice(angles))\n",
        "    return Rotation.from_rotvec(angle_radians * axis)\n",
        "\n",
        "\n",
        "def rotate_uvgrid(inp, rotation):\n",
        "    \"\"\"Rotate the node features in the graph by a given rotation\"\"\"\n",
        "    Rmat = torch.tensor(rotation.as_matrix()).float()\n",
        "    orig_size = inp[..., :3].size()\n",
        "    inp[..., :3] = torch.mm(inp[..., :3].view(-1, 3), Rmat).view(\n",
        "        orig_size\n",
        "    )  # Points\n",
        "    inp[..., 3:6] = torch.mm(inp[..., 3:6].view(-1, 3), Rmat).view(\n",
        "        orig_size\n",
        "    )  # Normals/tangents\n",
        "    return inp\n",
        "\n",
        "\n",
        "INVALID_FONTS = [\n",
        "    \"Bokor\",\n",
        "    \"Lao Muang Khong\",\n",
        "    \"Lao Sans Pro\",\n",
        "    \"MS Outlook\",\n",
        "    \"Catamaran Black\",\n",
        "    \"Dubai\",\n",
        "    \"HoloLens MDL2 Assets\",\n",
        "    \"Lao Muang Don\",\n",
        "    \"Oxanium Medium\",\n",
        "    \"Rounded Mplus 1c\",\n",
        "    \"Moul Pali\",\n",
        "    \"Noto Sans Tamil\",\n",
        "    \"Webdings\",\n",
        "    \"Armata\",\n",
        "    \"Koulen\",\n",
        "    \"Yinmar\",\n",
        "    \"Ponnala\",\n",
        "    \"Noto Sans Tamil\",\n",
        "    \"Chenla\",\n",
        "    \"Lohit Devanagari\",\n",
        "    \"Metal\",\n",
        "    \"MS Office Symbol\",\n",
        "    \"Cormorant Garamond Medium\",\n",
        "    \"Chiller\",\n",
        "    \"Give You Glory\",\n",
        "    \"Hind Vadodara Light\",\n",
        "    \"Libre Barcode 39 Extended\",\n",
        "    \"Myanmar Sans Pro\",\n",
        "    \"Scheherazade\",\n",
        "    \"Segoe MDL2 Assets\",\n",
        "    \"Siemreap\",\n",
        "    \"Signika SemiBold\" \"Taprom\",\n",
        "    \"Times New Roman TUR\",\n",
        "    \"Playfair Display SC Black\",\n",
        "    \"Poppins Thin\",\n",
        "    \"Raleway Dots\",\n",
        "    \"Raleway Thin\",\n",
        "    \"Segoe MDL2 Assets\",\n",
        "    \"Segoe MDL2 Assets\",\n",
        "    \"Spectral SC ExtraLight\",\n",
        "    \"Txt\",\n",
        "    \"Uchen\",\n",
        "    \"Yinmar\",\n",
        "    \"Almarai ExtraBold\",\n",
        "    \"Fasthand\",\n",
        "    \"Exo\",\n",
        "    \"Freckle Face\",\n",
        "    \"Montserrat Light\",\n",
        "    \"Inter\",\n",
        "    \"MS Reference Specialty\",\n",
        "    \"MS Outlook\",\n",
        "    \"Preah Vihear\",\n",
        "    \"Sitara\",\n",
        "    \"Barkerville Old Face\",\n",
        "    \"Bodoni MT\" \"Bokor\",\n",
        "    \"Fasthand\",\n",
        "    \"HoloLens MDL2 Assests\",\n",
        "    \"Libre Barcode 39\",\n",
        "    \"Lohit Tamil\",\n",
        "    \"Marlett\",\n",
        "    \"MS outlook\",\n",
        "    \"MS office Symbol Semilight\",\n",
        "    \"MS office symbol regular\",\n",
        "    \"Ms office symbol extralight\",\n",
        "    \"Ms Reference speciality\",\n",
        "    \"Segoe MDL2 Assets\",\n",
        "    \"Siemreap\",\n",
        "    \"Sitara\",\n",
        "    \"Symbol\",\n",
        "    \"Wingdings\",\n",
        "    \"Metal\",\n",
        "    \"Ponnala\",\n",
        "    \"Webdings\",\n",
        "    \"Souliyo Unicode\",\n",
        "    \"Aguafina Script\",\n",
        "    \"Yantramanav Black\",\n",
        "    # \"Yaldevi\",\n",
        "    # Taprom,\n",
        "    # \"Zhi Mang Xing\",\n",
        "    # \"Taviraj\",\n",
        "    # \"SeoulNamsan EB\",\n",
        "]\n",
        "\n",
        "\n",
        "def valid_font(filename):\n",
        "    for name in INVALID_FONTS:\n",
        "        if name.lower() in str(filename).lower():\n",
        "            return False\n",
        "    return True"
      ],
      "metadata": {
        "id": "c6v5Xp_mIs4G"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import FloatTensor\n",
        "import dgl\n",
        "from dgl.data.utils import load_graphs\n",
        "from tqdm import tqdm\n",
        "from abc import abstractmethod\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import dgl\n",
        "from torch import FloatTensor, stack\n",
        "\n",
        "class BaseDataset(Dataset):\n",
        "    @staticmethod\n",
        "    def num_classes():\n",
        "        pass\n",
        "\n",
        "    def __init__(self, X_train, Y_train):\n",
        "        \"\"\"\n",
        "        self.data is a list of dictionaries with keys graph and label\n",
        "        \"\"\"\n",
        "        assert len(X_train) == len(Y_train), \"The number of graphs must match the number of labels\"\n",
        "        self.data = [{\"graph\": graph, \"label\": label} for graph, label in zip(X_train, Y_train)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        return sample[\"graph\"], sample[\"label\"]  # Returns a tuple of the sample graph and its corresponding label\n",
        "\n",
        "    def _collate(self, batch):\n",
        "        graphs, labels = zip(*batch)\n",
        "        batched_graph = dgl.batch(graphs)\n",
        "        #print(\"batched_graph\", batched_graph)\n",
        "        num_nodes_per_graph = [graph.number_of_nodes() for graph in graphs]\n",
        "\n",
        "        # Create a default padding command vector\n",
        "        pad_vector = torch.tensor([6] + [-1]*16, dtype=torch.float32)\n",
        "\n",
        "        # Prepare labels with padding\n",
        "        padded_labels = []\n",
        "        for label in labels:\n",
        "            label_length = label.shape[0]\n",
        "            if label_length < 60:\n",
        "                # Calculate how many padding vectors are needed\n",
        "                padding_count = 60 - label_length\n",
        "                # Create a tensor of padding vectors\n",
        "                padding = pad_vector.repeat(padding_count, 1)\n",
        "                # Concatenate the original label with the padding\n",
        "                padded_label = torch.cat([torch.tensor(label, dtype=torch.float32), padding], dim=0)\n",
        "            else:\n",
        "                padded_label = torch.tensor(label, dtype=torch.float32)\n",
        "            padded_labels.append(padded_label)\n",
        "\n",
        "        # Stack all the padded labels into a single tensor\n",
        "        padded_labels = stack(padded_labels)\n",
        "        return {\"graph\": batched_graph, \"labels\": padded_labels, \"num_nodes\": num_nodes_per_graph}\n",
        "        # return {\"graph\": batched_graph, \"labels\": padded_labels}\n",
        "\n",
        "\n",
        "    def get_dataloader(self, batch_size, shuffle=True, num_workers=0):\n",
        "        return DataLoader(\n",
        "            self,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle,\n",
        "            collate_fn=self._collate,\n",
        "            num_workers=num_workers,  # Can be set to non-zero on Linux\n",
        "            drop_last=True\n",
        "        )"
      ],
      "metadata": {
        "id": "vrB6d1ioIVZw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from dgl.nn.pytorch.conv import NNConv\n",
        "from dgl.nn.pytorch.glob import MaxPooling\n",
        "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
        "import math\n",
        "\n",
        "\n",
        "# Convolutional Layers\n",
        "def _conv1d(in_channels, out_channels, kernel_size=3, padding=0, bias=False):\n",
        "    \"\"\"\n",
        "    Helper function to create a 1D convolutional layer with batchnorm and LeakyReLU activation\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Input channels\n",
        "        out_channels (int): Output channels\n",
        "        kernel_size (int, optional): Size of the convolutional kernel. Defaults to 3.\n",
        "        padding (int, optional): Padding size on each side. Defaults to 0.\n",
        "        bias (bool, optional): Whether bias is used. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        nn.Sequential: Sequential contained the Conv1d, BatchNorm1d and LeakyReLU layers\n",
        "    \"\"\"\n",
        "    return nn.Sequential(\n",
        "        nn.Conv1d(\n",
        "            in_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=bias\n",
        "        ),\n",
        "        nn.BatchNorm1d(out_channels),\n",
        "        nn.LeakyReLU(),\n",
        "    )\n",
        "\n",
        "\n",
        "def _conv2d(in_channels, out_channels, kernel_size, padding=0, bias=False):\n",
        "    \"\"\"\n",
        "    Helper function to create a 2D convolutional layer with batchnorm and LeakyReLU activation\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Input channels\n",
        "        out_channels (int): Output channels\n",
        "        kernel_size (int, optional): Size of the convolutional kernel. Defaults to 3.\n",
        "        padding (int, optional): Padding size on each side. Defaults to 0.\n",
        "        bias (bool, optional): Whether bias is used. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        nn.Sequential: Sequential contained the Conv2d, BatchNorm2d and LeakyReLU layers\n",
        "    \"\"\"\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=padding,\n",
        "            bias=bias,\n",
        "        ),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.LeakyReLU(),\n",
        "    )\n",
        "\n",
        "\n",
        "def _fc(in_features, out_features, bias=False):\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(in_features, out_features, bias=bias),\n",
        "        nn.BatchNorm1d(out_features),\n",
        "        nn.LeakyReLU(),\n",
        "    )\n",
        "\n",
        "class _MLP(nn.Module):\n",
        "    \"\"\"\"\"\"\n",
        "\n",
        "    def __init__(self, num_layers, input_dim, hidden_dim, output_dim):\n",
        "        \"\"\"\n",
        "        MLP with linear output\n",
        "        Args:\n",
        "            num_layers (int): The number of linear layers in the MLP\n",
        "            input_dim (int): Input feature dimension\n",
        "            hidden_dim (int): Hidden feature dimensions for all hidden layers\n",
        "            output_dim (int): Output feature dimension\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the given number of layers is <1\n",
        "        \"\"\"\n",
        "        super(_MLP, self).__init__()\n",
        "        self.linear_or_not = True  # default is linear model\n",
        "        self.num_layers = num_layers\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        if num_layers < 1:\n",
        "            raise ValueError(\"Number of layers should be positive!\")\n",
        "        elif num_layers == 1:\n",
        "            # Linear model\n",
        "            self.linear = nn.Linear(input_dim, output_dim)\n",
        "        else:\n",
        "            # Multi-layer model\n",
        "            self.linear_or_not = False\n",
        "            self.linears = torch.nn.ModuleList()\n",
        "            self.batch_norms = torch.nn.ModuleList()\n",
        "\n",
        "            self.linears.append(nn.Linear(input_dim, hidden_dim))\n",
        "            for layer in range(num_layers - 2):\n",
        "                self.linears.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "            self.linears.append(nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "            # TODO: this could move inside the above loop\n",
        "            for layer in range(num_layers - 1):\n",
        "                self.batch_norms.append(nn.BatchNorm1d((hidden_dim)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.linear_or_not:\n",
        "            # If linear model\n",
        "            return self.linear(x)\n",
        "        else:\n",
        "            # If MLP\n",
        "            h = x\n",
        "            for i in range(self.num_layers - 1):\n",
        "                h = F.relu(self.batch_norms[i](self.linears[i](h)))\n",
        "            return self.linears[-1](h)\n",
        "\n",
        "class UVNetCurveEncoder(nn.Module):\n",
        "    def __init__(self, in_channels=6, output_dims=64):\n",
        "        \"\"\"\n",
        "        This is the 1D convolutional network that extracts features from the B-rep edge\n",
        "        geometry described as 1D UV-grids (see Section 3.2, Curve & surface convolution\n",
        "        in paper)\n",
        "\n",
        "        Args:\n",
        "            in_channels (int, optional): Number of channels in the edge UV-grids. By default\n",
        "                                         we expect 3 channels for point coordinates and 3 for\n",
        "                                         curve tangents. Defaults to 6.\n",
        "            output_dims (int, optional): Output curve embedding dimension. Defaults to 64.\n",
        "        \"\"\"\n",
        "        super(UVNetCurveEncoder, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.conv1 = _conv1d(in_channels, 64, kernel_size=3, padding=1, bias=False)\n",
        "        self.conv2 = _conv1d(64, 128, kernel_size=3, padding=1, bias=False)\n",
        "        self.conv3 = _conv1d(128, 256, kernel_size=3, padding=1, bias=False)\n",
        "        self.final_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = _fc(256, output_dims, bias=False)\n",
        "\n",
        "        for m in self.modules():\n",
        "            self.weights_init(m)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, (nn.Linear, nn.Conv1d)):\n",
        "            torch.nn.init.kaiming_uniform_(m.weight.data)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.size(1) == self.in_channels\n",
        "        batch_size = x.size(0)\n",
        "        x = x.float()\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.final_pool(x)\n",
        "        x = x.view(batch_size, -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UVNetSurfaceEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=7,\n",
        "        output_dims=64,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        This is the 2D convolutional network that extracts features from the B-rep face\n",
        "        geometry described as 2D UV-grids (see Section 3.2, Curve & surface convolution\n",
        "        in paper)\n",
        "\n",
        "        Args:\n",
        "            in_channels (int, optional): Number of channels in the edge UV-grids. By default\n",
        "                                         we expect 3 channels for point coordinates and 3 for\n",
        "                                         surface normals and 1 for the trimming mask. Defaults\n",
        "                                         to 7.\n",
        "            output_dims (int, optional): Output surface embedding dimension. Defaults to 64.\n",
        "        \"\"\"\n",
        "        super(UVNetSurfaceEncoder, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.conv1 = _conv2d(in_channels, 64, 3, padding=1, bias=False)\n",
        "        self.conv2 = _conv2d(64, 128, 3, padding=1, bias=False)\n",
        "        self.conv3 = _conv2d(128, 256, 3, padding=1, bias=False)\n",
        "        self.final_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = _fc(256, output_dims, bias=False)\n",
        "        for m in self.modules():\n",
        "            self.weights_init(m)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
        "            torch.nn.init.kaiming_uniform_(m.weight.data)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.size(1) == self.in_channels\n",
        "        batch_size = x.size(0)\n",
        "        x = x.float()\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.final_pool(x)\n",
        "        x = x.view(batch_size, -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class _EdgeConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        edge_feats,\n",
        "        out_feats,\n",
        "        node_feats,\n",
        "        num_mlp_layers=2,\n",
        "        hidden_mlp_dim=64,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        This module implements Eq. 2 from the paper where the edge features are\n",
        "        updated using the node features at the endpoints.\n",
        "\n",
        "        Args:\n",
        "            edge_feats (int): Input edge feature dimension\n",
        "            out_feats (int): Output feature deimension\n",
        "            node_feats (int): Input node feature dimension\n",
        "            num_mlp_layers (int, optional): Number of layers used in the MLP. Defaults to 2.\n",
        "            hidden_mlp_dim (int, optional): Hidden feature dimension in the MLP. Defaults to 64.\n",
        "        \"\"\"\n",
        "        super(_EdgeConv, self).__init__()\n",
        "        self.proj = _MLP(1, node_feats, hidden_mlp_dim, edge_feats)\n",
        "        self.mlp = _MLP(num_mlp_layers, edge_feats, hidden_mlp_dim, out_feats)\n",
        "        self.batchnorm = nn.BatchNorm1d(out_feats)\n",
        "        self.eps = torch.nn.Parameter(torch.FloatTensor([0.0]))\n",
        "\n",
        "    def forward(self, graph, nfeat, efeat):\n",
        "        src, dst = graph.edges()\n",
        "        proj1, proj2 = self.proj(nfeat[src]), self.proj(nfeat[dst])\n",
        "        agg = proj1 + proj2\n",
        "        h = self.mlp((1 + self.eps) * efeat + agg)\n",
        "        h = F.leaky_relu(self.batchnorm(h))\n",
        "        return h\n",
        "\n",
        "\n",
        "class _NodeConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        node_feats,\n",
        "        out_feats,\n",
        "        edge_feats,\n",
        "        num_mlp_layers=2,\n",
        "        hidden_mlp_dim=64,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        This module implements Eq. 1 from the paper where the node features are\n",
        "        updated using the neighboring node and edge features.\n",
        "\n",
        "        Args:\n",
        "            node_feats (int): Input edge feature dimension\n",
        "            out_feats (int): Output feature deimension\n",
        "            node_feats (int): Input node feature dimension\n",
        "            num_mlp_layers (int, optional): Number of layers used in the MLP. Defaults to 2.\n",
        "            hidden_mlp_dim (int, optional): Hidden feature dimension in the MLP. Defaults to 64.\n",
        "        \"\"\"\n",
        "        super(_NodeConv, self).__init__()\n",
        "        self.gconv = NNConv(\n",
        "            in_feats=node_feats,\n",
        "            out_feats=out_feats,\n",
        "            edge_func=nn.Linear(edge_feats, node_feats * out_feats),\n",
        "            aggregator_type=\"sum\",\n",
        "            bias=False,\n",
        "        )\n",
        "        self.batchnorm = nn.BatchNorm1d(out_feats)\n",
        "        self.mlp = _MLP(num_mlp_layers, node_feats, hidden_mlp_dim, out_feats)\n",
        "        self.eps = torch.nn.Parameter(torch.FloatTensor([0.0]))\n",
        "\n",
        "    def forward(self, graph, nfeat, efeat):\n",
        "        h = (1 + self.eps) * nfeat\n",
        "        h = self.gconv(graph, h, efeat)\n",
        "        h = self.mlp(h)\n",
        "        h = F.leaky_relu(self.batchnorm(h))\n",
        "        return h\n",
        "\n",
        "\n",
        "class UVNetGraphEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        input_edge_dim,\n",
        "        output_dim,\n",
        "        hidden_dim=64,\n",
        "        learn_eps=True,\n",
        "        num_layers=3,\n",
        "        num_mlp_layers=2,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        This is the graph neural network used for message-passing features in the\n",
        "        face-adjacency graph.\n",
        "\n",
        "        Args:\n",
        "            input_dim ([type]): [description]\n",
        "            input_edge_dim ([type]): [description]\n",
        "            output_dim ([type]): [description]\n",
        "            hidden_dim (int, optional): [description]. Defaults to 64.\n",
        "            learn_eps (bool, optional): [description]. Defaults to True.\n",
        "            num_layers (int, optional): [description]. Defaults to 3.\n",
        "            num_mlp_layers (int, optional): [description]. Defaults to 2.\n",
        "        \"\"\"\n",
        "        super(UVNetGraphEncoder, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.learn_eps = learn_eps\n",
        "\n",
        "        # List of layers for node and edge feature message passing\n",
        "        self.node_conv_layers = torch.nn.ModuleList()\n",
        "        self.edge_conv_layers = torch.nn.ModuleList()\n",
        "\n",
        "        for layer in range(self.num_layers - 1):\n",
        "            node_feats = input_dim if layer == 0 else hidden_dim\n",
        "            edge_feats = input_edge_dim if layer == 0 else hidden_dim\n",
        "            self.node_conv_layers.append(\n",
        "                _NodeConv(\n",
        "                    node_feats=node_feats,\n",
        "                    out_feats=hidden_dim,\n",
        "                    edge_feats=edge_feats,\n",
        "                    num_mlp_layers=num_mlp_layers,\n",
        "                    hidden_mlp_dim=hidden_dim,\n",
        "                ),\n",
        "            )\n",
        "            self.edge_conv_layers.append(\n",
        "                _EdgeConv(\n",
        "                    edge_feats=edge_feats,\n",
        "                    out_feats=hidden_dim,\n",
        "                    node_feats=node_feats,\n",
        "                    num_mlp_layers=num_mlp_layers,\n",
        "                    hidden_mlp_dim=hidden_dim,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Linear function for graph poolings of output of each layer\n",
        "        # which maps the output of different layers into a prediction score\n",
        "        self.linears_prediction = torch.nn.ModuleList()\n",
        "\n",
        "        for layer in range(num_layers):\n",
        "            if layer == 0:\n",
        "                self.linears_prediction.append(nn.Linear(input_dim, output_dim))\n",
        "            else:\n",
        "                self.linears_prediction.append(nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "        self.drop1 = nn.Dropout(0.3)\n",
        "        self.drop = nn.Dropout(0.5)\n",
        "        self.pool = MaxPooling()\n",
        "\n",
        "    def forward(self, g, h, efeat):\n",
        "        hidden_rep = [h]\n",
        "        he = efeat\n",
        "\n",
        "        for i in range(self.num_layers - 1):\n",
        "            # Update node features\n",
        "            h = self.node_conv_layers[i](g, h, he)\n",
        "            # Update edge features\n",
        "            he = self.edge_conv_layers[i](g, h, he)\n",
        "            hidden_rep.append(h)\n",
        "        # print(f'hidden_rep is {type(hidden_rep)}')\n",
        "        # Use the node embeddings from the last layer\n",
        "        node_embeddings = hidden_rep[-1]\n",
        "        # print(f'node_embeddings_0 = {node_embeddings.shape}')\n",
        "        node_embeddings = self.drop1(node_embeddings)\n",
        "        # print(f'node_embeddings_1 = {node_embeddings.shape}')\n",
        "\n",
        "        # Optional: Perform pooling to get a graph-level representation\n",
        "        graph_representation = 0\n",
        "        for i, h in enumerate(hidden_rep):\n",
        "            pooled_h = self.pool(g, h)\n",
        "            graph_representation += self.drop(self.linears_prediction[i](pooled_h))\n",
        "\n",
        "        # print(f'node_embeddings_2.shape = {node_embeddings.shape}')\n",
        "        # print(f'graph_representation.shape = {graph_representation.shape}')\n",
        "\n",
        "        return node_embeddings, graph_representation\n",
        "\n",
        "class CADEncoder(nn.Module):\n",
        "  def __init__(self, crv_emb_dim=64, srf_emb_dim=64, graph_emb_dim=128, dropout=0.3):\n",
        "    super(CADEncoder, self).__init__()\n",
        "    self.curv_encoder = UVNetCurveEncoder(in_channels=10, output_dims=crv_emb_dim) # in_channels originally 6\n",
        "    self.surf_encoder = UVNetSurfaceEncoder(in_channels=10, output_dims=srf_emb_dim)\n",
        "    self.graph_encoder = UVNetGraphEncoder(srf_emb_dim, crv_emb_dim, graph_emb_dim)\n",
        "\n",
        "  def forward(self, batched_graph):\n",
        "    input_crv_feat = batched_graph.edata[\"x\"]\n",
        "    input_srf_feat = batched_graph.ndata[\"x\"]\n",
        "    hidden_crv_feat = self.curv_encoder(input_crv_feat)\n",
        "    hidden_srf_feat = self.surf_encoder(input_srf_feat)\n",
        "    node_emb, graph_emb = self.graph_encoder(batched_graph, hidden_srf_feat, hidden_crv_feat)\n",
        "    return node_emb, graph_emb\n",
        "\n",
        "\n",
        "class PositionalEncodingLUT(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=250):\n",
        "        super(PositionalEncodingLUT, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(0, max_len, dtype=torch.long).unsqueeze(1)\n",
        "        self.register_buffer('position', position)\n",
        "\n",
        "        self.pos_embed = nn.Embedding(max_len, d_model)\n",
        "\n",
        "        self._init_embeddings()\n",
        "\n",
        "    def _init_embeddings(self):\n",
        "        nn.init.kaiming_normal_(self.pos_embed.weight, mode=\"fan_in\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, _ = x.shape\n",
        "\n",
        "        # Get positional encodings for the sequence length\n",
        "        pos = self.position[:seq_length]  # This will have shape [seq_length, 1]\n",
        "\n",
        "        # Expand positional encodings to cover the whole batch\n",
        "        pos = pos.expand(-1, batch_size).contiguous()  # Reshape to [seq_length, batch_size]\n",
        "        pos = pos.transpose(0, 1)  # Transpose to [batch_size, seq_length]\n",
        "\n",
        "        # Retrieve positional embeddings\n",
        "        pos_embeddings = self.pos_embed(pos)  # This should now be [batch_size, seq_length, d_model]\n",
        "\n",
        "        # Element-wise addition of embeddings to input x\n",
        "        x = x + pos_embeddings\n",
        "\n",
        "        # Apply dropout and return\n",
        "        return self.dropout(x)\n",
        "\n",
        "class CADEmbedding(nn.Module):\n",
        "    \"\"\"Embedding: positional embed + command embed + parameter embed + group embed (optional)\"\"\"\n",
        "    def __init__(self, n_commands=7, d_model=64, n_args=16, args_dim=257, seq_len=60):\n",
        "        super(CADEmbedding, self).__init__()\n",
        "        self.command_embed = nn.Embedding(n_commands, d_model)\n",
        "        self.arg_embed = nn.Embedding(args_dim, d_model, padding_idx=0)\n",
        "        self.embed_fcn = nn.Linear(d_model * n_args, d_model)\n",
        "        self.pos_encoding = PositionalEncodingLUT(d_model, max_len=seq_len+2)\n",
        "\n",
        "    def forward(self, commands, args):\n",
        "        S, N = commands.shape\n",
        "        src = self.command_embed(commands.long()) + \\\n",
        "              self.embed_fcn(self.arg_embed((args + 1).long()).view(S, N, -1))\n",
        "        src = self.pos_encoding(src)\n",
        "        return src\n",
        "\n",
        "class FusionModule(nn.Module):\n",
        "    def __init__(self, latent_size, command_embedding_size=64, hidden_size=128, output_size=64):\n",
        "        super(FusionModule, self).__init__()\n",
        "        self.fc1 = nn.Linear(latent_size + command_embedding_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, latent, command_embedding):\n",
        "        # print(\"latent dimension\", latent.shape)\n",
        "        # print(\"command embedding\", command_embedding.shape)\n",
        "        command_embedding = command_embedding.unsqueeze(1)\n",
        "        # print('command embedding 1', command_embedding.shape)\n",
        "        combined = torch.cat((latent, command_embedding), dim=2)\n",
        "        # print('combined shape', combined.shape)\n",
        "        x = self.fc1(combined)\n",
        "        # print('x1 shape', x.shape)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        # print('x2 shape', x.shape)\n",
        "        x = F.relu(x)\n",
        "        # print('fusion output', x.shape)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "from torch import Tensor\n",
        "from typing import Optional\n",
        "\n",
        "# class CausalTransformerDecoder(nn.TransformerDecoder):\n",
        "#     def forward(\n",
        "#         self,\n",
        "#         tgt: Tensor,\n",
        "#         memory: Optional[Tensor] = None,\n",
        "#         cache: Optional[Tensor] = None,\n",
        "#         memory_mask: Optional[Tensor] = None,\n",
        "#         tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "#         memory_key_padding_mask: Optional[Tensor] = None,\n",
        "#     ) -> Tensor:\n",
        "#         print(\"tgt in TransformerDecoder \",tgt.shape)\n",
        "#         print(\"memory in TransformerDecoder \",memory.shape)\n",
        "\n",
        "#         if self.training:\n",
        "#             if cache is not None:\n",
        "#                 raise ValueError(\"cache parameter should be None in training mode\")\n",
        "#             for mod in self.layers:\n",
        "#                 tgt = mod(\n",
        "#                     tgt,\n",
        "#                     memory,\n",
        "#                     tgt_mask=None,\n",
        "#                     memory_mask=memory_mask,\n",
        "#                     tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "#                     memory_key_padding_mask=memory_key_padding_mask,\n",
        "#                 )\n",
        "#             return tgt\n",
        "\n",
        "#         new_token_cache = []\n",
        "\n",
        "#         for i, mod in enumerate(self.layers):\n",
        "#             tgt = mod(tgt, memory)\n",
        "#             new_token_cache.append(tgt)\n",
        "#             if cache is not None:\n",
        "#                 tgt = torch.cat([cache[i], tgt], dim=0)\n",
        "\n",
        "#         if cache is not None:\n",
        "#             new_cache = torch.cat([cache, torch.stack(new_token_cache, dim=0)], dim=1)\n",
        "#         else:\n",
        "#             new_cache = torch.stack(new_token_cache, dim=0)\n",
        "\n",
        "#         # Return only the last token's prediction and the new cache\n",
        "#         print(\"tgt[-1:] from the transformer decoder block \", tgt[-1:])\n",
        "#         print(\"new_cache from the transformer decoder block \", new_cache)\n",
        "#         return tgt[-1:], new_cache\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# class CausalTransformerDecoderLayer(nn.TransformerDecoderLayer):\n",
        "#     def __init__(self, d_model, nhead=8, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
        "#         super().__init__(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, activation=activation)\n",
        "\n",
        "#     def forward(\n",
        "#         self,\n",
        "#         tgt: Tensor,\n",
        "#         memory: Optional[Tensor] = None,\n",
        "#         tgt_mask: Optional[Tensor] = None,\n",
        "#         memory_mask: Optional[Tensor] = None,\n",
        "#         tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "#         memory_key_padding_mask: Optional[Tensor] = None,\n",
        "#     ) -> Tensor:\n",
        "#         if self.training:\n",
        "#             # In training mode, follow the standard procedure including masking\n",
        "#             print(\"tgt\", tgt.shape)\n",
        "#             print(\"memory\", memory.shape)\n",
        "#             returned = super().forward(\n",
        "#                 tgt,\n",
        "#                 memory,\n",
        "#                 tgt_mask=tgt_mask,\n",
        "#                 memory_mask=memory_mask,\n",
        "#                 tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "#                 memory_key_padding_mask=memory_key_padding_mask,\n",
        "#             )\n",
        "#             print(\"returned: \", returned.shape)\n",
        "#             return returned\n",
        "#         else:\n",
        "#             # In evaluation mode, proceed with the autoregressive manner\n",
        "#             tgt_last_tok = tgt[-1:, :, :]  # Handle the last token from the sequence only\n",
        "\n",
        "#             # Perform self-attention on the last token\n",
        "#             tgt_last_tok = self.self_attn(\n",
        "#                 tgt_last_tok,\n",
        "#                 tgt,\n",
        "#                 tgt,\n",
        "#                 attn_mask=None,\n",
        "#                 key_padding_mask=tgt_key_padding_mask,\n",
        "#             )[0] + tgt_last_tok\n",
        "#             tgt_last_tok = self.norm1(tgt_last_tok)\n",
        "\n",
        "#             # Perform cross-attention with the memory (encoder's output)\n",
        "#             if memory is not None:\n",
        "#                 tgt_last_tok = self.multihead_attn(\n",
        "#                     tgt_last_tok,\n",
        "#                     memory,\n",
        "#                     memory,\n",
        "#                     attn_mask=memory_mask,\n",
        "#                     key_padding_mask=memory_key_padding_mask,\n",
        "#                 )[0] + tgt_last_tok\n",
        "#                 tgt_last_tok = self.norm2(tgt_last_tok)\n",
        "\n",
        "#             # Pass through the final feed-forward network\n",
        "#             tgt_last_tok = self.linear2(self.dropout(self.activation(self.linear1(tgt_last_tok)))) + tgt_last_tok\n",
        "#             tgt_last_tok = self.norm3(tgt_last_tok)\n",
        "\n",
        "class CausalTransformerDecoderLayer(nn.TransformerDecoderLayer):\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        if self.training:\n",
        "            return super().forward(tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\n",
        "        else:\n",
        "            tgt_last_tok = tgt[-1:, :, :]\n",
        "            tgt_last_tok = self.self_attn(tgt_last_tok, tgt, tgt)[0] + tgt_last_tok\n",
        "            tgt_last_tok = self.norm1(tgt_last_tok)\n",
        "            if memory is not None:\n",
        "                tgt_last_tok = self.multihead_attn(tgt_last_tok, memory, memory)[0] + tgt_last_tok\n",
        "                tgt_last_tok = self.norm2(tgt_last_tok)\n",
        "            tgt_last_tok = self.linear2(self.dropout(self.activation(self.linear1(tgt_last_tok)))) + tgt_last_tok\n",
        "            tgt_last_tok = self.norm3(tgt_last_tok)\n",
        "            return tgt_last_tok\n",
        "\n",
        "\n",
        "class CausalTransformerDecoder(nn.TransformerDecoder):\n",
        "    def forward(self, tgt, memory=None, cache=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        # print(\"tgt in TransformerDecoder \", tgt.shape)\n",
        "        # print(\"memory in TransformerDecoder \", memory.shape)\n",
        "        if self.training:\n",
        "            if cache is not None:\n",
        "                raise ValueError(\"cache parameter should be None in training mode\")\n",
        "            for mod in self.layers:\n",
        "                tgt = mod(tgt, memory, tgt_mask=None, memory_mask=memory_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
        "            return tgt\n",
        "        else:\n",
        "            new_token_cache = []\n",
        "            for i, mod in enumerate(self.layers):\n",
        "                tgt = mod(tgt, memory)\n",
        "                new_token_cache.append(tgt)\n",
        "                if cache is not None:\n",
        "                    tgt = torch.cat([cache[i], tgt], dim=0)\n",
        "            if cache is not None:\n",
        "                new_cache = torch.cat([cache, torch.stack(new_token_cache, dim=0)], dim=1)\n",
        "            else:\n",
        "                new_cache = torch.stack(new_token_cache, dim=0)\n",
        "            # print(\"tgt[-1:] from the transformer decoder block \", tgt[-1:])\n",
        "            # print(\"new_cache from the transformer decoder block \", new_cache)\n",
        "            return tgt[-1:], new_cache\n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _generate_causal_mask(sz, device):\n",
        "        mask = torch.full((sz, sz), float('-inf'))\n",
        "        mask = torch.triu(mask, diagonal=1)\n",
        "        return mask.to(device)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _generate_causal_mask(sz, device):\n",
        "        \"\"\"\n",
        "        Generates a causal mask to hide future tokens for autoregressive tasks.\n",
        "        \"\"\"\n",
        "        mask = torch.full((sz, sz), float('-inf'))\n",
        "        mask = torch.triu(mask, diagonal=1)\n",
        "        return mask.to(device)\n",
        "\n",
        "\n",
        "# class CADDecoder(nn.Module):\n",
        "#     def __init__(self, d_model=64, nhead=8, num_decoder_layers=4, dim_feedforward=2048, dropout=0.1, activation=\"relu\", num_commands=7, max_seq_len=5000, num_parameters=16, param_cat=257):\n",
        "#         super(CADDecoder, self).__init__()\n",
        "#         self.param_cat = param_cat\n",
        "#         # Embedding\n",
        "#         self.cad_command_embedding = CADEmbedding(d_model=d_model, n_commands=num_commands, n_args=num_parameters, seq_len=max_seq_len)\n",
        "\n",
        "#         # Fusion module\n",
        "#         self.fusion_module = FusionModule(latent_size=128)\n",
        "\n",
        "#         # Decoder\n",
        "#         decoder_layer = CausalTransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, activation=activation)\n",
        "#         self.transformer_decoder = CausalTransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "\n",
        "#         # Final output\n",
        "#         self.output_layer1 = nn.Linear(d_model, num_commands)  # t_i\n",
        "#         self.output_layer2 = nn.Linear(d_model, num_parameters * param_cat)  # p_i\n",
        "\n",
        "#     def forward(self, node_embeddings, graph_embeddings, command_seq, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "#         # Command embedding\n",
        "#         print(\"command seq \", command_seq.shape)\n",
        "#         commands_count = command_seq.shape[1]\n",
        "#         commands = command_seq[:, :, 0]\n",
        "#         args = command_seq[:, :, 1:]\n",
        "#         construct_embed = self.cad_command_embedding(commands, args)\n",
        "\n",
        "#         # Fusion module (Modified the Fusion Module)\n",
        "#         fusion_outputs = None\n",
        "#         fusion_outputs = []\n",
        "#         for i in range(commands_count):\n",
        "#             fusion_output = self.fusion_module(graph_embeddings, construct_embed[:, i, :])\n",
        "#             fusion_outputs.append(fusion_output)\n",
        "\n",
        "#         fusion_outputs = torch.cat(fusion_outputs, dim=1)\n",
        "\n",
        "#         # Reshape node embeddings to [number of nodes, batch size, embedding dimension]\n",
        "#         batch_size, num_nodes, embed_dim = node_embeddings.shape\n",
        "#         node_embeddings = node_embeddings.transpose(0, 1)  # [number of nodes, batch size, embedding dimension]\n",
        "\n",
        "#         # Decoder\n",
        "#         print(\"fusion_outputs\", fusion_outputs.shape)\n",
        "#         decoder_output = self.transformer_decoder(\n",
        "#             tgt=fusion_outputs.transpose(0, 1),  # Ensure tgt shape is [sequence length, batch size, embedding dimension]\n",
        "#             memory=node_embeddings,\n",
        "#             memory_mask=memory_mask,\n",
        "#             tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "#             memory_key_padding_mask=memory_key_padding_mask\n",
        "#         )\n",
        "#         decoder_output = decoder_output.permute(1, 0, 2)\n",
        "#         print(\"decoder_output: \",decoder_output.shape)\n",
        "#         # Final output layers\n",
        "#         output1 = self.output_layer1(decoder_output)\n",
        "#         print('output1.1 =', output1.shape)\n",
        "#         # output1 = output1.transpose(0,1)\n",
        "#         print('output1.2 =', output1.shape)\n",
        "#         output1 = F.softmax(output1, dim=-1)  # t_i\n",
        "#         print('output1.3 =', output1.shape)\n",
        "\n",
        "#         output2 = self.output_layer2(decoder_output)\n",
        "#         print('output2.1 =', output2.shape)\n",
        "#         output2 = output2.view(batch_size, -1, self.param_cat)\n",
        "#         print('output2.2 =', output2.shape)\n",
        "\n",
        "\n",
        "#         output2 = F.softmax(output2, dim=2)  # p_i\n",
        "#         print(\"output1 \",output1.shape)\n",
        "#         print(\"output2 \", output2.shape)\n",
        "#         return output1, output2\n",
        "\n",
        "class CADDecoder(nn.Module):\n",
        "    def __init__(self, d_model=64, nhead=8, num_decoder_layers=4, dim_feedforward=2048, dropout=0.1, activation=\"relu\", num_commands=7, max_seq_len=5000, num_parameters=16, param_cat=257):\n",
        "        super(CADDecoder, self).__init__()\n",
        "        self.param_cat = param_cat\n",
        "        # Embedding\n",
        "        self.cad_command_embedding = CADEmbedding(d_model=d_model, n_commands=num_commands, n_args=num_parameters, seq_len=max_seq_len)\n",
        "\n",
        "        # Fusion module\n",
        "        self.fusion_module = FusionModule(latent_size=128)\n",
        "\n",
        "        # Decoder\n",
        "        decoder_layer = CausalTransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, activation=activation)\n",
        "        self.transformer_decoder = CausalTransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "\n",
        "        # Final output\n",
        "        self.output_layer1 = nn.Linear(d_model, num_commands)  # t_i\n",
        "        self.output_layer2 = nn.Linear(d_model, num_parameters * param_cat)  # p_i\n",
        "\n",
        "    def forward(self, node_embeddings, graph_embeddings, command_seq, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        # Command embedding\n",
        "        # print(\"command seq \", command_seq.shape)\n",
        "        commands_count = command_seq.shape[1]\n",
        "        commands = command_seq[:, :, 0]\n",
        "        args = command_seq[:, :, 1:]\n",
        "        construct_embed = self.cad_command_embedding(commands, args)\n",
        "\n",
        "        # Fusion module\n",
        "        fusion_outputs = []\n",
        "        # print('commands count=', commands_count)\n",
        "        for i in range(commands_count):\n",
        "            fusion_output = self.fusion_module(graph_embeddings, construct_embed[:, i, :])\n",
        "            # print('fusion_output shape =', fusion_output.shape)\n",
        "            fusion_outputs.append(fusion_output)\n",
        "        # print('fusion_outputs b4 len =', len(fusion_outputs))\n",
        "        # print('fusion_outputs b4 =', fusion_outputs[0].shape)\n",
        "        fusion_outputs = torch.cat(fusion_outputs, dim=1)  # [batch_size, commands_count, d_model]\n",
        "        # print('fusion_outputs af =', fusion_outputs.shape)\n",
        "\n",
        "        # Reshape node embeddings to [number of nodes, batch size, embedding dimension]\n",
        "        batch_size, num_nodes, embed_dim = node_embeddings.shape\n",
        "        node_embeddings = node_embeddings.transpose(0, 1)  # [number of nodes, batch size, embedding dimension]\n",
        "\n",
        "        # Initialize decoder outputs\n",
        "        all_decoder_outputs = []\n",
        "\n",
        "        # Autoregressive decoding\n",
        "        for t in range(commands_count):\n",
        "            if t == 0:\n",
        "                current_input = fusion_outputs[:, :1, :]  # [batch_size, 1, d_model]\n",
        "                # print('t==0, current_input shape =', current_input.shape)\n",
        "            else:\n",
        "                current_input = torch.cat((fusion_outputs[:, :t, :], last_output), dim=1)\n",
        "            #     print('t==else, current_input shape =', current_input.shape)\n",
        "            # print('current_input shape =', current_input.shape)\n",
        "\n",
        "            current_input = current_input.transpose(0, 1)  # [sequence_length, batch_size, d_model]\n",
        "\n",
        "            decoder_output = self.transformer_decoder(\n",
        "                tgt=current_input,\n",
        "                memory=node_embeddings,\n",
        "                memory_mask=memory_mask,\n",
        "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                memory_key_padding_mask=memory_key_padding_mask\n",
        "            )\n",
        "\n",
        "            decoder_output = decoder_output.transpose(0, 1)  # [batch_size, sequence_length, d_model]\n",
        "\n",
        "            # Get the last output token\n",
        "            last_output = decoder_output[:, -1, :].unsqueeze(1)  # [batch_size, 1, d_model]\n",
        "\n",
        "            all_decoder_outputs.append(last_output)\n",
        "\n",
        "        all_decoder_outputs = torch.cat(all_decoder_outputs, dim=1)  # [batch_size, commands_count, d_model]\n",
        "\n",
        "        # Final output layers\n",
        "        output1 = self.output_layer1(all_decoder_outputs)\n",
        "        # print('output1.1 =', output1.shape)\n",
        "        output1 = F.softmax(output1[:, -1:, :], dim=-1)  # t_i\n",
        "        # print('output1.2 =', output1.shape)\n",
        "\n",
        "        output2 = self.output_layer2(all_decoder_outputs)\n",
        "        # print('output2.1 =', output2.shape)\n",
        "        output2 = output2.view(batch_size, commands_count, -1, self.param_cat)[:, -1, :, :]  # Reshape and select last\n",
        "        # print('output2.2 =', output2.shape)\n",
        "        output2 = F.softmax(output2, dim=2)  # p_i\n",
        "        # print(\"output1 \", output1.shape)\n",
        "        # print(\"output2 \", output2.shape)\n",
        "        return output1, output2\n",
        "\n",
        "\n",
        "class CADParser(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CADParser, self).__init__()\n",
        "\n",
        "        # encoder\n",
        "        self.cad_encoder = CADEncoder()\n",
        "\n",
        "        # decoder\n",
        "        self.cad_decoder = CADDecoder()"
      ],
      "metadata": {
        "id": "P7DLd_7XIeaN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sflWNhu6J_D4",
        "outputId": "da1107a4-2780-4398-e8eb-bb85565cdabd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "# from basedataset import BaseDataset\n",
        "\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# from models import CADParser\n",
        "\n",
        "\n",
        "\n",
        "graphs, label_dict = dgl.load_graphs(\"/content/drive/My Drive/DeepCADDataset/all_graphs.bin\")\n",
        "# print(graphs)\n",
        "# print(label_dict)\n",
        "\n",
        "npz = np.load(\"/content/drive/My Drive/DeepCADDataset/all_npz.npz\")\n",
        "\n",
        "\n",
        "seednumber=2024\n",
        "torch.manual_seed(seednumber)\n",
        "torch.cuda.manual_seed(seednumber)\n",
        "np.random.seed(seednumber)\n",
        "\n",
        "# graphs, label_dict = dgl.load_graphs(\"data/all_graphs.bin\")\n",
        "# print(\"number of graphs: \",len(graphs))\n",
        "# print(\"detail of one of the graph: \",graphs[0])\n",
        "# npz = np.load(\"data/all_npz.npz\")\n",
        "# print(\"type: \",type(npz))\n",
        "# print(\"length: \",len(npz))\n",
        "# print(npz)\n",
        "# print(npz['vec_0'])\n",
        "\n",
        "# import dgl\n",
        "\n",
        "max_nodes = 0\n",
        "\n",
        "for graph in graphs:\n",
        "    num_nodes = graph.number_of_nodes()\n",
        "    if num_nodes > max_nodes:\n",
        "        max_nodes = num_nodes\n",
        "\n",
        "print(\"Maximum number of nodes in any graph:\", max_nodes)\n",
        "\n",
        "Y = []\n",
        "\n",
        "# Iterate over the sorted keys to maintain the order\n",
        "for key in npz.keys():\n",
        "    # print(key)\n",
        "    Y.append(npz[key])\n",
        "\n",
        "X = graphs\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=42) # train size = 6353\n",
        "\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "torch.set_printoptions(threshold=10_000)\n",
        "# Path where checkpoints are stored\n",
        "checkpoint_dir = 'checkpoint'\n",
        "loss_log_file_path = os.path.join(checkpoint_dir, 'loss_log.txt')\n",
        "\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "num_epochs = 100\n",
        "initial_learning_rate = 1e-3\n",
        "batch_size = 8  # Adjust according to your GPU memory\n",
        "warmup_epochs = 10\n",
        "root_dir = \"\"\n",
        "\n",
        "# Initialize the dataset and data loader\n",
        "dataset = BaseDataset(X_train, Y_train)\n",
        "data_loader = dataset.get_dataloader(batch_size)\n",
        "\n",
        "# Initialize the model\n",
        "model = CADParser().to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=initial_learning_rate)\n",
        "\n",
        "# Gradual warmup and learning rate decay\n",
        "scheduler = LambdaLR(\n",
        "    optimizer,\n",
        "    lr_lambda=lambda epoch: 0.9**(epoch // 30) * min((epoch + 1) / warmup_epochs, 1)\n",
        ")\n",
        "\n",
        "# Function to find the latest checkpoint file\n",
        "def find_latest_checkpoint(checkpoint_dir):\n",
        "    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth')]\n",
        "    if checkpoint_files:\n",
        "        latest_file = max(checkpoint_files, key=lambda x: int(x.strip('model_epoch_').strip('.pth')))\n",
        "        return os.path.join(checkpoint_dir, latest_file)\n",
        "    return None\n",
        "start_epoch = 0\n",
        "\n",
        "# Load the latest checkpoint if it exists\n",
        "# latest_checkpoint = find_latest_checkpoint(checkpoint_dir)\n",
        "\n",
        "# if latest_checkpoint:\n",
        "#     print(f\"Loading checkpoint '{latest_checkpoint}'\")\n",
        "#     checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
        "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
        "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "#     scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "#     start_epoch = checkpoint['epoch']\n",
        "#     print(f\"Resuming training from epoch {start_epoch}\")\n",
        "\n",
        "loss_list = []\n",
        "\n",
        "X_all_node_counts = []\n",
        "for batch in data_loader:\n",
        "    num_nodes_batch = batch['num_nodes']\n",
        "    X_all_node_counts.append(num_nodes_batch)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    print_out = True\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "        graphs, sequences = batch['graph'].to(device), batch['labels'].to(device)\n",
        "        num_nodes_per_graph = batch['num_nodes']\n",
        "        # print(\"num_nodes_per_graph\", num_nodes_per_graph)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        batch_loss = 0\n",
        "\n",
        "        node_embeddings, graph_embeddings = model.cad_encoder(graphs)\n",
        "        decoder_input_seq = sequences[:, 0:1, :]  # Start with the first vector (START token)\n",
        "\n",
        "        batch_num_nodes = node_embeddings.shape[0]\n",
        "        # print(\"len num_nodes_per_graph\", len(num_nodes_per_graph))\n",
        "        padded_node_embeddings = torch.zeros(len(num_nodes_per_graph), 66, 64).to(device)\n",
        "        padded_node_embeddings.to(device)\n",
        "        graph_embeddings.to(device)\n",
        "        # decoder_input_seq.to(device)\n",
        "\n",
        "        start_idx = 0\n",
        "        for i, num_nodes in enumerate(num_nodes_per_graph):\n",
        "            end_idx = start_idx + num_nodes\n",
        "            padded_node_embeddings[i, :num_nodes] = node_embeddings[start_idx:end_idx]\n",
        "            start_idx = end_idx\n",
        "        # padded_node_embeddings: [batch size * padded node numbers * embd dim 64]\n",
        "\n",
        "        graph_embeddings = torch.unsqueeze(graph_embeddings, 1).to(device) # transform from [batch size * embd dim] to [batch size * 1 * embd dim 128]\n",
        "\n",
        "\n",
        "        for t in range(1, sequences.size(1)):\n",
        "            # sequences: [batch size * sequence length (padding 60) * 17 len raw vector]\n",
        "            # print(f\"-------------------------{t}-----------------------------\")\n",
        "            # print(\"decoder input seq: \", decoder_input_seq.shape) # [batch size * sequence length [t] * len-17 raw vector]\n",
        "            decoder_output_t_i, decoder_output_p_i = model.cad_decoder(padded_node_embeddings, graph_embeddings, decoder_input_seq) # two probability vectors: t_i, v_i\n",
        "            # print(\"decoder_output_t_i \", decoder_output_t_i.shape) # [batch size * sequence length [t] * len-17 one hot vector]\n",
        "            # print(\"decoder_output_p_i \", decoder_output_p_i.shape) # [batch size * sequence length [t] * len-257 one hot vector]\n",
        "\n",
        "            gt_t = sequences[:, :t, :] # ground truth seq in raw 17-len vector form\n",
        "\n",
        "            # command_type_t = sequences[:, :t, 0] # get command type\n",
        "            command_type_t = sequences[:, t, 0]\n",
        "            # print('command type true', command_type_t)\n",
        "            command_type_t = command_type_t.long()\n",
        "            # print(\"command_type_t: \", command_type_t.shape)\n",
        "            # param_t = sequences[:, :t, 1:] # get parameters\n",
        "            param_t = sequences[:, t, 1:]\n",
        "            # print('param true', param_t)\n",
        "            param_t_mapped = param_t + 1\n",
        "            # param_t_mapped = param_t_mapped.long()  # Ensure param_t_mapped is of type Long\n",
        "            # param_t_mapped = torch.flatten(param_t_mapped, start_dim=1, end_dim=2)\n",
        "            # print(\"param_t_mapped: \", param_t_mapped.shape)\n",
        "\n",
        "            # decoder_output_t_i = decoder_output_t_i.view(-1, 7)  # Flatten to [batch_size*sequence_length, num_classes]\n",
        "            # command_type_t = command_type_t.view(-1)  # Flatten to [batch_size*sequence_length]\n",
        "            # decoder_output_p_i = decoder_output_p_i.view(-1, 257)  # Flatten to [batch_size*sequence_length*num_parameters, num_classes]\n",
        "            # param_t_mapped = param_t_mapped.view(-1)  # Flatten to [batch_size*sequence_length*num_parameters]\n",
        "            true_t_i = F.one_hot(command_type_t, num_classes=7)\n",
        "            # print(\"true_t_i\", true_t_i.shape)\n",
        "            true_p_i = F.one_hot(param_t_mapped.long(), num_classes=257)\n",
        "            # print(\"true_p_i \", true_p_i.shape)\n",
        "\n",
        "            # Calculate losses\n",
        "            t_i_loss = criterion(decoder_output_t_i.squeeze(), true_t_i.float())\n",
        "            p_i_loss = criterion(decoder_output_p_i, true_p_i.float())\n",
        "\n",
        "            # print(f't_i_loss: {t_i_loss.item()}')\n",
        "            # print(f'p_i_loss: {p_i_loss.item()}')\n",
        "\n",
        "\n",
        "            loss = t_i_loss + p_i_loss # compare decoder output with the true next output\n",
        "\n",
        "            # convert softmax distribution back into valid token\n",
        "            _, command_type_pred_next = torch.max(decoder_output_t_i, dim=2, keepdim=True)\n",
        "\n",
        "            _, command_args_pred_next = torch.max(decoder_output_p_i, dim=2, keepdim = True)\n",
        "\n",
        "            command_args_pred_next = command_args_pred_next - 1\n",
        "            # print(\"command_type_pred_next\", command_type_pred_next.shape)\n",
        "            # print(\"command_args_pred_next\", command_args_pred_next.shape)\n",
        "            next_token_pred = torch.cat((command_type_pred_next, command_args_pred_next), dim=1)\n",
        "\n",
        "            next_token_pred = next_token_pred.transpose(1,2)\n",
        "            # print(\"decoder_input_seq\", decoder_input_seq.shape)\n",
        "            next_token_pred = next_token_pred.view(batch_size, -1, 17)\n",
        "            # print(\"next_token_pred \", next_token_pred.shape)\n",
        "            decoder_input_seq = torch.cat((decoder_input_seq, next_token_pred), dim=1)\n",
        "            batch_loss += loss\n",
        "\n",
        "        if print_out:\n",
        "            print('batch_idx:', batch_idx)\n",
        "            # print('pred:', decoder_input_seq[0])\n",
        "            # print('true:', sequences[0])\n",
        "\n",
        "        # Normalize loss by seq length\n",
        "        batch_loss /= (sequences.size(1) - 1)\n",
        "        batch_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        total_loss += batch_loss.item()\n",
        "\n",
        "        scheduler.step()  # Update the learning rate\n",
        "        # print(f'batch_idx = {batch_idx}')\n",
        "        # print(f'epoch {epoch+1}, training batch_loss = {batch_loss}')\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "    loss_list.append(total_loss / 49)\n",
        "    with open(loss_log_file_path, 'a') as f:\n",
        "        f.write(str(total_loss / 49))\n",
        "    print(f'----------------------EPOCH {epoch+1}, TOTAL LOSS = {total_loss / 49}')\n",
        "        # if epoch % 10 == 0:\n",
        "        #     print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(data_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Save checkpoint at the end of each epoch\n",
        "    checkpoint = {\n",
        "        'epoch': epoch + 1,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'loss': loss_list,\n",
        "    }\n",
        "    torch.cuda.empty_cache()\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        torch.save(checkpoint, f'{checkpoint_dir}/model_epoch_{epoch+1}.pth')\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a plot of the losses\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(loss_list, label='Loss per Epoch')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Save the plot to a PNG file\n",
        "plt.savefig('plot/loss_plot.png')\n",
        "plt.close()  # Close the plot explicitly after saving to free up memory\n"
      ],
      "metadata": {
        "id": "v4nZtymaIfvY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "c8310222-366c-40eb-a2d9-ac1a573cdb76"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum number of nodes in any graph: 66\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.17 GiB of which 10.88 MiB is free. Process 28898 has 22.15 GiB memory in use. Of the allocated memory 21.88 GiB is allocated by PyTorch, and 18.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-4470d4f45796>\u001b[0m in \u001b[0;36m<cell line: 133>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;31m# print(f\"-------------------------{t}-----------------------------\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;31m# print(\"decoder input seq: \", decoder_input_seq.shape) # [batch size * sequence length [t] * len-17 raw vector]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mdecoder_output_t_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_output_p_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcad_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_node_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input_seq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# two probability vectors: t_i, v_i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0;31m# print(\"decoder_output_t_i \", decoder_output_t_i.shape) # [batch size * sequence length [t] * len-17 one hot vector]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;31m# print(\"decoder_output_p_i \", decoder_output_p_i.shape) # [batch size * sequence length [t] * len-257 one hot vector]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-11709a504eda>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, node_embeddings, graph_embeddings, command_seq, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mcurrent_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [sequence_length, batch_size, d_model]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             decoder_output = self.transformer_decoder(\n\u001b[0m\u001b[1;32m    764\u001b[0m                 \u001b[0mtgt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m                 \u001b[0mmemory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-11709a504eda>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, cache, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    599\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cache parameter should be None in training mode\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m                 \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-11709a504eda>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m             \u001b[0mtgt_last_tok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_is_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mha_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_is_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_ff_block\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0;31m# feed forward block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1471\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1472\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1473\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.17 GiB of which 10.88 MiB is free. Process 28898 has 22.15 GiB memory in use. Of the allocated memory 21.88 GiB is allocated by PyTorch, and 18.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "HkfbrVrVkZ4S"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}