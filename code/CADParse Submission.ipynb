{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63GnJL2wm2a3",
        "outputId": "614670d5-fa9d-4623-c4ac-74841e74cb40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels/torch-2.3/cu121/repo.html\n",
            "Collecting dgl\n",
            "  Downloading https://data.dgl.ai/wheels/torch-2.3/cu121/dgl-2.2.1%2Bcu121-cp310-cp310-manylinux1_x86_64.whl (199.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.66.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Collecting torchdata>=0.5.0 (from dgl)\n",
            "  Downloading torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from dgl) (2.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2024.2.2)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata>=0.5.0->dgl) (2.3.0+cu121)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->dgl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->dgl) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->dgl) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->dgl) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->torchdata>=0.5.0->dgl) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->torchdata>=0.5.0->dgl) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchdata, dgl\n",
            "Successfully installed dgl-2.2.1+cu121 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 torchdata-0.7.1\n"
          ]
        }
      ],
      "source": [
        "!pip install  dgl -f https://data.dgl.ai/wheels/torch-2.3/cu121/repo.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vrB6d1ioIVZw"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import FloatTensor\n",
        "import dgl\n",
        "from dgl.data.utils import load_graphs\n",
        "from tqdm import tqdm\n",
        "from abc import abstractmethod\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import dgl\n",
        "from torch import FloatTensor, stack\n",
        "\n",
        "class BaseDataset(Dataset):\n",
        "    @staticmethod\n",
        "    def num_classes():\n",
        "        pass\n",
        "\n",
        "    def __init__(self, X_train, Y_train):\n",
        "        \"\"\"\n",
        "        self.data is a list of dictionaries with keys graph and label\n",
        "        \"\"\"\n",
        "        assert len(X_train) == len(Y_train), \"The number of graphs must match the number of labels\"\n",
        "        self.data = [{\"graph\": graph, \"label\": label} for graph, label in zip(X_train, Y_train)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        return sample[\"graph\"], sample[\"label\"]\n",
        "\n",
        "    def _collate(self, batch):\n",
        "        graphs, labels = zip(*batch)\n",
        "        batched_graph = dgl.batch(graphs)\n",
        "        num_nodes_per_graph = [graph.number_of_nodes() for graph in graphs]\n",
        "\n",
        "\n",
        "        pad_vector = torch.tensor([6] + [-1]*16, dtype=torch.float32)\n",
        "\n",
        "\n",
        "        max_length = 20\n",
        "        padded_labels = []\n",
        "        for label in labels:\n",
        "            label_tensor = torch.tensor(label, dtype=torch.float32)\n",
        "            label_length = label_tensor.shape[0]\n",
        "\n",
        "            if label_length < max_length:\n",
        "\n",
        "                padding_count = max_length - label_length\n",
        "\n",
        "                padding = pad_vector.repeat(padding_count, 1)\n",
        "\n",
        "                padded_label = torch.cat([label_tensor, padding], dim=0)\n",
        "            elif label_length > max_length:\n",
        "\n",
        "                padded_label = label_tensor[:max_length]\n",
        "            else:\n",
        "                padded_label = label_tensor\n",
        "\n",
        "            padded_labels.append(padded_label)\n",
        "\n",
        "\n",
        "        padded_labels = torch.stack(padded_labels)\n",
        "        return {\"graph\": batched_graph, \"labels\": padded_labels, \"num_nodes\": num_nodes_per_graph}\n",
        "\n",
        "\n",
        "    def get_dataloader(self, batch_size, shuffle=True, num_workers=0):\n",
        "        return DataLoader(\n",
        "            self,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle,\n",
        "            collate_fn=self._collate,\n",
        "            num_workers=num_workers,\n",
        "            drop_last=True\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "P7DLd_7XIeaN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a32cedb-06ed-40d6-9a26-14e1608780cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from dgl.nn.pytorch.conv import NNConv\n",
        "from dgl.nn.pytorch.glob import MaxPooling\n",
        "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
        "import math\n",
        "\n",
        "\n",
        "# Convolutional Layers\n",
        "def _conv1d(in_channels, out_channels, kernel_size=3, padding=0, bias=False):\n",
        "    \"\"\"\n",
        "    Helper function to create a 1D convolutional layer with batchnorm and LeakyReLU activation\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Input channels\n",
        "        out_channels (int): Output channels\n",
        "        kernel_size (int, optional): Size of the convolutional kernel. Defaults to 3.\n",
        "        padding (int, optional): Padding size on each side. Defaults to 0.\n",
        "        bias (bool, optional): Whether bias is used. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        nn.Sequential: Sequential contained the Conv1d, BatchNorm1d and LeakyReLU layers\n",
        "    \"\"\"\n",
        "    return nn.Sequential(\n",
        "        nn.Conv1d(\n",
        "            in_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=bias\n",
        "        ),\n",
        "        nn.BatchNorm1d(out_channels),\n",
        "        nn.LeakyReLU(),\n",
        "    )\n",
        "\n",
        "\n",
        "def _conv2d(in_channels, out_channels, kernel_size, padding=0, bias=False):\n",
        "    \"\"\"\n",
        "    Helper function to create a 2D convolutional layer with batchnorm and LeakyReLU activation\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Input channels\n",
        "        out_channels (int): Output channels\n",
        "        kernel_size (int, optional): Size of the convolutional kernel. Defaults to 3.\n",
        "        padding (int, optional): Padding size on each side. Defaults to 0.\n",
        "        bias (bool, optional): Whether bias is used. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        nn.Sequential: Sequential contained the Conv2d, BatchNorm2d and LeakyReLU layers\n",
        "    \"\"\"\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=padding,\n",
        "            bias=bias,\n",
        "        ),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.LeakyReLU(),\n",
        "    )\n",
        "\n",
        "\n",
        "def _fc(in_features, out_features, bias=False):\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(in_features, out_features, bias=bias),\n",
        "        nn.BatchNorm1d(out_features),\n",
        "        nn.LeakyReLU(),\n",
        "    )\n",
        "\n",
        "class _MLP(nn.Module):\n",
        "    \"\"\"\"\"\"\n",
        "\n",
        "    def __init__(self, num_layers, input_dim, hidden_dim, output_dim):\n",
        "        \"\"\"\n",
        "        MLP with linear output\n",
        "        Args:\n",
        "            num_layers (int): The number of linear layers in the MLP\n",
        "            input_dim (int): Input feature dimension\n",
        "            hidden_dim (int): Hidden feature dimensions for all hidden layers\n",
        "            output_dim (int): Output feature dimension\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the given number of layers is <1\n",
        "        \"\"\"\n",
        "        super(_MLP, self).__init__()\n",
        "        self.linear_or_not = True  # default is linear model\n",
        "        self.num_layers = num_layers\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        if num_layers < 1:\n",
        "            raise ValueError(\"Number of layers should be positive!\")\n",
        "        elif num_layers == 1:\n",
        "            # Linear model\n",
        "            self.linear = nn.Linear(input_dim, output_dim)\n",
        "        else:\n",
        "            # Multi-layer model\n",
        "            self.linear_or_not = False\n",
        "            self.linears = torch.nn.ModuleList()\n",
        "            self.batch_norms = torch.nn.ModuleList()\n",
        "\n",
        "            self.linears.append(nn.Linear(input_dim, hidden_dim))\n",
        "            for layer in range(num_layers - 2):\n",
        "                self.linears.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "            self.linears.append(nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "            # TODO: this could move inside the above loop\n",
        "            for layer in range(num_layers - 1):\n",
        "                self.batch_norms.append(nn.BatchNorm1d((hidden_dim)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.linear_or_not:\n",
        "            # If linear model\n",
        "            return self.linear(x)\n",
        "        else:\n",
        "            # If MLP\n",
        "            h = x\n",
        "            for i in range(self.num_layers - 1):\n",
        "                h = F.relu(self.batch_norms[i](self.linears[i](h)))\n",
        "            return self.linears[-1](h)\n",
        "\n",
        "class UVNetCurveEncoder(nn.Module):\n",
        "    def __init__(self, in_channels=6, output_dims=64):\n",
        "        \"\"\"\n",
        "        This is the 1D convolutional network that extracts features from the B-rep edge\n",
        "        geometry described as 1D UV-grids (see Section 3.2, Curve & surface convolution\n",
        "        in paper)\n",
        "\n",
        "        Args:\n",
        "            in_channels (int, optional): Number of channels in the edge UV-grids. By default\n",
        "                                         we expect 3 channels for point coordinates and 3 for\n",
        "                                         curve tangents. Defaults to 6.\n",
        "            output_dims (int, optional): Output curve embedding dimension. Defaults to 64.\n",
        "        \"\"\"\n",
        "        super(UVNetCurveEncoder, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.conv1 = _conv1d(in_channels, 64, kernel_size=3, padding=1, bias=False)\n",
        "        self.conv2 = _conv1d(64, 128, kernel_size=3, padding=1, bias=False)\n",
        "        self.conv3 = _conv1d(128, 256, kernel_size=3, padding=1, bias=False)\n",
        "        self.final_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = _fc(256, output_dims, bias=False)\n",
        "\n",
        "        for m in self.modules():\n",
        "            self.weights_init(m)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, (nn.Linear, nn.Conv1d)):\n",
        "            torch.nn.init.kaiming_uniform_(m.weight.data)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.size(1) == self.in_channels\n",
        "        batch_size = x.size(0)\n",
        "        x = x.float()\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.final_pool(x)\n",
        "        x = x.view(batch_size, -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UVNetSurfaceEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=7,\n",
        "        output_dims=64,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        This is the 2D convolutional network that extracts features from the B-rep face\n",
        "        geometry described as 2D UV-grids (see Section 3.2, Curve & surface convolution\n",
        "        in paper)\n",
        "\n",
        "        Args:\n",
        "            in_channels (int, optional): Number of channels in the edge UV-grids. By default\n",
        "                                         we expect 3 channels for point coordinates and 3 for\n",
        "                                         surface normals and 1 for the trimming mask. Defaults\n",
        "                                         to 7.\n",
        "            output_dims (int, optional): Output surface embedding dimension. Defaults to 64.\n",
        "        \"\"\"\n",
        "        super(UVNetSurfaceEncoder, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.conv1 = _conv2d(in_channels, 64, 3, padding=1, bias=False)\n",
        "        self.conv2 = _conv2d(64, 128, 3, padding=1, bias=False)\n",
        "        self.conv3 = _conv2d(128, 256, 3, padding=1, bias=False)\n",
        "        self.final_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = _fc(256, output_dims, bias=False)\n",
        "        for m in self.modules():\n",
        "            self.weights_init(m)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
        "            torch.nn.init.kaiming_uniform_(m.weight.data)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.size(1) == self.in_channels\n",
        "        batch_size = x.size(0)\n",
        "        x = x.float()\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.final_pool(x)\n",
        "        x = x.view(batch_size, -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class _EdgeConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        edge_feats,\n",
        "        out_feats,\n",
        "        node_feats,\n",
        "        num_mlp_layers=2,\n",
        "        hidden_mlp_dim=64,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        This module implements Eq. 2 from the paper where the edge features are\n",
        "        updated using the node features at the endpoints.\n",
        "\n",
        "        Args:\n",
        "            edge_feats (int): Input edge feature dimension\n",
        "            out_feats (int): Output feature deimension\n",
        "            node_feats (int): Input node feature dimension\n",
        "            num_mlp_layers (int, optional): Number of layers used in the MLP. Defaults to 2.\n",
        "            hidden_mlp_dim (int, optional): Hidden feature dimension in the MLP. Defaults to 64.\n",
        "        \"\"\"\n",
        "        super(_EdgeConv, self).__init__()\n",
        "        self.proj = _MLP(1, node_feats, hidden_mlp_dim, edge_feats)\n",
        "        self.mlp = _MLP(num_mlp_layers, edge_feats, hidden_mlp_dim, out_feats)\n",
        "        self.batchnorm = nn.BatchNorm1d(out_feats)\n",
        "        self.eps = torch.nn.Parameter(torch.FloatTensor([0.0]))\n",
        "\n",
        "    def forward(self, graph, nfeat, efeat):\n",
        "        src, dst = graph.edges()\n",
        "        proj1, proj2 = self.proj(nfeat[src]), self.proj(nfeat[dst])\n",
        "        agg = proj1 + proj2\n",
        "        h = self.mlp((1 + self.eps) * efeat + agg)\n",
        "        h = F.leaky_relu(self.batchnorm(h))\n",
        "        return h\n",
        "\n",
        "\n",
        "class _NodeConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        node_feats,\n",
        "        out_feats,\n",
        "        edge_feats,\n",
        "        num_mlp_layers=2,\n",
        "        hidden_mlp_dim=64,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        This module implements Eq. 1 from the paper where the node features are\n",
        "        updated using the neighboring node and edge features.\n",
        "\n",
        "        Args:\n",
        "            node_feats (int): Input edge feature dimension\n",
        "            out_feats (int): Output feature deimension\n",
        "            node_feats (int): Input node feature dimension\n",
        "            num_mlp_layers (int, optional): Number of layers used in the MLP. Defaults to 2.\n",
        "            hidden_mlp_dim (int, optional): Hidden feature dimension in the MLP. Defaults to 64.\n",
        "        \"\"\"\n",
        "        super(_NodeConv, self).__init__()\n",
        "        self.gconv = NNConv(\n",
        "            in_feats=node_feats,\n",
        "            out_feats=out_feats,\n",
        "            edge_func=nn.Linear(edge_feats, node_feats * out_feats),\n",
        "            aggregator_type=\"sum\",\n",
        "            bias=False,\n",
        "        )\n",
        "        self.batchnorm = nn.BatchNorm1d(out_feats)\n",
        "        self.mlp = _MLP(num_mlp_layers, node_feats, hidden_mlp_dim, out_feats)\n",
        "        self.eps = torch.nn.Parameter(torch.FloatTensor([0.0]))\n",
        "\n",
        "    def forward(self, graph, nfeat, efeat):\n",
        "        h = (1 + self.eps) * nfeat\n",
        "        h = self.gconv(graph, h, efeat)\n",
        "        h = self.mlp(h)\n",
        "        h = F.leaky_relu(self.batchnorm(h))\n",
        "        return h\n",
        "\n",
        "\n",
        "class UVNetGraphEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        input_edge_dim,\n",
        "        output_dim,\n",
        "        hidden_dim=64,\n",
        "        learn_eps=True,\n",
        "        num_layers=3,\n",
        "        num_mlp_layers=2,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        This is the graph neural network used for message-passing features in the\n",
        "        face-adjacency graph.\n",
        "\n",
        "        Args:\n",
        "            input_dim ([type]): [description]\n",
        "            input_edge_dim ([type]): [description]\n",
        "            output_dim ([type]): [description]\n",
        "            hidden_dim (int, optional): [description]. Defaults to 64.\n",
        "            learn_eps (bool, optional): [description]. Defaults to True.\n",
        "            num_layers (int, optional): [description]. Defaults to 3.\n",
        "            num_mlp_layers (int, optional): [description]. Defaults to 2.\n",
        "        \"\"\"\n",
        "        super(UVNetGraphEncoder, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.learn_eps = learn_eps\n",
        "\n",
        "        # List of layers for node and edge feature message passing\n",
        "        self.node_conv_layers = torch.nn.ModuleList()\n",
        "        self.edge_conv_layers = torch.nn.ModuleList()\n",
        "\n",
        "        for layer in range(self.num_layers - 1):\n",
        "            node_feats = input_dim if layer == 0 else hidden_dim\n",
        "            edge_feats = input_edge_dim if layer == 0 else hidden_dim\n",
        "            self.node_conv_layers.append(\n",
        "                _NodeConv(\n",
        "                    node_feats=node_feats,\n",
        "                    out_feats=hidden_dim,\n",
        "                    edge_feats=edge_feats,\n",
        "                    num_mlp_layers=num_mlp_layers,\n",
        "                    hidden_mlp_dim=hidden_dim,\n",
        "                ),\n",
        "            )\n",
        "            self.edge_conv_layers.append(\n",
        "                _EdgeConv(\n",
        "                    edge_feats=edge_feats,\n",
        "                    out_feats=hidden_dim,\n",
        "                    node_feats=node_feats,\n",
        "                    num_mlp_layers=num_mlp_layers,\n",
        "                    hidden_mlp_dim=hidden_dim,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Linear function for graph poolings of output of each layer\n",
        "        # which maps the output of different layers into a prediction score\n",
        "        self.linears_prediction = torch.nn.ModuleList()\n",
        "\n",
        "        for layer in range(num_layers):\n",
        "            if layer == 0:\n",
        "                self.linears_prediction.append(nn.Linear(input_dim, output_dim))\n",
        "            else:\n",
        "                self.linears_prediction.append(nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "        self.drop1 = nn.Dropout(0.3)\n",
        "        self.drop = nn.Dropout(0.5)\n",
        "        self.pool = MaxPooling()\n",
        "\n",
        "    def forward(self, g, h, efeat):\n",
        "        hidden_rep = [h]\n",
        "        he = efeat\n",
        "\n",
        "        for i in range(self.num_layers - 1):\n",
        "\n",
        "            h = self.node_conv_layers[i](g, h, he)\n",
        "            he = self.edge_conv_layers[i](g, h, he)\n",
        "            hidden_rep.append(h)\n",
        "        node_embeddings = hidden_rep[-1]\n",
        "        node_embeddings = self.drop1(node_embeddings)\n",
        "\n",
        "        graph_representation = 0\n",
        "        for i, h in enumerate(hidden_rep):\n",
        "            pooled_h = self.pool(g, h)\n",
        "            graph_representation += self.drop(self.linears_prediction[i](pooled_h))\n",
        "\n",
        "\n",
        "        return node_embeddings, graph_representation\n",
        "\n",
        "class CADEncoder(nn.Module):\n",
        "  def __init__(self, crv_emb_dim=64, srf_emb_dim=64, graph_emb_dim=128, dropout=0.3):\n",
        "    super(CADEncoder, self).__init__()\n",
        "    self.curv_encoder = UVNetCurveEncoder(in_channels=10, output_dims=crv_emb_dim) # in_channels originally 6\n",
        "    self.surf_encoder = UVNetSurfaceEncoder(in_channels=10, output_dims=srf_emb_dim)\n",
        "    self.graph_encoder = UVNetGraphEncoder(srf_emb_dim, crv_emb_dim, graph_emb_dim)\n",
        "\n",
        "  def forward(self, batched_graph):\n",
        "    input_crv_feat = batched_graph.edata[\"x\"]\n",
        "    input_srf_feat = batched_graph.ndata[\"x\"]\n",
        "    hidden_crv_feat = self.curv_encoder(input_crv_feat)\n",
        "    hidden_srf_feat = self.surf_encoder(input_srf_feat)\n",
        "    node_emb, graph_emb = self.graph_encoder(batched_graph, hidden_srf_feat, hidden_crv_feat)\n",
        "    return node_emb, graph_emb\n",
        "\n",
        "\n",
        "class PositionalEncodingLUT(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=250):\n",
        "        super(PositionalEncodingLUT, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(0, max_len, dtype=torch.long).unsqueeze(1)\n",
        "        self.register_buffer('position', position)\n",
        "\n",
        "        self.pos_embed = nn.Embedding(max_len, d_model)\n",
        "\n",
        "        self._init_embeddings()\n",
        "\n",
        "    def _init_embeddings(self):\n",
        "        nn.init.kaiming_normal_(self.pos_embed.weight, mode=\"fan_in\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, _ = x.shape\n",
        "\n",
        "        pos = self.position[:seq_length]  # This will have shape [seq_length, 1]\n",
        "\n",
        "        pos = pos.expand(-1, batch_size).contiguous()  # Reshape to [seq_length, batch_size]\n",
        "        pos = pos.transpose(0, 1)  # Transpose to [batch_size, seq_length]\n",
        "\n",
        "        pos_embeddings = self.pos_embed(pos)  # This should now be [batch_size, seq_length, d_model]\n",
        "\n",
        "        x = x + pos_embeddings\n",
        "\n",
        "        return self.dropout(x)\n",
        "\n",
        "class CADEmbedding(nn.Module):\n",
        "    \"\"\"Embedding: positional embed + command embed + parameter embed + group embed (optional)\"\"\"\n",
        "    def __init__(self, n_commands=7, d_model=64, n_args=16, args_dim=257, seq_len=60):\n",
        "        super(CADEmbedding, self).__init__()\n",
        "        self.command_embed = nn.Embedding(n_commands, d_model)\n",
        "        self.arg_embed = nn.Embedding(args_dim, d_model, padding_idx=0)\n",
        "        self.embed_fcn = nn.Linear(d_model * n_args, d_model)\n",
        "        self.pos_encoding = PositionalEncodingLUT(d_model, max_len=seq_len+2)\n",
        "\n",
        "    def forward(self, commands, args):\n",
        "        S, N = commands.shape\n",
        "        src = self.command_embed(commands.long()) + \\\n",
        "              self.embed_fcn(self.arg_embed((args + 1).long()).view(S, N, -1))\n",
        "        src = self.pos_encoding(src)\n",
        "        return src\n",
        "\n",
        "class FusionModule(nn.Module):\n",
        "    def __init__(self, latent_size, command_embedding_size=64, hidden_size=128, output_size=64):\n",
        "        super(FusionModule, self).__init__()\n",
        "        self.fc1 = nn.Linear(latent_size + command_embedding_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, latent, command_embedding):\n",
        "        command_embedding = command_embedding.unsqueeze(1)\n",
        "        combined = torch.cat((latent, command_embedding), dim=2)\n",
        "        x = self.fc1(combined)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "from torch import Tensor\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "\n",
        "class CausalTransformerDecoderLayer(nn.TransformerDecoderLayer):\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        if self.training:\n",
        "            return super().forward(tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\n",
        "        else:\n",
        "            tgt_last_tok = tgt[-1:, :, :]\n",
        "            tgt_last_tok = self.self_attn(tgt_last_tok, tgt, tgt)[0] + tgt_last_tok\n",
        "            tgt_last_tok = self.norm1(tgt_last_tok)\n",
        "            if memory is not None:\n",
        "                tgt_last_tok = self.multihead_attn(tgt_last_tok, memory, memory)[0] + tgt_last_tok\n",
        "                tgt_last_tok = self.norm2(tgt_last_tok)\n",
        "            tgt_last_tok = self.linear2(self.dropout(self.activation(self.linear1(tgt_last_tok)))) + tgt_last_tok\n",
        "            tgt_last_tok = self.norm3(tgt_last_tok)\n",
        "            return tgt_last_tok\n",
        "\n",
        "\n",
        "class CausalTransformerDecoder(nn.TransformerDecoder):\n",
        "    def forward(self, tgt, memory=None, cache=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        # print(\"tgt in TransformerDecoder \", tgt.shape)\n",
        "        # print(\"memory in TransformerDecoder \", memory.shape)\n",
        "        if self.training:\n",
        "            if cache is not None:\n",
        "                raise ValueError(\"cache parameter should be None in training mode\")\n",
        "            for mod in self.layers:\n",
        "                tgt = mod(tgt, memory, tgt_mask=None, memory_mask=memory_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
        "            return tgt\n",
        "        else:\n",
        "            new_token_cache = []\n",
        "            for i, mod in enumerate(self.layers):\n",
        "                tgt = mod(tgt, memory)\n",
        "                new_token_cache.append(tgt)\n",
        "                if cache is not None:\n",
        "                    tgt = torch.cat([cache[i], tgt], dim=0)\n",
        "            if cache is not None:\n",
        "                new_cache = torch.cat([cache, torch.stack(new_token_cache, dim=0)], dim=1)\n",
        "            else:\n",
        "                new_cache = torch.stack(new_token_cache, dim=0)\n",
        "            # print(\"tgt[-1:] from the transformer decoder block \", tgt[-1:])\n",
        "            # print(\"new_cache from the transformer decoder block \", new_cache)\n",
        "            return tgt[-1:], new_cache\n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _generate_causal_mask(sz, device):\n",
        "        mask = torch.full((sz, sz), float('-inf'))\n",
        "        mask = torch.triu(mask, diagonal=1)\n",
        "        return mask.to(device)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _generate_causal_mask(sz, device):\n",
        "        \"\"\"\n",
        "        Generates a causal mask to hide future tokens for autoregressive tasks.\n",
        "        \"\"\"\n",
        "        mask = torch.full((sz, sz), float('-inf'))\n",
        "        mask = torch.triu(mask, diagonal=1)\n",
        "        return mask.to(device)\n",
        "\n",
        "\n",
        "class CADDecoder(nn.Module):\n",
        "    def __init__(self, d_model=64, nhead=8, num_decoder_layers=4, dim_feedforward=2048, dropout=0.1, activation=\"relu\", num_commands=7, max_seq_len=5000, num_parameters=16, param_cat=257):\n",
        "        super(CADDecoder, self).__init__()\n",
        "        self.param_cat = param_cat\n",
        "        # Embedding\n",
        "        self.cad_command_embedding = CADEmbedding(d_model=d_model, n_commands=num_commands, n_args=num_parameters, seq_len=max_seq_len)\n",
        "\n",
        "        # Fusion module\n",
        "        self.fusion_module = FusionModule(latent_size=128)\n",
        "\n",
        "        # Decoder\n",
        "        decoder_layer = CausalTransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, activation=activation)\n",
        "        self.transformer_decoder = CausalTransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "\n",
        "        # Final output\n",
        "        self.output_layer1 = nn.Linear(d_model, num_commands)  # t_i\n",
        "        self.output_layer2 = nn.Linear(d_model, num_parameters * param_cat)  # p_i\n",
        "\n",
        "    def forward(self, node_embeddings, graph_embeddings, command_seq, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        commands_count = command_seq.shape[1]\n",
        "        commands = command_seq[:, :, 0]\n",
        "        args = command_seq[:, :, 1:]\n",
        "        construct_embed = self.cad_command_embedding(commands, args)\n",
        "\n",
        "        # Fusion module\n",
        "        fusion_outputs = []\n",
        "        for i in range(commands_count):\n",
        "            fusion_output = self.fusion_module(graph_embeddings, construct_embed[:, i, :])\n",
        "            fusion_outputs.append(fusion_output)\n",
        "        fusion_outputs = torch.cat(fusion_outputs, dim=1)  # [batch_size, commands_count, d_model]\n",
        "\n",
        "        # Reshape node embeddings to [number of nodes, batch size, embedding dimension]\n",
        "        batch_size, num_nodes, embed_dim = node_embeddings.shape\n",
        "        node_embeddings = node_embeddings.transpose(0, 1)  # [number of nodes, batch size, embedding dimension]\n",
        "\n",
        "        all_decoder_outputs = []\n",
        "\n",
        "        # Autoregressive decoding\n",
        "        for t in range(commands_count):\n",
        "            if t == 0:\n",
        "                current_input = fusion_outputs[:, :1, :]  # [batch_size, 1, d_model]\n",
        "            else:\n",
        "                current_input = torch.cat((fusion_outputs[:, :t, :], last_output), dim=1)\n",
        "\n",
        "            current_input = current_input.transpose(0, 1)  # [sequence_length, batch_size, d_model]\n",
        "\n",
        "            decoder_output = self.transformer_decoder(\n",
        "                tgt=current_input,\n",
        "                memory=node_embeddings,\n",
        "                memory_mask=memory_mask,\n",
        "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                memory_key_padding_mask=memory_key_padding_mask\n",
        "            )\n",
        "\n",
        "            decoder_output = decoder_output.transpose(0, 1)  # [batch_size, sequence_length, d_model]\n",
        "\n",
        "            # Get the last output token\n",
        "            last_output = decoder_output[:, -1, :].unsqueeze(1)  # [batch_size, 1, d_model]\n",
        "\n",
        "            all_decoder_outputs.append(last_output)\n",
        "\n",
        "        all_decoder_outputs = torch.cat(all_decoder_outputs, dim=1)  # [batch_size, commands_count, d_model]\n",
        "\n",
        "        # Final output layers\n",
        "        output1 = self.output_layer1(all_decoder_outputs)\n",
        "        output1 = F.softmax(output1[:, -1:, :], dim=-1)  # t_i\n",
        "\n",
        "        output2 = self.output_layer2(all_decoder_outputs)\n",
        "        output2 = output2.view(batch_size, commands_count, -1, self.param_cat)[:, -1, :, :]  # Reshape and select last\n",
        "        output2 = F.softmax(output2, dim=2)  # p_i\n",
        "        return output1, output2\n",
        "\n",
        "\n",
        "class CADParser(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CADParser, self).__init__()\n",
        "\n",
        "        # encoder\n",
        "        self.cad_encoder = CADEncoder()\n",
        "\n",
        "        # decoder\n",
        "        self.cad_decoder = CADDecoder()\n",
        "    def forward(graphs, sequences):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sflWNhu6J_D4",
        "outputId": "5b520703-8d9a-4c23-9bee-1a74272001b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "graphs, label_dict = dgl.load_graphs(\"/content/drive/My Drive/DeepCADDataset/all_graphs.bin\")\n",
        "npz = np.load(\"/content/drive/My Drive/DeepCADDataset/all_npz.npz\")\n",
        "# Prepare data\n",
        "max_nodes = max(graph.number_of_nodes() for graph in graphs)\n",
        "print(\"Maximum number of nodes in any graph:\", max_nodes)\n",
        "\n",
        "# Filter graphs and corresponding npz entries\n",
        "filtered_graphs = []\n",
        "filtered_npz_keys = []\n",
        "for i in range(len(graphs)):\n",
        "    if graphs[i].number_of_nodes() <= 20:\n",
        "        filtered_graphs.append(graphs[i])\n",
        "        filtered_npz_keys.append(list(npz.keys())[i])\n",
        "\n",
        "# Count the number of filtered graphs\n",
        "count = len(filtered_graphs)\n",
        "print(\"Number of graphs with <= 20 nodes:\", count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lrc58J9PROEO",
        "outputId": "2b360ae3-774c-426f-adeb-97b4e18d23af"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum number of nodes in any graph: 66\n",
            "Number of graphs with <= 20 nodes: 6703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Assuming BaseDataset and CADParser are already defined\n",
        "\n",
        "# Load data\n",
        "graphs, label_dict = dgl.load_graphs(\"/content/drive/My Drive/DeepCADDataset/all_graphs.bin\")\n",
        "npz = np.load(\"/content/drive/My Drive/DeepCADDataset/all_npz.npz\")\n",
        "\n",
        "# Set seed for reproducibility\n",
        "seednumber = 2024\n",
        "torch.manual_seed(seednumber)\n",
        "torch.cuda.manual_seed(seednumber)\n",
        "np.random.seed(seednumber)\n",
        "\n",
        "# Prepare data\n",
        "max_nodes = max(graph.number_of_nodes() for graph in graphs)\n",
        "print(\"Maximum number of nodes in any graph:\", max_nodes)\n",
        "\n",
        "Y = [npz[key] for key in npz.keys()]\n",
        "X = graphs\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrTC4YLNaBCW",
        "outputId": "537d6b35-25f3-408e-f539-b7351f0fa641"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum number of nodes in any graph: 66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_X = []\n",
        "filtered_Y = []\n",
        "for i in range(len(X)):\n",
        "    if X[i].number_of_nodes() <= 20:\n",
        "        filtered_X.append(X[i])\n",
        "        filtered_Y.append(Y[i])\n",
        "\n",
        "# Count the number of filtered graphs\n",
        "count = len(filtered_X)\n",
        "print(\"Number of graphs with <= 20 nodes:\", count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAjbNl50aKbr",
        "outputId": "faf711d9-b36d-47f8-b98c-32dd64087f50"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of graphs with <= 20 nodes: 6703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(filtered_X, filtered_Y, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "oO0lY__gaDks"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4nZtymaIfvY",
        "outputId": "74be6206-9670-4022-cabe-a279e4c1fce4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch loss:  2.1503868103027344\n",
            "After processing batch 0 - Allocated: 90.37 MiB, Reserved: 610.00 MiB\n",
            "batch loss:  2.152381420135498\n",
            "After processing batch 1 - Allocated: 90.20 MiB, Reserved: 644.00 MiB\n",
            "batch loss:  2.13252592086792\n",
            "After processing batch 2 - Allocated: 90.30 MiB, Reserved: 596.00 MiB\n",
            "batch loss:  2.1202807426452637\n",
            "After processing batch 3 - Allocated: 90.40 MiB, Reserved: 588.00 MiB\n",
            "batch loss:  2.088874578475952\n",
            "After processing batch 4 - Allocated: 90.25 MiB, Reserved: 610.00 MiB\n",
            "batch loss:  2.0595810413360596\n",
            "After processing batch 5 - Allocated: 90.80 MiB, Reserved: 616.00 MiB\n",
            "batch loss:  2.0069761276245117\n",
            "After processing batch 6 - Allocated: 66.56 MiB, Reserved: 434.00 MiB\n",
            "batch loss:  2.0009541511535645\n",
            "After processing batch 7 - Allocated: 67.35 MiB, Reserved: 508.00 MiB\n",
            "batch loss:  1.954213261604309\n",
            "After processing batch 8 - Allocated: 66.74 MiB, Reserved: 470.00 MiB\n",
            "batch loss:  1.9417903423309326\n",
            "After processing batch 9 - Allocated: 66.69 MiB, Reserved: 416.00 MiB\n",
            "batch loss:  1.9392051696777344\n",
            "After processing batch 10 - Allocated: 66.91 MiB, Reserved: 466.00 MiB\n",
            "batch loss:  1.9139400720596313\n",
            "After processing batch 11 - Allocated: 66.91 MiB, Reserved: 444.00 MiB\n",
            "batch loss:  1.9213945865631104\n",
            "After processing batch 12 - Allocated: 66.55 MiB, Reserved: 438.00 MiB\n",
            "batch loss:  1.9045580625534058\n",
            "After processing batch 13 - Allocated: 66.22 MiB, Reserved: 424.00 MiB\n",
            "batch loss:  1.8907121419906616\n",
            "After processing batch 14 - Allocated: 66.40 MiB, Reserved: 430.00 MiB\n",
            "batch loss:  1.9089891910552979\n",
            "After processing batch 15 - Allocated: 66.26 MiB, Reserved: 446.00 MiB\n",
            "batch loss:  1.8374621868133545\n",
            "After processing batch 16 - Allocated: 66.78 MiB, Reserved: 446.00 MiB\n",
            "batch loss:  1.900446891784668\n",
            "After processing batch 17 - Allocated: 66.39 MiB, Reserved: 406.00 MiB\n",
            "batch loss:  1.9093431234359741\n",
            "After processing batch 18 - Allocated: 66.50 MiB, Reserved: 374.00 MiB\n",
            "batch loss:  1.8902863264083862\n",
            "After processing batch 19 - Allocated: 66.74 MiB, Reserved: 398.00 MiB\n",
            "batch loss:  1.8821803331375122\n",
            "After processing batch 20 - Allocated: 66.49 MiB, Reserved: 444.00 MiB\n",
            "batch loss:  1.86247718334198\n",
            "After processing batch 21 - Allocated: 66.47 MiB, Reserved: 428.00 MiB\n",
            "batch loss:  1.8782312870025635\n",
            "After processing batch 22 - Allocated: 66.97 MiB, Reserved: 402.00 MiB\n",
            "batch loss:  1.8340321779251099\n",
            "After processing batch 23 - Allocated: 66.47 MiB, Reserved: 420.00 MiB\n",
            "batch loss:  1.8087016344070435\n",
            "After processing batch 24 - Allocated: 66.19 MiB, Reserved: 442.00 MiB\n",
            "batch loss:  1.8848594427108765\n",
            "After processing batch 25 - Allocated: 67.06 MiB, Reserved: 446.00 MiB\n",
            "batch loss:  1.8648227453231812\n",
            "After processing batch 26 - Allocated: 66.13 MiB, Reserved: 392.00 MiB\n",
            "batch loss:  1.8550746440887451\n",
            "After processing batch 27 - Allocated: 66.33 MiB, Reserved: 462.00 MiB\n",
            "batch loss:  1.8073971271514893\n",
            "After processing batch 28 - Allocated: 67.16 MiB, Reserved: 460.00 MiB\n",
            "batch loss:  1.807374119758606\n",
            "After processing batch 29 - Allocated: 66.70 MiB, Reserved: 416.00 MiB\n",
            "batch loss:  1.8802986145019531\n",
            "After processing batch 30 - Allocated: 66.63 MiB, Reserved: 426.00 MiB\n",
            "batch loss:  1.8103176355361938\n",
            "After processing batch 31 - Allocated: 66.19 MiB, Reserved: 404.00 MiB\n",
            "batch loss:  1.8388065099716187\n",
            "After processing batch 32 - Allocated: 66.60 MiB, Reserved: 394.00 MiB\n",
            "batch loss:  1.8370624780654907\n",
            "After processing batch 33 - Allocated: 66.72 MiB, Reserved: 422.00 MiB\n",
            "batch loss:  1.8741936683654785\n",
            "After processing batch 34 - Allocated: 66.76 MiB, Reserved: 412.00 MiB\n",
            "batch loss:  1.8298321962356567\n",
            "After processing batch 35 - Allocated: 66.71 MiB, Reserved: 398.00 MiB\n",
            "batch loss:  1.8189014196395874\n",
            "After processing batch 36 - Allocated: 66.74 MiB, Reserved: 442.00 MiB\n",
            "batch loss:  1.8363778591156006\n",
            "After processing batch 37 - Allocated: 66.89 MiB, Reserved: 402.00 MiB\n",
            "batch loss:  1.8716374635696411\n",
            "After processing batch 38 - Allocated: 66.19 MiB, Reserved: 430.00 MiB\n",
            "batch loss:  1.863411784172058\n",
            "After processing batch 39 - Allocated: 66.38 MiB, Reserved: 398.00 MiB\n",
            "batch loss:  1.8111610412597656\n",
            "After processing batch 40 - Allocated: 66.72 MiB, Reserved: 410.00 MiB\n",
            "batch loss:  1.813488245010376\n",
            "After processing batch 41 - Allocated: 66.47 MiB, Reserved: 408.00 MiB\n",
            "batch loss:  1.8084079027175903\n",
            "After processing batch 42 - Allocated: 66.47 MiB, Reserved: 470.00 MiB\n",
            "batch loss:  1.847511649131775\n",
            "After processing batch 43 - Allocated: 66.52 MiB, Reserved: 464.00 MiB\n",
            "batch loss:  1.8195035457611084\n",
            "After processing batch 44 - Allocated: 66.12 MiB, Reserved: 408.00 MiB\n",
            "batch loss:  1.824152946472168\n",
            "After processing batch 45 - Allocated: 66.38 MiB, Reserved: 452.00 MiB\n",
            "batch loss:  1.8750252723693848\n",
            "After processing batch 46 - Allocated: 66.62 MiB, Reserved: 472.00 MiB\n",
            "batch loss:  1.8063820600509644\n",
            "After processing batch 47 - Allocated: 66.42 MiB, Reserved: 474.00 MiB\n",
            "batch loss:  1.8556791543960571\n",
            "After processing batch 48 - Allocated: 66.61 MiB, Reserved: 474.00 MiB\n",
            "batch loss:  1.795741081237793\n",
            "After processing batch 49 - Allocated: 67.24 MiB, Reserved: 412.00 MiB\n",
            "batch loss:  1.8490606546401978\n",
            "After processing batch 50 - Allocated: 66.55 MiB, Reserved: 478.00 MiB\n",
            "batch loss:  1.8318935632705688\n",
            "After processing batch 51 - Allocated: 66.43 MiB, Reserved: 452.00 MiB\n",
            "batch loss:  1.8264923095703125\n",
            "After processing batch 52 - Allocated: 66.50 MiB, Reserved: 448.00 MiB\n",
            "batch loss:  1.8312641382217407\n",
            "After processing batch 53 - Allocated: 66.86 MiB, Reserved: 438.00 MiB\n",
            "batch loss:  1.8072550296783447\n",
            "After processing batch 54 - Allocated: 66.92 MiB, Reserved: 420.00 MiB\n",
            "batch loss:  1.8294970989227295\n",
            "After processing batch 55 - Allocated: 66.29 MiB, Reserved: 414.00 MiB\n",
            "batch loss:  1.8288946151733398\n",
            "After processing batch 56 - Allocated: 66.66 MiB, Reserved: 432.00 MiB\n",
            "batch loss:  1.813049554824829\n",
            "After processing batch 57 - Allocated: 66.74 MiB, Reserved: 420.00 MiB\n",
            "batch loss:  1.8337857723236084\n",
            "After processing batch 58 - Allocated: 66.14 MiB, Reserved: 468.00 MiB\n",
            "batch loss:  1.7926961183547974\n",
            "After processing batch 59 - Allocated: 66.10 MiB, Reserved: 384.00 MiB\n",
            "batch loss:  1.8102983236312866\n",
            "After processing batch 60 - Allocated: 66.36 MiB, Reserved: 452.00 MiB\n",
            "batch loss:  1.809149146080017\n",
            "After processing batch 61 - Allocated: 66.68 MiB, Reserved: 410.00 MiB\n",
            "----------------------EPOCH 1, TOTAL LOSS = 1.2155279343326886\n",
            "batch loss:  1.7829644680023193\n",
            "After processing batch 0 - Allocated: 66.40 MiB, Reserved: 394.00 MiB\n",
            "batch loss:  1.812233567237854\n",
            "After processing batch 1 - Allocated: 66.61 MiB, Reserved: 468.00 MiB\n",
            "batch loss:  1.82859206199646\n",
            "After processing batch 2 - Allocated: 66.79 MiB, Reserved: 446.00 MiB\n",
            "batch loss:  1.8396718502044678\n",
            "After processing batch 3 - Allocated: 66.46 MiB, Reserved: 400.00 MiB\n",
            "batch loss:  1.8027805089950562\n",
            "After processing batch 4 - Allocated: 66.19 MiB, Reserved: 442.00 MiB\n",
            "batch loss:  1.8457306623458862\n",
            "After processing batch 5 - Allocated: 66.76 MiB, Reserved: 494.00 MiB\n",
            "batch loss:  1.8105741739273071\n",
            "After processing batch 6 - Allocated: 67.08 MiB, Reserved: 412.00 MiB\n",
            "batch loss:  1.8142495155334473\n",
            "After processing batch 7 - Allocated: 66.19 MiB, Reserved: 394.00 MiB\n",
            "batch loss:  1.8030427694320679\n",
            "After processing batch 8 - Allocated: 66.39 MiB, Reserved: 462.00 MiB\n",
            "batch loss:  1.835302710533142\n",
            "After processing batch 9 - Allocated: 67.22 MiB, Reserved: 464.00 MiB\n",
            "batch loss:  1.7500965595245361\n",
            "After processing batch 10 - Allocated: 66.70 MiB, Reserved: 472.00 MiB\n",
            "batch loss:  1.7663787603378296\n",
            "After processing batch 11 - Allocated: 66.30 MiB, Reserved: 454.00 MiB\n",
            "batch loss:  1.8232008218765259\n",
            "After processing batch 12 - Allocated: 66.75 MiB, Reserved: 408.00 MiB\n",
            "batch loss:  1.828092098236084\n",
            "After processing batch 13 - Allocated: 66.43 MiB, Reserved: 410.00 MiB\n",
            "batch loss:  1.811739444732666\n",
            "After processing batch 14 - Allocated: 66.41 MiB, Reserved: 436.00 MiB\n",
            "batch loss:  1.8645063638687134\n",
            "After processing batch 15 - Allocated: 66.31 MiB, Reserved: 412.00 MiB\n",
            "batch loss:  1.8144736289978027\n",
            "After processing batch 16 - Allocated: 66.76 MiB, Reserved: 474.00 MiB\n",
            "batch loss:  1.7582464218139648\n",
            "After processing batch 17 - Allocated: 66.23 MiB, Reserved: 426.00 MiB\n",
            "batch loss:  1.8494409322738647\n",
            "After processing batch 18 - Allocated: 66.46 MiB, Reserved: 422.00 MiB\n",
            "batch loss:  1.8168065547943115\n",
            "After processing batch 19 - Allocated: 66.62 MiB, Reserved: 452.00 MiB\n",
            "batch loss:  1.8096864223480225\n",
            "After processing batch 20 - Allocated: 66.85 MiB, Reserved: 488.00 MiB\n",
            "batch loss:  1.7916027307510376\n",
            "After processing batch 21 - Allocated: 66.70 MiB, Reserved: 444.00 MiB\n",
            "batch loss:  1.8477903604507446\n",
            "After processing batch 22 - Allocated: 67.06 MiB, Reserved: 472.00 MiB\n",
            "batch loss:  1.8375918865203857\n",
            "After processing batch 23 - Allocated: 66.67 MiB, Reserved: 412.00 MiB\n",
            "batch loss:  1.772761344909668\n",
            "After processing batch 24 - Allocated: 66.06 MiB, Reserved: 466.00 MiB\n",
            "batch loss:  1.8488689661026\n",
            "After processing batch 25 - Allocated: 66.51 MiB, Reserved: 460.00 MiB\n",
            "batch loss:  1.809706687927246\n",
            "After processing batch 26 - Allocated: 66.55 MiB, Reserved: 464.00 MiB\n",
            "batch loss:  1.8218929767608643\n",
            "After processing batch 27 - Allocated: 66.29 MiB, Reserved: 498.00 MiB\n",
            "batch loss:  1.8575297594070435\n",
            "After processing batch 28 - Allocated: 66.80 MiB, Reserved: 420.00 MiB\n",
            "batch loss:  1.7370681762695312\n",
            "After processing batch 29 - Allocated: 66.10 MiB, Reserved: 384.00 MiB\n",
            "batch loss:  1.855804443359375\n",
            "After processing batch 30 - Allocated: 66.43 MiB, Reserved: 480.00 MiB\n",
            "batch loss:  1.8186113834381104\n",
            "After processing batch 31 - Allocated: 66.47 MiB, Reserved: 426.00 MiB\n",
            "batch loss:  1.8245408535003662\n",
            "After processing batch 32 - Allocated: 66.95 MiB, Reserved: 388.00 MiB\n",
            "batch loss:  1.8133938312530518\n",
            "After processing batch 33 - Allocated: 66.38 MiB, Reserved: 408.00 MiB\n",
            "batch loss:  1.8419268131256104\n",
            "After processing batch 34 - Allocated: 66.60 MiB, Reserved: 432.00 MiB\n",
            "batch loss:  1.7851121425628662\n",
            "After processing batch 35 - Allocated: 66.43 MiB, Reserved: 408.00 MiB\n",
            "batch loss:  1.8597412109375\n",
            "After processing batch 36 - Allocated: 66.52 MiB, Reserved: 430.00 MiB\n",
            "batch loss:  1.7574063539505005\n",
            "After processing batch 37 - Allocated: 66.94 MiB, Reserved: 428.00 MiB\n",
            "batch loss:  1.8305408954620361\n",
            "After processing batch 38 - Allocated: 66.12 MiB, Reserved: 458.00 MiB\n",
            "batch loss:  1.8098106384277344\n",
            "After processing batch 39 - Allocated: 66.75 MiB, Reserved: 406.00 MiB\n",
            "batch loss:  1.7572529315948486\n",
            "After processing batch 40 - Allocated: 66.59 MiB, Reserved: 496.00 MiB\n",
            "batch loss:  1.8690989017486572\n",
            "After processing batch 41 - Allocated: 67.01 MiB, Reserved: 402.00 MiB\n",
            "batch loss:  1.7306771278381348\n",
            "After processing batch 42 - Allocated: 66.48 MiB, Reserved: 458.00 MiB\n",
            "batch loss:  1.8574934005737305\n",
            "After processing batch 43 - Allocated: 66.65 MiB, Reserved: 412.00 MiB\n",
            "batch loss:  1.8272221088409424\n",
            "After processing batch 44 - Allocated: 67.07 MiB, Reserved: 474.00 MiB\n",
            "batch loss:  1.8371185064315796\n",
            "After processing batch 45 - Allocated: 66.55 MiB, Reserved: 466.00 MiB\n",
            "batch loss:  1.817022442817688\n",
            "After processing batch 46 - Allocated: 66.16 MiB, Reserved: 448.00 MiB\n",
            "batch loss:  1.8151572942733765\n",
            "After processing batch 47 - Allocated: 66.66 MiB, Reserved: 440.00 MiB\n",
            "batch loss:  1.8113288879394531\n",
            "After processing batch 48 - Allocated: 66.34 MiB, Reserved: 424.00 MiB\n",
            "batch loss:  1.8280770778656006\n",
            "After processing batch 49 - Allocated: 66.58 MiB, Reserved: 494.00 MiB\n",
            "batch loss:  1.8082773685455322\n",
            "After processing batch 50 - Allocated: 66.60 MiB, Reserved: 424.00 MiB\n",
            "batch loss:  1.833041787147522\n",
            "After processing batch 51 - Allocated: 66.80 MiB, Reserved: 450.00 MiB\n",
            "batch loss:  1.792911171913147\n",
            "After processing batch 52 - Allocated: 66.23 MiB, Reserved: 444.00 MiB\n",
            "batch loss:  1.8279746770858765\n",
            "After processing batch 53 - Allocated: 66.86 MiB, Reserved: 472.00 MiB\n",
            "batch loss:  1.831022024154663\n",
            "After processing batch 54 - Allocated: 66.35 MiB, Reserved: 468.00 MiB\n",
            "batch loss:  1.7606323957443237\n",
            "After processing batch 55 - Allocated: 66.70 MiB, Reserved: 470.00 MiB\n",
            "batch loss:  1.8431578874588013\n",
            "After processing batch 56 - Allocated: 66.98 MiB, Reserved: 498.00 MiB\n",
            "batch loss:  1.7793246507644653\n",
            "After processing batch 57 - Allocated: 66.46 MiB, Reserved: 462.00 MiB\n",
            "batch loss:  1.7693148851394653\n",
            "After processing batch 58 - Allocated: 77.06 MiB, Reserved: 474.00 MiB\n",
            "batch loss:  1.8407062292099\n",
            "After processing batch 59 - Allocated: 66.66 MiB, Reserved: 386.00 MiB\n",
            "batch loss:  1.8116806745529175\n",
            "After processing batch 60 - Allocated: 66.87 MiB, Reserved: 472.00 MiB\n",
            "batch loss:  1.748272180557251\n",
            "After processing batch 61 - Allocated: 66.17 MiB, Reserved: 390.00 MiB\n",
            "----------------------EPOCH 2, TOTAL LOSS = 1.1706903787950675\n",
            "batch loss:  1.784382939338684\n",
            "After processing batch 0 - Allocated: 66.26 MiB, Reserved: 456.00 MiB\n",
            "batch loss:  1.8027137517929077\n",
            "After processing batch 1 - Allocated: 66.48 MiB, Reserved: 418.00 MiB\n",
            "batch loss:  1.821961522102356\n",
            "After processing batch 2 - Allocated: 66.55 MiB, Reserved: 470.00 MiB\n",
            "batch loss:  1.7956697940826416\n",
            "After processing batch 3 - Allocated: 66.32 MiB, Reserved: 496.00 MiB\n",
            "batch loss:  1.817141056060791\n",
            "After processing batch 4 - Allocated: 66.28 MiB, Reserved: 460.00 MiB\n",
            "batch loss:  1.8120956420898438\n",
            "After processing batch 5 - Allocated: 66.85 MiB, Reserved: 440.00 MiB\n",
            "batch loss:  1.7883330583572388\n",
            "After processing batch 6 - Allocated: 66.53 MiB, Reserved: 446.00 MiB\n",
            "batch loss:  1.806474208831787\n",
            "After processing batch 7 - Allocated: 66.47 MiB, Reserved: 500.00 MiB\n",
            "batch loss:  1.7784358263015747\n",
            "After processing batch 8 - Allocated: 66.11 MiB, Reserved: 404.00 MiB\n",
            "batch loss:  1.8376837968826294\n",
            "After processing batch 9 - Allocated: 66.13 MiB, Reserved: 446.00 MiB\n",
            "batch loss:  1.7980660200119019\n",
            "After processing batch 10 - Allocated: 66.47 MiB, Reserved: 410.00 MiB\n",
            "batch loss:  1.8080124855041504\n",
            "After processing batch 11 - Allocated: 66.74 MiB, Reserved: 410.00 MiB\n",
            "batch loss:  1.8058269023895264\n",
            "After processing batch 12 - Allocated: 66.49 MiB, Reserved: 418.00 MiB\n",
            "batch loss:  1.7958353757858276\n",
            "After processing batch 13 - Allocated: 66.59 MiB, Reserved: 392.00 MiB\n",
            "batch loss:  1.7863494157791138\n",
            "After processing batch 14 - Allocated: 66.35 MiB, Reserved: 408.00 MiB\n",
            "batch loss:  1.7760370969772339\n",
            "After processing batch 15 - Allocated: 66.16 MiB, Reserved: 426.00 MiB\n",
            "batch loss:  1.7866566181182861\n",
            "After processing batch 16 - Allocated: 66.35 MiB, Reserved: 470.00 MiB\n",
            "batch loss:  1.8024442195892334\n",
            "After processing batch 17 - Allocated: 66.78 MiB, Reserved: 414.00 MiB\n",
            "batch loss:  1.8321131467819214\n",
            "After processing batch 18 - Allocated: 66.29 MiB, Reserved: 430.00 MiB\n",
            "batch loss:  1.7954655885696411\n",
            "After processing batch 19 - Allocated: 67.06 MiB, Reserved: 468.00 MiB\n",
            "batch loss:  1.8042854070663452\n",
            "After processing batch 20 - Allocated: 67.08 MiB, Reserved: 468.00 MiB\n",
            "batch loss:  1.8176556825637817\n",
            "After processing batch 21 - Allocated: 66.64 MiB, Reserved: 484.00 MiB\n",
            "batch loss:  1.7735663652420044\n",
            "After processing batch 22 - Allocated: 66.16 MiB, Reserved: 444.00 MiB\n",
            "batch loss:  1.7971795797348022\n",
            "After processing batch 23 - Allocated: 66.38 MiB, Reserved: 472.00 MiB\n",
            "batch loss:  1.7713379859924316\n",
            "After processing batch 24 - Allocated: 66.35 MiB, Reserved: 470.00 MiB\n",
            "batch loss:  1.8011747598648071\n",
            "After processing batch 25 - Allocated: 66.74 MiB, Reserved: 418.00 MiB\n",
            "batch loss:  1.8344913721084595\n",
            "After processing batch 26 - Allocated: 66.56 MiB, Reserved: 428.00 MiB\n",
            "batch loss:  1.7650073766708374\n",
            "After processing batch 27 - Allocated: 66.76 MiB, Reserved: 470.00 MiB\n",
            "batch loss:  1.7810328006744385\n",
            "After processing batch 28 - Allocated: 66.35 MiB, Reserved: 470.00 MiB\n",
            "batch loss:  1.8521995544433594\n",
            "After processing batch 29 - Allocated: 67.11 MiB, Reserved: 458.00 MiB\n",
            "batch loss:  1.8528178930282593\n",
            "After processing batch 30 - Allocated: 66.09 MiB, Reserved: 452.00 MiB\n",
            "batch loss:  1.7770898342132568\n",
            "After processing batch 31 - Allocated: 66.53 MiB, Reserved: 400.00 MiB\n",
            "batch loss:  1.7782028913497925\n",
            "After processing batch 32 - Allocated: 66.47 MiB, Reserved: 396.00 MiB\n",
            "batch loss:  1.7684378623962402\n",
            "After processing batch 33 - Allocated: 66.21 MiB, Reserved: 438.00 MiB\n",
            "batch loss:  1.7504680156707764\n",
            "After processing batch 34 - Allocated: 66.35 MiB, Reserved: 442.00 MiB\n",
            "batch loss:  1.7919944524765015\n",
            "After processing batch 35 - Allocated: 66.54 MiB, Reserved: 450.00 MiB\n",
            "batch loss:  1.835823655128479\n",
            "After processing batch 36 - Allocated: 66.31 MiB, Reserved: 442.00 MiB\n",
            "batch loss:  1.800138235092163\n",
            "After processing batch 37 - Allocated: 66.94 MiB, Reserved: 392.00 MiB\n",
            "batch loss:  1.7765324115753174\n",
            "After processing batch 38 - Allocated: 66.82 MiB, Reserved: 418.00 MiB\n",
            "batch loss:  1.7703275680541992\n",
            "After processing batch 39 - Allocated: 66.44 MiB, Reserved: 446.00 MiB\n",
            "batch loss:  1.7918219566345215\n",
            "After processing batch 40 - Allocated: 66.44 MiB, Reserved: 438.00 MiB\n",
            "batch loss:  1.781680703163147\n",
            "After processing batch 41 - Allocated: 66.59 MiB, Reserved: 410.00 MiB\n",
            "batch loss:  1.7500592470169067\n",
            "After processing batch 42 - Allocated: 66.51 MiB, Reserved: 392.00 MiB\n",
            "batch loss:  1.8116097450256348\n",
            "After processing batch 43 - Allocated: 66.96 MiB, Reserved: 380.00 MiB\n",
            "batch loss:  1.77605402469635\n",
            "After processing batch 44 - Allocated: 66.16 MiB, Reserved: 428.00 MiB\n",
            "batch loss:  1.788063883781433\n",
            "After processing batch 45 - Allocated: 66.47 MiB, Reserved: 484.00 MiB\n",
            "batch loss:  1.7897793054580688\n",
            "After processing batch 46 - Allocated: 66.84 MiB, Reserved: 420.00 MiB\n",
            "batch loss:  1.8249534368515015\n",
            "After processing batch 47 - Allocated: 67.04 MiB, Reserved: 452.00 MiB\n",
            "batch loss:  1.8008325099945068\n",
            "After processing batch 48 - Allocated: 66.16 MiB, Reserved: 466.00 MiB\n",
            "batch loss:  1.7745813131332397\n",
            "After processing batch 49 - Allocated: 66.65 MiB, Reserved: 402.00 MiB\n",
            "batch loss:  1.8245769739151\n",
            "After processing batch 50 - Allocated: 66.46 MiB, Reserved: 398.00 MiB\n",
            "batch loss:  1.8180078268051147\n",
            "After processing batch 51 - Allocated: 67.08 MiB, Reserved: 404.00 MiB\n",
            "batch loss:  1.8636808395385742\n",
            "After processing batch 52 - Allocated: 66.44 MiB, Reserved: 476.00 MiB\n",
            "batch loss:  1.7922297716140747\n",
            "After processing batch 53 - Allocated: 66.14 MiB, Reserved: 420.00 MiB\n",
            "batch loss:  1.7914472818374634\n",
            "After processing batch 54 - Allocated: 66.54 MiB, Reserved: 412.00 MiB\n",
            "batch loss:  1.8075196743011475\n",
            "After processing batch 55 - Allocated: 66.31 MiB, Reserved: 438.00 MiB\n",
            "batch loss:  1.8069857358932495\n",
            "After processing batch 56 - Allocated: 66.47 MiB, Reserved: 398.00 MiB\n",
            "batch loss:  1.7968541383743286\n",
            "After processing batch 57 - Allocated: 66.33 MiB, Reserved: 408.00 MiB\n",
            "batch loss:  1.8070341348648071\n",
            "After processing batch 58 - Allocated: 66.78 MiB, Reserved: 434.00 MiB\n",
            "batch loss:  1.8030709028244019\n",
            "After processing batch 59 - Allocated: 66.52 MiB, Reserved: 444.00 MiB\n",
            "batch loss:  1.815277099609375\n",
            "After processing batch 60 - Allocated: 66.98 MiB, Reserved: 424.00 MiB\n",
            "batch loss:  1.7652593851089478\n",
            "After processing batch 61 - Allocated: 66.11 MiB, Reserved: 392.00 MiB\n",
            "----------------------EPOCH 3, TOTAL LOSS = 1.1615921047826607\n",
            "batch loss:  1.7764892578125\n",
            "After processing batch 0 - Allocated: 66.85 MiB, Reserved: 416.00 MiB\n",
            "batch loss:  1.8091163635253906\n",
            "After processing batch 1 - Allocated: 66.41 MiB, Reserved: 386.00 MiB\n",
            "batch loss:  1.8413892984390259\n",
            "After processing batch 2 - Allocated: 66.89 MiB, Reserved: 442.00 MiB\n",
            "batch loss:  1.7676608562469482\n",
            "After processing batch 3 - Allocated: 66.39 MiB, Reserved: 402.00 MiB\n",
            "batch loss:  1.778151512145996\n",
            "After processing batch 4 - Allocated: 66.11 MiB, Reserved: 446.00 MiB\n",
            "batch loss:  1.7742294073104858\n",
            "After processing batch 5 - Allocated: 67.02 MiB, Reserved: 410.00 MiB\n",
            "batch loss:  1.7487411499023438\n",
            "After processing batch 6 - Allocated: 66.18 MiB, Reserved: 394.00 MiB\n",
            "batch loss:  1.7835004329681396\n",
            "After processing batch 7 - Allocated: 66.92 MiB, Reserved: 400.00 MiB\n",
            "batch loss:  1.7807824611663818\n",
            "After processing batch 8 - Allocated: 66.98 MiB, Reserved: 398.00 MiB\n",
            "batch loss:  1.8206454515457153\n",
            "After processing batch 9 - Allocated: 66.27 MiB, Reserved: 436.00 MiB\n",
            "batch loss:  1.7358498573303223\n",
            "After processing batch 10 - Allocated: 66.79 MiB, Reserved: 450.00 MiB\n",
            "batch loss:  1.757900595664978\n",
            "After processing batch 11 - Allocated: 66.35 MiB, Reserved: 438.00 MiB\n",
            "batch loss:  1.7534615993499756\n",
            "After processing batch 12 - Allocated: 66.43 MiB, Reserved: 482.00 MiB\n",
            "batch loss:  1.7304822206497192\n",
            "After processing batch 13 - Allocated: 66.66 MiB, Reserved: 440.00 MiB\n",
            "batch loss:  1.8045525550842285\n",
            "After processing batch 14 - Allocated: 66.36 MiB, Reserved: 370.00 MiB\n",
            "batch loss:  1.766360878944397\n",
            "After processing batch 15 - Allocated: 66.79 MiB, Reserved: 452.00 MiB\n",
            "batch loss:  1.8022838830947876\n",
            "After processing batch 16 - Allocated: 66.63 MiB, Reserved: 436.00 MiB\n",
            "batch loss:  1.7884858846664429\n",
            "After processing batch 17 - Allocated: 66.70 MiB, Reserved: 432.00 MiB\n",
            "batch loss:  1.8122329711914062\n",
            "After processing batch 18 - Allocated: 66.52 MiB, Reserved: 422.00 MiB\n",
            "batch loss:  1.7497243881225586\n",
            "After processing batch 19 - Allocated: 66.98 MiB, Reserved: 428.00 MiB\n",
            "batch loss:  1.7955735921859741\n",
            "After processing batch 20 - Allocated: 66.08 MiB, Reserved: 424.00 MiB\n",
            "batch loss:  1.7902699708938599\n",
            "After processing batch 21 - Allocated: 66.63 MiB, Reserved: 426.00 MiB\n",
            "batch loss:  1.7502635717391968\n",
            "After processing batch 22 - Allocated: 66.78 MiB, Reserved: 434.00 MiB\n",
            "batch loss:  1.7624505758285522\n",
            "After processing batch 23 - Allocated: 66.21 MiB, Reserved: 426.00 MiB\n",
            "batch loss:  1.7938146591186523\n",
            "After processing batch 24 - Allocated: 66.93 MiB, Reserved: 476.00 MiB\n",
            "batch loss:  1.7749214172363281\n",
            "After processing batch 25 - Allocated: 66.19 MiB, Reserved: 390.00 MiB\n",
            "batch loss:  1.7705405950546265\n",
            "After processing batch 26 - Allocated: 66.33 MiB, Reserved: 446.00 MiB\n",
            "batch loss:  1.820534348487854\n",
            "After processing batch 27 - Allocated: 66.40 MiB, Reserved: 442.00 MiB\n",
            "batch loss:  1.7721052169799805\n",
            "After processing batch 28 - Allocated: 66.44 MiB, Reserved: 482.00 MiB\n",
            "batch loss:  1.8106446266174316\n",
            "After processing batch 29 - Allocated: 66.92 MiB, Reserved: 468.00 MiB\n",
            "batch loss:  1.7842700481414795\n",
            "After processing batch 30 - Allocated: 66.98 MiB, Reserved: 390.00 MiB\n",
            "batch loss:  1.7446576356887817\n",
            "After processing batch 31 - Allocated: 66.06 MiB, Reserved: 414.00 MiB\n",
            "batch loss:  1.7793103456497192\n",
            "After processing batch 32 - Allocated: 66.76 MiB, Reserved: 484.00 MiB\n",
            "batch loss:  1.7916409969329834\n",
            "After processing batch 33 - Allocated: 66.17 MiB, Reserved: 398.00 MiB\n",
            "batch loss:  1.7652658224105835\n",
            "After processing batch 34 - Allocated: 66.45 MiB, Reserved: 420.00 MiB\n",
            "batch loss:  1.7667162418365479\n",
            "After processing batch 35 - Allocated: 66.92 MiB, Reserved: 432.00 MiB\n",
            "batch loss:  1.8056299686431885\n",
            "After processing batch 36 - Allocated: 66.79 MiB, Reserved: 458.00 MiB\n",
            "batch loss:  1.8119632005691528\n",
            "After processing batch 37 - Allocated: 66.32 MiB, Reserved: 508.00 MiB\n",
            "batch loss:  1.7851731777191162\n",
            "After processing batch 38 - Allocated: 66.60 MiB, Reserved: 420.00 MiB\n",
            "batch loss:  1.7884174585342407\n",
            "After processing batch 39 - Allocated: 67.04 MiB, Reserved: 396.00 MiB\n",
            "batch loss:  1.7986267805099487\n",
            "After processing batch 40 - Allocated: 66.92 MiB, Reserved: 408.00 MiB\n",
            "batch loss:  1.813685417175293\n",
            "After processing batch 41 - Allocated: 66.17 MiB, Reserved: 438.00 MiB\n",
            "batch loss:  1.78533136844635\n",
            "After processing batch 42 - Allocated: 66.44 MiB, Reserved: 448.00 MiB\n",
            "batch loss:  1.7723139524459839\n",
            "After processing batch 43 - Allocated: 66.75 MiB, Reserved: 418.00 MiB\n",
            "batch loss:  1.788152813911438\n",
            "After processing batch 44 - Allocated: 66.49 MiB, Reserved: 498.00 MiB\n",
            "batch loss:  1.8307033777236938\n",
            "After processing batch 45 - Allocated: 66.75 MiB, Reserved: 466.00 MiB\n",
            "batch loss:  1.7763466835021973\n",
            "After processing batch 46 - Allocated: 66.34 MiB, Reserved: 428.00 MiB\n",
            "batch loss:  1.7689437866210938\n",
            "After processing batch 47 - Allocated: 66.80 MiB, Reserved: 446.00 MiB\n",
            "batch loss:  1.7890161275863647\n",
            "After processing batch 48 - Allocated: 67.06 MiB, Reserved: 478.00 MiB\n",
            "batch loss:  1.7565110921859741\n",
            "After processing batch 49 - Allocated: 66.50 MiB, Reserved: 434.00 MiB\n",
            "batch loss:  1.786476969718933\n",
            "After processing batch 50 - Allocated: 66.45 MiB, Reserved: 410.00 MiB\n",
            "batch loss:  1.7590276002883911\n",
            "After processing batch 51 - Allocated: 66.71 MiB, Reserved: 412.00 MiB\n",
            "batch loss:  1.7985070943832397\n",
            "After processing batch 52 - Allocated: 66.46 MiB, Reserved: 416.00 MiB\n",
            "batch loss:  1.74726402759552\n",
            "After processing batch 53 - Allocated: 66.33 MiB, Reserved: 448.00 MiB\n",
            "batch loss:  1.7795535326004028\n",
            "After processing batch 54 - Allocated: 66.46 MiB, Reserved: 466.00 MiB\n",
            "batch loss:  1.7282356023788452\n",
            "After processing batch 55 - Allocated: 66.99 MiB, Reserved: 498.00 MiB\n",
            "batch loss:  1.7827779054641724\n",
            "After processing batch 56 - Allocated: 66.55 MiB, Reserved: 414.00 MiB\n",
            "batch loss:  1.793056845664978\n",
            "After processing batch 57 - Allocated: 66.43 MiB, Reserved: 486.00 MiB\n",
            "batch loss:  1.8403794765472412\n",
            "After processing batch 58 - Allocated: 67.16 MiB, Reserved: 476.00 MiB\n",
            "batch loss:  1.761586308479309\n",
            "After processing batch 59 - Allocated: 66.66 MiB, Reserved: 390.00 MiB\n",
            "batch loss:  1.797582745552063\n",
            "After processing batch 60 - Allocated: 66.27 MiB, Reserved: 456.00 MiB\n",
            "batch loss:  1.7918002605438232\n",
            "After processing batch 61 - Allocated: 66.71 MiB, Reserved: 436.00 MiB\n",
            "----------------------EPOCH 4, TOTAL LOSS = 1.1509592110912006\n",
            "batch loss:  1.7867587804794312\n",
            "After processing batch 0 - Allocated: 66.93 MiB, Reserved: 416.00 MiB\n",
            "batch loss:  1.7704877853393555\n",
            "After processing batch 1 - Allocated: 66.20 MiB, Reserved: 426.00 MiB\n",
            "batch loss:  1.7587268352508545\n",
            "After processing batch 2 - Allocated: 66.70 MiB, Reserved: 502.00 MiB\n",
            "batch loss:  1.755875825881958\n",
            "After processing batch 3 - Allocated: 66.18 MiB, Reserved: 398.00 MiB\n",
            "batch loss:  1.7457119226455688\n",
            "After processing batch 4 - Allocated: 66.95 MiB, Reserved: 394.00 MiB\n",
            "batch loss:  1.7710193395614624\n",
            "After processing batch 5 - Allocated: 66.14 MiB, Reserved: 474.00 MiB\n",
            "batch loss:  1.8239467144012451\n",
            "After processing batch 6 - Allocated: 66.69 MiB, Reserved: 408.00 MiB\n",
            "batch loss:  1.7698273658752441\n",
            "After processing batch 7 - Allocated: 66.26 MiB, Reserved: 452.00 MiB\n",
            "batch loss:  1.7434319257736206\n",
            "After processing batch 8 - Allocated: 66.43 MiB, Reserved: 410.00 MiB\n",
            "batch loss:  1.7433221340179443\n",
            "After processing batch 9 - Allocated: 66.80 MiB, Reserved: 402.00 MiB\n",
            "batch loss:  1.7492997646331787\n",
            "After processing batch 10 - Allocated: 66.91 MiB, Reserved: 384.00 MiB\n",
            "batch loss:  1.7827329635620117\n",
            "After processing batch 11 - Allocated: 66.84 MiB, Reserved: 432.00 MiB\n",
            "batch loss:  1.8085174560546875\n",
            "After processing batch 12 - Allocated: 66.53 MiB, Reserved: 410.00 MiB\n",
            "batch loss:  1.777083158493042\n",
            "After processing batch 13 - Allocated: 66.55 MiB, Reserved: 448.00 MiB\n",
            "batch loss:  1.8195871114730835\n",
            "After processing batch 14 - Allocated: 66.71 MiB, Reserved: 414.00 MiB\n",
            "batch loss:  1.7450892925262451\n",
            "After processing batch 15 - Allocated: 67.06 MiB, Reserved: 420.00 MiB\n",
            "batch loss:  1.783414363861084\n",
            "After processing batch 16 - Allocated: 66.89 MiB, Reserved: 430.00 MiB\n",
            "batch loss:  1.752597451210022\n",
            "After processing batch 17 - Allocated: 66.59 MiB, Reserved: 426.00 MiB\n",
            "batch loss:  1.7996063232421875\n",
            "After processing batch 18 - Allocated: 66.59 MiB, Reserved: 462.00 MiB\n",
            "batch loss:  1.8125935792922974\n",
            "After processing batch 19 - Allocated: 66.43 MiB, Reserved: 470.00 MiB\n",
            "batch loss:  1.7875709533691406\n",
            "After processing batch 20 - Allocated: 66.65 MiB, Reserved: 486.00 MiB\n",
            "batch loss:  1.7793710231781006\n",
            "After processing batch 21 - Allocated: 66.85 MiB, Reserved: 450.00 MiB\n",
            "batch loss:  1.7838399410247803\n",
            "After processing batch 22 - Allocated: 66.70 MiB, Reserved: 460.00 MiB\n",
            "batch loss:  1.7757400274276733\n",
            "After processing batch 23 - Allocated: 66.12 MiB, Reserved: 406.00 MiB\n",
            "batch loss:  1.764382243156433\n",
            "After processing batch 24 - Allocated: 66.21 MiB, Reserved: 426.00 MiB\n",
            "batch loss:  1.7637457847595215\n",
            "After processing batch 25 - Allocated: 66.89 MiB, Reserved: 398.00 MiB\n",
            "batch loss:  1.7795426845550537\n",
            "After processing batch 26 - Allocated: 66.38 MiB, Reserved: 404.00 MiB\n",
            "batch loss:  1.7755237817764282\n",
            "After processing batch 27 - Allocated: 66.20 MiB, Reserved: 420.00 MiB\n",
            "batch loss:  1.7578691244125366\n",
            "After processing batch 28 - Allocated: 66.66 MiB, Reserved: 394.00 MiB\n",
            "batch loss:  1.814545750617981\n",
            "After processing batch 29 - Allocated: 66.31 MiB, Reserved: 422.00 MiB\n",
            "batch loss:  1.8175253868103027\n",
            "After processing batch 30 - Allocated: 66.85 MiB, Reserved: 468.00 MiB\n",
            "batch loss:  1.7999728918075562\n",
            "After processing batch 31 - Allocated: 66.14 MiB, Reserved: 414.00 MiB\n",
            "batch loss:  1.77280592918396\n",
            "After processing batch 32 - Allocated: 66.38 MiB, Reserved: 442.00 MiB\n",
            "batch loss:  1.7678580284118652\n",
            "After processing batch 33 - Allocated: 66.97 MiB, Reserved: 438.00 MiB\n",
            "batch loss:  1.7988735437393188\n",
            "After processing batch 34 - Allocated: 66.68 MiB, Reserved: 414.00 MiB\n",
            "batch loss:  1.7506322860717773\n",
            "After processing batch 35 - Allocated: 66.14 MiB, Reserved: 406.00 MiB\n",
            "batch loss:  1.8038440942764282\n",
            "After processing batch 36 - Allocated: 66.32 MiB, Reserved: 452.00 MiB\n",
            "batch loss:  1.7142689228057861\n",
            "After processing batch 37 - Allocated: 66.43 MiB, Reserved: 434.00 MiB\n",
            "batch loss:  1.71721613407135\n",
            "After processing batch 38 - Allocated: 66.06 MiB, Reserved: 436.00 MiB\n",
            "batch loss:  1.8312374353408813\n",
            "After processing batch 39 - Allocated: 66.69 MiB, Reserved: 510.00 MiB\n",
            "batch loss:  1.8241817951202393\n",
            "After processing batch 40 - Allocated: 67.02 MiB, Reserved: 388.00 MiB\n",
            "batch loss:  1.7918627262115479\n",
            "After processing batch 41 - Allocated: 66.49 MiB, Reserved: 414.00 MiB\n",
            "batch loss:  1.7719762325286865\n",
            "After processing batch 42 - Allocated: 67.18 MiB, Reserved: 446.00 MiB\n",
            "batch loss:  1.8081241846084595\n",
            "After processing batch 43 - Allocated: 67.08 MiB, Reserved: 406.00 MiB\n",
            "batch loss:  1.7744299173355103\n",
            "After processing batch 44 - Allocated: 67.05 MiB, Reserved: 492.00 MiB\n",
            "batch loss:  1.768338680267334\n",
            "After processing batch 45 - Allocated: 66.65 MiB, Reserved: 390.00 MiB\n",
            "batch loss:  1.8099757432937622\n",
            "After processing batch 46 - Allocated: 66.39 MiB, Reserved: 388.00 MiB\n",
            "batch loss:  1.7618129253387451\n",
            "After processing batch 47 - Allocated: 66.08 MiB, Reserved: 460.00 MiB\n",
            "batch loss:  1.767297625541687\n",
            "After processing batch 48 - Allocated: 66.48 MiB, Reserved: 426.00 MiB\n",
            "batch loss:  1.7784863710403442\n",
            "After processing batch 49 - Allocated: 66.45 MiB, Reserved: 466.00 MiB\n",
            "batch loss:  1.7599856853485107\n",
            "After processing batch 50 - Allocated: 66.31 MiB, Reserved: 406.00 MiB\n",
            "batch loss:  1.759832739830017\n",
            "After processing batch 51 - Allocated: 66.34 MiB, Reserved: 466.00 MiB\n",
            "batch loss:  1.7617859840393066\n",
            "After processing batch 52 - Allocated: 66.54 MiB, Reserved: 438.00 MiB\n",
            "batch loss:  1.7647963762283325\n",
            "After processing batch 53 - Allocated: 66.56 MiB, Reserved: 438.00 MiB\n",
            "batch loss:  1.8184787034988403\n",
            "After processing batch 54 - Allocated: 66.96 MiB, Reserved: 472.00 MiB\n",
            "batch loss:  1.7972004413604736\n",
            "After processing batch 55 - Allocated: 66.78 MiB, Reserved: 424.00 MiB\n",
            "batch loss:  1.745322823524475\n",
            "After processing batch 56 - Allocated: 66.46 MiB, Reserved: 394.00 MiB\n",
            "batch loss:  1.7621673345565796\n",
            "After processing batch 57 - Allocated: 67.03 MiB, Reserved: 398.00 MiB\n",
            "batch loss:  1.7315993309020996\n",
            "After processing batch 58 - Allocated: 66.09 MiB, Reserved: 434.00 MiB\n",
            "batch loss:  1.7819706201553345\n",
            "After processing batch 59 - Allocated: 66.35 MiB, Reserved: 476.00 MiB\n",
            "batch loss:  1.787021279335022\n",
            "After processing batch 60 - Allocated: 66.67 MiB, Reserved: 424.00 MiB\n",
            "batch loss:  1.7426276206970215\n",
            "After processing batch 61 - Allocated: 66.52 MiB, Reserved: 406.00 MiB\n",
            "----------------------EPOCH 5, TOTAL LOSS = 1.1471382416784763\n",
            "batch loss:  1.8522182703018188\n",
            "After processing batch 0 - Allocated: 67.00 MiB, Reserved: 510.00 MiB\n",
            "batch loss:  1.7753639221191406\n",
            "After processing batch 1 - Allocated: 66.24 MiB, Reserved: 462.00 MiB\n",
            "batch loss:  1.7335946559906006\n",
            "After processing batch 2 - Allocated: 66.28 MiB, Reserved: 410.00 MiB\n",
            "batch loss:  1.7689883708953857\n",
            "After processing batch 3 - Allocated: 66.95 MiB, Reserved: 384.00 MiB\n",
            "batch loss:  1.7411099672317505\n",
            "After processing batch 4 - Allocated: 66.50 MiB, Reserved: 422.00 MiB\n",
            "batch loss:  1.7686488628387451\n",
            "After processing batch 5 - Allocated: 66.24 MiB, Reserved: 422.00 MiB\n",
            "batch loss:  1.8280830383300781\n",
            "After processing batch 6 - Allocated: 67.08 MiB, Reserved: 410.00 MiB\n",
            "batch loss:  1.7443393468856812\n",
            "After processing batch 7 - Allocated: 66.11 MiB, Reserved: 402.00 MiB\n",
            "batch loss:  1.7631579637527466\n",
            "After processing batch 8 - Allocated: 66.65 MiB, Reserved: 484.00 MiB\n",
            "batch loss:  1.7867648601531982\n",
            "After processing batch 9 - Allocated: 66.96 MiB, Reserved: 406.00 MiB\n",
            "batch loss:  1.7622789144515991\n",
            "After processing batch 10 - Allocated: 67.21 MiB, Reserved: 464.00 MiB\n",
            "batch loss:  1.804121494293213\n",
            "After processing batch 11 - Allocated: 66.42 MiB, Reserved: 454.00 MiB\n",
            "batch loss:  1.775376796722412\n",
            "After processing batch 12 - Allocated: 66.29 MiB, Reserved: 414.00 MiB\n",
            "batch loss:  1.7639535665512085\n",
            "After processing batch 13 - Allocated: 66.53 MiB, Reserved: 408.00 MiB\n",
            "batch loss:  1.77924382686615\n",
            "After processing batch 14 - Allocated: 66.22 MiB, Reserved: 452.00 MiB\n",
            "batch loss:  1.7617686986923218\n",
            "After processing batch 15 - Allocated: 66.46 MiB, Reserved: 450.00 MiB\n",
            "batch loss:  1.789084792137146\n",
            "After processing batch 16 - Allocated: 66.42 MiB, Reserved: 452.00 MiB\n",
            "batch loss:  1.6911791563034058\n",
            "After processing batch 17 - Allocated: 67.13 MiB, Reserved: 494.00 MiB\n",
            "batch loss:  1.8249403238296509\n",
            "After processing batch 18 - Allocated: 66.31 MiB, Reserved: 432.00 MiB\n",
            "batch loss:  1.764565348625183\n",
            "After processing batch 19 - Allocated: 66.55 MiB, Reserved: 476.00 MiB\n",
            "batch loss:  1.770194411277771\n",
            "After processing batch 20 - Allocated: 66.67 MiB, Reserved: 474.00 MiB\n",
            "batch loss:  1.8101369142532349\n",
            "After processing batch 21 - Allocated: 66.91 MiB, Reserved: 462.00 MiB\n",
            "batch loss:  1.7656127214431763\n",
            "After processing batch 22 - Allocated: 66.71 MiB, Reserved: 418.00 MiB\n",
            "batch loss:  1.757488489151001\n",
            "After processing batch 23 - Allocated: 66.57 MiB, Reserved: 464.00 MiB\n",
            "batch loss:  1.8040374517440796\n",
            "After processing batch 24 - Allocated: 66.16 MiB, Reserved: 434.00 MiB\n",
            "batch loss:  1.7475124597549438\n",
            "After processing batch 25 - Allocated: 66.61 MiB, Reserved: 408.00 MiB\n",
            "batch loss:  1.79753839969635\n",
            "After processing batch 26 - Allocated: 66.93 MiB, Reserved: 424.00 MiB\n",
            "batch loss:  1.7513052225112915\n",
            "After processing batch 27 - Allocated: 66.56 MiB, Reserved: 438.00 MiB\n",
            "batch loss:  1.8059651851654053\n",
            "After processing batch 28 - Allocated: 66.16 MiB, Reserved: 430.00 MiB\n",
            "batch loss:  1.7565315961837769\n",
            "After processing batch 29 - Allocated: 67.05 MiB, Reserved: 434.00 MiB\n",
            "batch loss:  1.7572507858276367\n",
            "After processing batch 30 - Allocated: 66.46 MiB, Reserved: 454.00 MiB\n",
            "batch loss:  1.7842131853103638\n",
            "After processing batch 31 - Allocated: 66.33 MiB, Reserved: 412.00 MiB\n",
            "batch loss:  1.7458735704421997\n",
            "After processing batch 32 - Allocated: 66.97 MiB, Reserved: 474.00 MiB\n",
            "batch loss:  1.8060871362686157\n",
            "After processing batch 33 - Allocated: 66.16 MiB, Reserved: 440.00 MiB\n",
            "batch loss:  1.7593661546707153\n",
            "After processing batch 34 - Allocated: 66.73 MiB, Reserved: 446.00 MiB\n",
            "batch loss:  1.7993606328964233\n",
            "After processing batch 35 - Allocated: 67.16 MiB, Reserved: 470.00 MiB\n",
            "batch loss:  1.7442359924316406\n",
            "After processing batch 36 - Allocated: 66.45 MiB, Reserved: 408.00 MiB\n",
            "batch loss:  1.8064860105514526\n",
            "After processing batch 37 - Allocated: 66.97 MiB, Reserved: 426.00 MiB\n",
            "batch loss:  1.7621533870697021\n",
            "After processing batch 38 - Allocated: 66.98 MiB, Reserved: 438.00 MiB\n",
            "batch loss:  1.7636442184448242\n",
            "After processing batch 39 - Allocated: 66.37 MiB, Reserved: 400.00 MiB\n",
            "batch loss:  1.7728265523910522\n",
            "After processing batch 40 - Allocated: 66.83 MiB, Reserved: 456.00 MiB\n",
            "batch loss:  1.7701586484909058\n",
            "After processing batch 41 - Allocated: 66.52 MiB, Reserved: 438.00 MiB\n",
            "batch loss:  1.795925498008728\n",
            "After processing batch 42 - Allocated: 66.24 MiB, Reserved: 468.00 MiB\n",
            "batch loss:  1.8061925172805786\n",
            "After processing batch 43 - Allocated: 66.70 MiB, Reserved: 434.00 MiB\n",
            "batch loss:  1.7782853841781616\n",
            "After processing batch 44 - Allocated: 66.31 MiB, Reserved: 424.00 MiB\n",
            "batch loss:  1.7629024982452393\n",
            "After processing batch 45 - Allocated: 66.99 MiB, Reserved: 406.00 MiB\n",
            "batch loss:  1.8001515865325928\n",
            "After processing batch 46 - Allocated: 66.74 MiB, Reserved: 404.00 MiB\n",
            "batch loss:  1.7150588035583496\n",
            "After processing batch 47 - Allocated: 67.07 MiB, Reserved: 480.00 MiB\n",
            "batch loss:  1.751942753791809\n",
            "After processing batch 48 - Allocated: 66.59 MiB, Reserved: 458.00 MiB\n",
            "batch loss:  1.7930846214294434\n",
            "After processing batch 49 - Allocated: 66.83 MiB, Reserved: 410.00 MiB\n",
            "batch loss:  1.7544687986373901\n",
            "After processing batch 50 - Allocated: 66.14 MiB, Reserved: 406.00 MiB\n",
            "batch loss:  1.7713450193405151\n",
            "After processing batch 51 - Allocated: 67.05 MiB, Reserved: 450.00 MiB\n",
            "batch loss:  1.7961008548736572\n",
            "After processing batch 52 - Allocated: 66.92 MiB, Reserved: 406.00 MiB\n",
            "batch loss:  1.7151925563812256\n",
            "After processing batch 53 - Allocated: 66.33 MiB, Reserved: 416.00 MiB\n",
            "batch loss:  1.7947345972061157\n",
            "After processing batch 54 - Allocated: 66.85 MiB, Reserved: 392.00 MiB\n",
            "batch loss:  1.798775553703308\n",
            "After processing batch 55 - Allocated: 66.66 MiB, Reserved: 406.00 MiB\n",
            "batch loss:  1.7438316345214844\n",
            "After processing batch 56 - Allocated: 66.68 MiB, Reserved: 428.00 MiB\n",
            "batch loss:  1.7581485509872437\n",
            "After processing batch 57 - Allocated: 66.65 MiB, Reserved: 454.00 MiB\n",
            "batch loss:  1.7477816343307495\n",
            "After processing batch 58 - Allocated: 66.77 MiB, Reserved: 432.00 MiB\n",
            "batch loss:  1.7541743516921997\n",
            "After processing batch 59 - Allocated: 66.27 MiB, Reserved: 442.00 MiB\n",
            "batch loss:  1.741066336631775\n",
            "After processing batch 60 - Allocated: 66.92 MiB, Reserved: 402.00 MiB\n",
            "batch loss:  1.7870925664901733\n",
            "After processing batch 61 - Allocated: 66.92 MiB, Reserved: 426.00 MiB\n",
            "----------------------EPOCH 6, TOTAL LOSS = 1.1446147647996743\n",
            "batch loss:  1.7692348957061768\n",
            "After processing batch 0 - Allocated: 66.27 MiB, Reserved: 500.00 MiB\n",
            "batch loss:  1.732513666152954\n",
            "After processing batch 1 - Allocated: 66.16 MiB, Reserved: 372.00 MiB\n",
            "batch loss:  1.792705774307251\n",
            "After processing batch 2 - Allocated: 66.47 MiB, Reserved: 404.00 MiB\n",
            "batch loss:  1.7547906637191772\n",
            "After processing batch 3 - Allocated: 66.58 MiB, Reserved: 420.00 MiB\n",
            "batch loss:  1.798920750617981\n",
            "After processing batch 4 - Allocated: 66.61 MiB, Reserved: 446.00 MiB\n",
            "batch loss:  1.7607471942901611\n",
            "After processing batch 5 - Allocated: 66.29 MiB, Reserved: 456.00 MiB\n",
            "batch loss:  1.753770112991333\n",
            "After processing batch 6 - Allocated: 66.47 MiB, Reserved: 442.00 MiB\n",
            "batch loss:  1.8208913803100586\n",
            "After processing batch 7 - Allocated: 66.88 MiB, Reserved: 410.00 MiB\n",
            "batch loss:  1.7792603969573975\n",
            "After processing batch 8 - Allocated: 66.07 MiB, Reserved: 376.00 MiB\n",
            "batch loss:  1.7890528440475464\n",
            "After processing batch 9 - Allocated: 66.43 MiB, Reserved: 482.00 MiB\n",
            "batch loss:  1.7753406763076782\n",
            "After processing batch 10 - Allocated: 67.37 MiB, Reserved: 466.00 MiB\n",
            "batch loss:  1.7369210720062256\n",
            "After processing batch 11 - Allocated: 66.35 MiB, Reserved: 432.00 MiB\n",
            "batch loss:  1.7814991474151611\n",
            "After processing batch 12 - Allocated: 66.95 MiB, Reserved: 476.00 MiB\n",
            "batch loss:  1.7951194047927856\n",
            "After processing batch 13 - Allocated: 66.22 MiB, Reserved: 414.00 MiB\n",
            "batch loss:  1.7449654340744019\n",
            "After processing batch 14 - Allocated: 66.81 MiB, Reserved: 412.00 MiB\n",
            "batch loss:  1.755895972251892\n",
            "After processing batch 15 - Allocated: 66.31 MiB, Reserved: 416.00 MiB\n",
            "batch loss:  1.7572871446609497\n",
            "After processing batch 16 - Allocated: 66.94 MiB, Reserved: 438.00 MiB\n",
            "batch loss:  1.8126068115234375\n",
            "After processing batch 17 - Allocated: 66.12 MiB, Reserved: 456.00 MiB\n",
            "batch loss:  1.749847650527954\n",
            "After processing batch 18 - Allocated: 66.73 MiB, Reserved: 402.00 MiB\n",
            "batch loss:  1.779109001159668\n",
            "After processing batch 19 - Allocated: 66.56 MiB, Reserved: 472.00 MiB\n",
            "batch loss:  1.7836986780166626\n",
            "After processing batch 20 - Allocated: 66.77 MiB, Reserved: 426.00 MiB\n",
            "batch loss:  1.7886401414871216\n",
            "After processing batch 21 - Allocated: 66.67 MiB, Reserved: 472.00 MiB\n",
            "batch loss:  1.8040469884872437\n",
            "After processing batch 22 - Allocated: 66.32 MiB, Reserved: 444.00 MiB\n",
            "batch loss:  1.7364985942840576\n",
            "After processing batch 23 - Allocated: 66.43 MiB, Reserved: 464.00 MiB\n",
            "batch loss:  1.7383792400360107\n",
            "After processing batch 24 - Allocated: 66.17 MiB, Reserved: 398.00 MiB\n",
            "batch loss:  1.7819759845733643\n",
            "After processing batch 25 - Allocated: 66.41 MiB, Reserved: 424.00 MiB\n",
            "batch loss:  1.7113165855407715\n",
            "After processing batch 26 - Allocated: 66.50 MiB, Reserved: 428.00 MiB\n",
            "batch loss:  1.7372745275497437\n",
            "After processing batch 27 - Allocated: 66.95 MiB, Reserved: 456.00 MiB\n",
            "batch loss:  1.771808385848999\n",
            "After processing batch 28 - Allocated: 67.02 MiB, Reserved: 436.00 MiB\n",
            "batch loss:  1.7764922380447388\n",
            "After processing batch 29 - Allocated: 66.16 MiB, Reserved: 424.00 MiB\n",
            "batch loss:  1.7942856550216675\n",
            "After processing batch 30 - Allocated: 66.97 MiB, Reserved: 460.00 MiB\n",
            "batch loss:  1.7023664712905884\n",
            "After processing batch 31 - Allocated: 66.57 MiB, Reserved: 464.00 MiB\n",
            "batch loss:  1.7888685464859009\n",
            "After processing batch 32 - Allocated: 66.48 MiB, Reserved: 464.00 MiB\n",
            "batch loss:  1.7461254596710205\n",
            "After processing batch 33 - Allocated: 66.55 MiB, Reserved: 454.00 MiB\n",
            "batch loss:  1.8182168006896973\n",
            "After processing batch 34 - Allocated: 66.17 MiB, Reserved: 412.00 MiB\n",
            "batch loss:  1.7870137691497803\n",
            "After processing batch 35 - Allocated: 66.46 MiB, Reserved: 400.00 MiB\n",
            "batch loss:  1.765367031097412\n",
            "After processing batch 36 - Allocated: 66.52 MiB, Reserved: 394.00 MiB\n",
            "batch loss:  1.7442846298217773\n",
            "After processing batch 37 - Allocated: 66.16 MiB, Reserved: 432.00 MiB\n",
            "batch loss:  1.7424848079681396\n",
            "After processing batch 38 - Allocated: 66.85 MiB, Reserved: 406.00 MiB\n",
            "batch loss:  1.7609000205993652\n",
            "After processing batch 39 - Allocated: 66.31 MiB, Reserved: 396.00 MiB\n",
            "batch loss:  1.7268636226654053\n",
            "After processing batch 40 - Allocated: 66.17 MiB, Reserved: 468.00 MiB\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Set device and other configurations\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "checkpoint_dir = 'checkpoint'\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "# Define hyperparameters\n",
        "num_epochs = 10\n",
        "initial_learning_rate = 1e-4\n",
        "batch_size = 96\n",
        "warmup_epochs = 10\n",
        "torch.set_printoptions(threshold=10_000)\n",
        "\n",
        "# Initialize dataset, dataloader, model, criterion, optimizer, scheduler\n",
        "dataset = BaseDataset(X_train, Y_train)\n",
        "data_loader = dataset.get_dataloader(batch_size)\n",
        "model = CADParser().to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=initial_learning_rate)\n",
        "scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: 0.9 ** (epoch // 30) * min((epoch + 1) / warmup_epochs, 1))\n",
        "scaler = GradScaler()  # For mixed precision training\n",
        "\n",
        "def print_memory_usage(tag=\"\"):\n",
        "    print(f\"{tag} - Allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MiB, Reserved: {torch.cuda.memory_reserved()/1024**2:.2f} MiB\")\n",
        "\n",
        "start_epoch = 0\n",
        "loss_list = []\n",
        "# Training loop\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "        graphs, sequences = batch['graph'].to(device), batch['labels'].to(device)\n",
        "        # print_memory_usage(f\"After loading batch {batch_idx}\")\n",
        "\n",
        "        num_nodes_per_graph = batch['num_nodes']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        batch_loss = 0\n",
        "\n",
        "        node_embeddings, graph_embeddings = model.cad_encoder(graphs)\n",
        "        # print_memory_usage(f\"After cad_encoder {batch_idx}\")\n",
        "\n",
        "        del graphs  # Release memory of graphs\n",
        "        torch.cuda.empty_cache()  # Clear unused memory\n",
        "\n",
        "        decoder_input_seq = sequences[:, 0:1, :].to(device)\n",
        "\n",
        "        batch_num_nodes = node_embeddings.shape[0]\n",
        "        padded_node_embeddings = torch.zeros(len(num_nodes_per_graph), 66, 64, device=device)\n",
        "        # print_memory_usage(f\"After creating padded_node_embeddings {batch_idx}\")\n",
        "\n",
        "        start_idx = 0\n",
        "        for i, num_nodes in enumerate(num_nodes_per_graph):\n",
        "            end_idx = start_idx + num_nodes\n",
        "            padded_node_embeddings[i, :num_nodes] = node_embeddings[start_idx:end_idx]\n",
        "            start_idx = end_idx\n",
        "\n",
        "        graph_embeddings = torch.unsqueeze(graph_embeddings, 1).to(device)\n",
        "        # print_memory_usage(f\"After processing node_embeddings {batch_idx}\")\n",
        "\n",
        "        for t in range(1, sequences.size(1)):\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            # print_memory_usage(\"1\")\n",
        "            gt_t = sequences[:, :t, :]\n",
        "            # print_memory_usage(\"2\")\n",
        "            command_type_t = sequences[:, t, 0].long()\n",
        "            # print_memory_usage(\"3\")\n",
        "            param_t = sequences[:, t, 1:]\n",
        "            param_t_mapped = param_t + 1\n",
        "            # print_memory_usage(\"4\")\n",
        "            true_t_i = F.one_hot(command_type_t, num_classes=7).float()\n",
        "            true_p_i = F.one_hot(param_t_mapped.long(), num_classes=257).float()\n",
        "            # print_memory_usage(\"5\")\n",
        "\n",
        "            with autocast():  # Use mixed precision\n",
        "                decoder_output_t_i, decoder_output_p_i = model.cad_decoder(padded_node_embeddings, graph_embeddings, decoder_input_seq)\n",
        "\n",
        "            t_i_loss = criterion(decoder_output_t_i.squeeze(), true_t_i)\n",
        "            p_i_loss = criterion(decoder_output_p_i, true_p_i)\n",
        "            decoder_output_t_i = decoder_output_t_i.detach()  # Detach to free computation graph\n",
        "            decoder_output_p_i = decoder_output_p_i.detach()  # Detach to free computation graph\n",
        "            torch.cuda.empty_cache()\n",
        "            # print_memory_usage(\"6\")\n",
        "            loss = t_i_loss + p_i_loss\n",
        "            batch_loss += loss\n",
        "\n",
        "            # print(f\"t_i_loss.requires_grad: {t_i_loss.requires_grad}\")\n",
        "            # print(f\"p_i_loss.requires_grad: {p_i_loss.requires_grad}\")\n",
        "            # print(f\"loss.requires_grad: {loss.requires_grad}\")\n",
        "            # print(f\"batch_loss.requires_grad: {batch_loss.requires_grad}\")\n",
        "\n",
        "            _, command_type_pred_next = torch.max(decoder_output_t_i, dim=2, keepdim=True)\n",
        "            _, command_args_pred_next = torch.max(decoder_output_p_i, dim=2, keepdim=True)\n",
        "            # print_memory_usage(\"7\")\n",
        "            command_args_pred_next = command_args_pred_next - 1\n",
        "            next_token_pred = torch.cat((command_type_pred_next, command_args_pred_next), dim=1)\n",
        "            next_token_pred = next_token_pred.transpose(1, 2).view(batch_size, -1, 17)\n",
        "            decoder_input_seq = torch.cat((decoder_input_seq, next_token_pred), dim=1)\n",
        "            # print_memory_usage(\"8\")\n",
        "\n",
        "            # print_memory_usage(f\"After processing timestep {t} of batch {batch_idx}\")\n",
        "\n",
        "            # Free the memory used by previous iterations\n",
        "            del decoder_output_t_i, decoder_output_p_i, gt_t, command_type_t, param_t, param_t_mapped, true_t_i, true_p_i\n",
        "            torch.cuda.empty_cache()\n",
        "            # print_memory_usage(\"end\")\n",
        "\n",
        "        batch_loss /= (sequences.size(1) - 1)\n",
        "        batch_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        total_loss += batch_loss.item()\n",
        "        print(\"batch loss: \", batch_loss.item())\n",
        "\n",
        "        scheduler.step()\n",
        "        torch.cuda.empty_cache()  # Clear unused memory after each batch\n",
        "        print_memory_usage(f\"After processing batch {batch_idx}\")\n",
        "\n",
        "    print(f'----------------------EPOCH {epoch + 1}, TOTAL LOSS = {total_loss / 96}')\n",
        "    if epoch % 10 == 0:\n",
        "      torch.save({\n",
        "          'epoch': epoch + 1,\n",
        "          'model_state_dict': model.state_dict(),\n",
        "          'optimizer_state_dict': optimizer.state_dict(),\n",
        "          'scheduler_state_dict': scheduler.state_dict(),\n",
        "          'loss': total_loss / 96,\n",
        "      }, f'{checkpoint_dir}/model_epoch_{epoch + 1}.pth')\n",
        "    # print_memory_usage(f\"After saving checkpoint for epoch {epoch + 1}\")\n",
        "    loss_list.append(total_loss)\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(loss_list, label='Loss per Epoch')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('plot/loss_plot.png')\n",
        "plt.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "HkfbrVrVkZ4S"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}