{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63GnJL2wm2a3",
        "outputId": "f4936807-d669-4c56-8381-2342d11cabff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels/torch-2.3/cu121/repo.html\n",
            "Collecting dgl\n",
            "  Downloading https://data.dgl.ai/wheels/torch-2.3/cu121/dgl-2.2.1%2Bcu121-cp310-cp310-manylinux1_x86_64.whl (199.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.66.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Collecting torchdata>=0.5.0 (from dgl)\n",
            "  Downloading torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from dgl) (2.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2024.2.2)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata>=0.5.0->dgl) (2.3.0+cu121)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->dgl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->dgl) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->dgl) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->dgl) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->torchdata>=0.5.0->dgl) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->torchdata>=0.5.0->dgl) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchdata, dgl\n",
            "Successfully installed dgl-2.2.1+cu121 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 torchdata-0.7.1\n"
          ]
        }
      ],
      "source": [
        "!pip install  dgl -f https://data.dgl.ai/wheels/torch-2.3/cu121/repo.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vrB6d1ioIVZw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16fc7ed1-d36e-47e9-8406-28fa89a3e63c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import FloatTensor\n",
        "import dgl\n",
        "from dgl.data.utils import load_graphs\n",
        "from tqdm import tqdm\n",
        "from abc import abstractmethod\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import dgl\n",
        "from torch import FloatTensor, stack\n",
        "\n",
        "class BaseDataset(Dataset):\n",
        "    @staticmethod\n",
        "    def num_classes():\n",
        "        pass\n",
        "\n",
        "    def __init__(self, X_train, Y_train):\n",
        "        \"\"\"\n",
        "        self.data is a list of dictionaries with keys graph and label\n",
        "        \"\"\"\n",
        "        assert len(X_train) == len(Y_train), \"The number of graphs must match the number of labels\"\n",
        "        self.data = [{\"graph\": graph, \"label\": label} for graph, label in zip(X_train, Y_train)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        return sample[\"graph\"], sample[\"label\"]\n",
        "\n",
        "    def _collate(self, batch):\n",
        "        graphs, labels = zip(*batch)\n",
        "        batched_graph = dgl.batch(graphs)\n",
        "        num_nodes_per_graph = [graph.number_of_nodes() for graph in graphs]\n",
        "\n",
        "\n",
        "        pad_vector = torch.tensor([6] + [-1]*16, dtype=torch.float32)\n",
        "\n",
        "\n",
        "        max_length = 20\n",
        "        padded_labels = []\n",
        "        for label in labels:\n",
        "            label_tensor = torch.tensor(label, dtype=torch.float32)\n",
        "            label_length = label_tensor.shape[0]\n",
        "\n",
        "            if label_length < max_length:\n",
        "\n",
        "                padding_count = max_length - label_length\n",
        "\n",
        "                padding = pad_vector.repeat(padding_count, 1)\n",
        "\n",
        "                padded_label = torch.cat([label_tensor, padding], dim=0)\n",
        "            elif label_length > max_length:\n",
        "\n",
        "                padded_label = label_tensor[:max_length]\n",
        "            else:\n",
        "                padded_label = label_tensor\n",
        "\n",
        "            padded_labels.append(padded_label)\n",
        "\n",
        "\n",
        "        padded_labels = torch.stack(padded_labels)\n",
        "        return {\"graph\": batched_graph, \"labels\": padded_labels, \"num_nodes\": num_nodes_per_graph}\n",
        "\n",
        "\n",
        "    def get_dataloader(self, batch_size, shuffle=True, num_workers=0):\n",
        "        return DataLoader(\n",
        "            self,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle,\n",
        "            collate_fn=self._collate,\n",
        "            num_workers=num_workers,\n",
        "            drop_last=True\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "P7DLd_7XIeaN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from dgl.nn.pytorch.conv import NNConv\n",
        "from dgl.nn.pytorch.glob import MaxPooling\n",
        "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
        "import math\n",
        "\n",
        "# Convolutional Layers\n",
        "def _conv1d(in_channels, out_channels, kernel_size=3, padding=0, bias=False):\n",
        "    \"\"\"\n",
        "    Helper function to create a 1D convolutional layer with batchnorm and LeakyReLU activation\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Input channels\n",
        "        out_channels (int): Output channels\n",
        "        kernel_size (int, optional): Size of the convolutional kernel. Defaults to 3.\n",
        "        padding (int, optional): Padding size on each side. Defaults to 0.\n",
        "        bias (bool, optional): Whether bias is used. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        nn.Sequential: Sequential contained the Conv1d, BatchNorm1d and LeakyReLU layers\n",
        "    \"\"\"\n",
        "    return nn.Sequential(\n",
        "        nn.Conv1d(\n",
        "            in_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=bias\n",
        "        ),\n",
        "        nn.BatchNorm1d(out_channels),\n",
        "        nn.LeakyReLU(),\n",
        "    )\n",
        "\n",
        "\n",
        "def _conv2d(in_channels, out_channels, kernel_size, padding=0, bias=False):\n",
        "    \"\"\"\n",
        "    Helper function to create a 2D convolutional layer with batchnorm and LeakyReLU activation\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Input channels\n",
        "        out_channels (int): Output channels\n",
        "        kernel_size (int, optional): Size of the convolutional kernel. Defaults to 3.\n",
        "        padding (int, optional): Padding size on each side. Defaults to 0.\n",
        "        bias (bool, optional): Whether bias is used. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        nn.Sequential: Sequential contained the Conv2d, BatchNorm2d and LeakyReLU layers\n",
        "    \"\"\"\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=padding,\n",
        "            bias=bias,\n",
        "        ),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.LeakyReLU(),\n",
        "    )\n",
        "\n",
        "\n",
        "def _fc(in_features, out_features, bias=False):\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(in_features, out_features, bias=bias),\n",
        "        nn.BatchNorm1d(out_features),\n",
        "        nn.LeakyReLU(),\n",
        "    )\n",
        "\n",
        "\n",
        "class _MLP(nn.Module):\n",
        "    \"\"\"\"\"\"\n",
        "\n",
        "    def __init__(self, num_layers, input_dim, hidden_dim, output_dim):\n",
        "        \"\"\"\n",
        "        MLP with linear output\n",
        "        Args:\n",
        "            num_layers (int): The number of linear layers in the MLP\n",
        "            input_dim (int): Input feature dimension\n",
        "            hidden_dim (int): Hidden feature dimensions for all hidden layers\n",
        "            output_dim (int): Output feature dimension\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the given number of layers is <1\n",
        "        \"\"\"\n",
        "        super(_MLP, self).__init__()\n",
        "        self.linear_or_not = True  # default is linear model\n",
        "        self.num_layers = num_layers\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        if num_layers < 1:\n",
        "            raise ValueError(\"Number of layers should be positive!\")\n",
        "        elif num_layers == 1:\n",
        "            # Linear model\n",
        "            self.linear = nn.Linear(input_dim, output_dim)\n",
        "        else:\n",
        "            # Multi-layer model\n",
        "            self.linear_or_not = False\n",
        "            self.linears = torch.nn.ModuleList()\n",
        "            self.batch_norms = torch.nn.ModuleList()\n",
        "\n",
        "            self.linears.append(nn.Linear(input_dim, hidden_dim))\n",
        "            for layer in range(num_layers - 2):\n",
        "                self.linears.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "            self.linears.append(nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "            # TODO: this could move inside the above loop\n",
        "            for layer in range(num_layers - 1):\n",
        "                self.batch_norms.append(nn.BatchNorm1d((hidden_dim)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.linear_or_not:\n",
        "            # If linear model\n",
        "            return self.linear(x)\n",
        "        else:\n",
        "            # If MLP\n",
        "            h = x\n",
        "            for i in range(self.num_layers - 1):\n",
        "                h = F.relu(self.batch_norms[i](self.linears[i](h)))\n",
        "            return self.linears[-1](h)\n",
        "\n",
        "\n",
        "class UVNetCurveEncoder(nn.Module):\n",
        "    def __init__(self, in_channels=6, output_dims=64):\n",
        "        \"\"\"\n",
        "        This is the 1D convolutional network that extracts features from the B-rep edge\n",
        "        geometry described as 1D UV-grids (see Section 3.2, Curve & surface convolution\n",
        "        in paper)\n",
        "\n",
        "        Args:\n",
        "            in_channels (int, optional): Number of channels in the edge UV-grids. By default\n",
        "                                         we expect 3 channels for point coordinates and 3 for\n",
        "                                         curve tangents. Defaults to 6.\n",
        "            output_dims (int, optional): Output curve embedding dimension. Defaults to 64.\n",
        "        \"\"\"\n",
        "        super(UVNetCurveEncoder, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.conv1 = _conv1d(in_channels, 64, kernel_size=3, padding=1, bias=False)\n",
        "        self.conv2 = _conv1d(64, 128, kernel_size=3, padding=1, bias=False)\n",
        "        self.conv3 = _conv1d(128, 256, kernel_size=3, padding=1, bias=False)\n",
        "        self.final_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = _fc(256, output_dims, bias=False)\n",
        "\n",
        "        for m in self.modules():\n",
        "            self.weights_init(m)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, (nn.Linear, nn.Conv1d)):\n",
        "            torch.nn.init.kaiming_uniform_(m.weight.data)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.size(1) == self.in_channels\n",
        "        batch_size = x.size(0)\n",
        "        x = x.float()\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.final_pool(x)\n",
        "        x = x.view(batch_size, -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UVNetSurfaceEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=7,\n",
        "        output_dims=64,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        This is the 2D convolutional network that extracts features from the B-rep face\n",
        "        geometry described as 2D UV-grids (see Section 3.2, Curve & surface convolution\n",
        "        in paper)\n",
        "\n",
        "        Args:\n",
        "            in_channels (int, optional): Number of channels in the edge UV-grids. By default\n",
        "                                         we expect 3 channels for point coordinates and 3 for\n",
        "                                         surface normals and 1 for the trimming mask. Defaults\n",
        "                                         to 7.\n",
        "            output_dims (int, optional): Output surface embedding dimension. Defaults to 64.\n",
        "        \"\"\"\n",
        "        super(UVNetSurfaceEncoder, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.conv1 = _conv2d(in_channels, 64, 3, padding=1, bias=False)\n",
        "        self.conv2 = _conv2d(64, 128, 3, padding=1, bias=False)\n",
        "        self.conv3 = _conv2d(128, 256, 3, padding=1, bias=False)\n",
        "        self.final_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = _fc(256, output_dims, bias=False)\n",
        "        for m in self.modules():\n",
        "            self.weights_init(m)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
        "            torch.nn.init.kaiming_uniform_(m.weight.data)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.size(1) == self.in_channels\n",
        "        batch_size = x.size(0)\n",
        "        x = x.float()\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.final_pool(x)\n",
        "        x = x.view(batch_size, -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class _EdgeConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        edge_feats,\n",
        "        out_feats,\n",
        "        node_feats,\n",
        "        num_mlp_layers=2,\n",
        "        hidden_mlp_dim=64,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        This module implements Eq. 2 from the paper where the edge features are\n",
        "        updated using the node features at the endpoints.\n",
        "\n",
        "        Args:\n",
        "            edge_feats (int): Input edge feature dimension\n",
        "            out_feats (int): Output feature deimension\n",
        "            node_feats (int): Input node feature dimension\n",
        "            num_mlp_layers (int, optional): Number of layers used in the MLP. Defaults to 2.\n",
        "            hidden_mlp_dim (int, optional): Hidden feature dimension in the MLP. Defaults to 64.\n",
        "        \"\"\"\n",
        "        super(_EdgeConv, self).__init__()\n",
        "        self.proj = _MLP(1, node_feats, hidden_mlp_dim, edge_feats)\n",
        "        self.mlp = _MLP(num_mlp_layers, edge_feats, hidden_mlp_dim, out_feats)\n",
        "        self.batchnorm = nn.BatchNorm1d(out_feats)\n",
        "        self.eps = torch.nn.Parameter(torch.FloatTensor([0.0]))\n",
        "\n",
        "    def forward(self, graph, nfeat, efeat):\n",
        "        src, dst = graph.edges()\n",
        "        proj1, proj2 = self.proj(nfeat[src]), self.proj(nfeat[dst])\n",
        "        agg = proj1 + proj2\n",
        "        h = self.mlp((1 + self.eps) * efeat + agg)\n",
        "        h = F.leaky_relu(self.batchnorm(h))\n",
        "        return h\n",
        "\n",
        "\n",
        "class _NodeConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        node_feats,\n",
        "        out_feats,\n",
        "        edge_feats,\n",
        "        num_mlp_layers=2,\n",
        "        hidden_mlp_dim=64,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        This module implements Eq. 1 from the paper where the node features are\n",
        "        updated using the neighboring node and edge features.\n",
        "\n",
        "        Args:\n",
        "            node_feats (int): Input edge feature dimension\n",
        "            out_feats (int): Output feature deimension\n",
        "            node_feats (int): Input node feature dimension\n",
        "            num_mlp_layers (int, optional): Number of layers used in the MLP. Defaults to 2.\n",
        "            hidden_mlp_dim (int, optional): Hidden feature dimension in the MLP. Defaults to 64.\n",
        "        \"\"\"\n",
        "        super(_NodeConv, self).__init__()\n",
        "        self.gconv = NNConv(\n",
        "            in_feats=node_feats,\n",
        "            out_feats=out_feats,\n",
        "            edge_func=nn.Linear(edge_feats, node_feats * out_feats),\n",
        "            aggregator_type=\"sum\",\n",
        "            bias=False,\n",
        "        )\n",
        "        self.batchnorm = nn.BatchNorm1d(out_feats)\n",
        "        self.mlp = _MLP(num_mlp_layers, node_feats, hidden_mlp_dim, out_feats)\n",
        "        self.eps = torch.nn.Parameter(torch.FloatTensor([0.0]))\n",
        "\n",
        "    def forward(self, graph, nfeat, efeat):\n",
        "        h = (1 + self.eps) * nfeat\n",
        "        h = self.gconv(graph, h, efeat)\n",
        "        h = self.mlp(h)\n",
        "        h = F.leaky_relu(self.batchnorm(h))\n",
        "        return h\n",
        "\n",
        "\n",
        "\n",
        "class UVNetGraphEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        input_edge_dim,\n",
        "        output_dim,\n",
        "        hidden_dim=64,\n",
        "        learn_eps=True,\n",
        "        num_layers=3,\n",
        "        num_mlp_layers=2,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        This is the graph neural network used for message-passing features in the\n",
        "        face-adjacency graph.\n",
        "\n",
        "        Args:\n",
        "            input_dim ([type]): [description]\n",
        "            input_edge_dim ([type]): [description]\n",
        "            output_dim ([type]): [description]\n",
        "            hidden_dim (int, optional): [description]. Defaults to 64.\n",
        "            learn_eps (bool, optional): [description]. Defaults to True.\n",
        "            num_layers (int, optional): [description]. Defaults to 3.\n",
        "            num_mlp_layers (int, optional): [description]. Defaults to 2.\n",
        "        \"\"\"\n",
        "        super(UVNetGraphEncoder, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.learn_eps = learn_eps\n",
        "\n",
        "        # List of layers for node and edge feature message passing\n",
        "        self.node_conv_layers = torch.nn.ModuleList()\n",
        "        self.edge_conv_layers = torch.nn.ModuleList()\n",
        "\n",
        "        for layer in range(self.num_layers - 1):\n",
        "            node_feats = input_dim if layer == 0 else hidden_dim\n",
        "            edge_feats = input_edge_dim if layer == 0 else hidden_dim\n",
        "            self.node_conv_layers.append(\n",
        "                _NodeConv(\n",
        "                    node_feats=node_feats,\n",
        "                    out_feats=hidden_dim,\n",
        "                    edge_feats=edge_feats,\n",
        "                    num_mlp_layers=num_mlp_layers,\n",
        "                    hidden_mlp_dim=hidden_dim,\n",
        "                ),\n",
        "            )\n",
        "            self.edge_conv_layers.append(\n",
        "                _EdgeConv(\n",
        "                    edge_feats=edge_feats,\n",
        "                    out_feats=hidden_dim,\n",
        "                    node_feats=node_feats,\n",
        "                    num_mlp_layers=num_mlp_layers,\n",
        "                    hidden_mlp_dim=hidden_dim,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Linear function for graph poolings of output of each layer\n",
        "        # which maps the output of different layers into a prediction score\n",
        "        self.linears_prediction = torch.nn.ModuleList()\n",
        "\n",
        "        for layer in range(num_layers):\n",
        "            if layer == 0:\n",
        "                self.linears_prediction.append(nn.Linear(input_dim, output_dim))\n",
        "            else:\n",
        "                self.linears_prediction.append(nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "        self.drop1 = nn.Dropout(0.3)\n",
        "        self.drop = nn.Dropout(0.5)\n",
        "        self.pool = MaxPooling()\n",
        "\n",
        "    def forward(self, g, h, efeat):\n",
        "        hidden_rep = [h]\n",
        "        he = efeat\n",
        "\n",
        "        for i in range(self.num_layers - 1):\n",
        "            print(f\"Layer {i} - NodeConv Input: {h.shape}, EdgeConv Input: {he.shape}\")\n",
        "            h = self.node_conv_layers[i](g, h, he)\n",
        "            print(f\"Layer {i} - NodeConv Output: {h.shape}\")\n",
        "            print(f\"Layer {i} - NodeConv Output requires_grad: {h.requires_grad}\")\n",
        "            he = self.edge_conv_layers[i](g, h, he)\n",
        "            print(f\"Layer {i} - EdgeConv Output: {he.shape}\")\n",
        "            print(f\"Layer {i} - EdgeConv Output requires_grad: {he.requires_grad}\")\n",
        "            hidden_rep.append(h)\n",
        "        node_embeddings = hidden_rep[-1]\n",
        "        node_embeddings = self.drop1(node_embeddings)\n",
        "\n",
        "        graph_representation = 0\n",
        "        for i, h in enumerate(hidden_rep):\n",
        "            pooled_h = self.pool(g, h)\n",
        "            graph_representation += self.drop(self.linears_prediction[i](pooled_h))\n",
        "\n",
        "\n",
        "        return node_embeddings, graph_representation\n",
        "\n",
        "class CADEncoder(nn.Module):\n",
        "  def __init__(self, crv_emb_dim=64, srf_emb_dim=64, graph_emb_dim=128, dropout=0.3):\n",
        "    super(CADEncoder, self).__init__()\n",
        "    self.curv_encoder = UVNetCurveEncoder(in_channels=10, output_dims=crv_emb_dim) # in_channels originally 6\n",
        "    self.surf_encoder = UVNetSurfaceEncoder(in_channels=10, output_dims=srf_emb_dim)\n",
        "    self.graph_encoder = UVNetGraphEncoder(srf_emb_dim, crv_emb_dim, graph_emb_dim)\n",
        "\n",
        "  def forward(self, batched_graph):\n",
        "    input_crv_feat = batched_graph.edata[\"x\"]\n",
        "    input_srf_feat = batched_graph.ndata[\"x\"]\n",
        "    hidden_crv_feat = self.curv_encoder(input_crv_feat)\n",
        "    hidden_srf_feat = self.surf_encoder(input_srf_feat)\n",
        "    node_emb, graph_emb = self.graph_encoder(batched_graph, hidden_srf_feat, hidden_crv_feat)\n",
        "    return node_emb, graph_emb\n",
        "\n",
        "\n",
        "class PositionalEncodingLUT(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=250):\n",
        "        super(PositionalEncodingLUT, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(0, max_len, dtype=torch.long).unsqueeze(1)\n",
        "        self.register_buffer('position', position)\n",
        "\n",
        "        self.pos_embed = nn.Embedding(max_len, d_model)\n",
        "\n",
        "        self._init_embeddings()\n",
        "\n",
        "    def _init_embeddings(self):\n",
        "        nn.init.kaiming_normal_(self.pos_embed.weight, mode=\"fan_in\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, _ = x.shape\n",
        "\n",
        "        pos = self.position[:seq_length]  # This will have shape [seq_length, 1]\n",
        "\n",
        "        pos = pos.expand(-1, batch_size).contiguous()  # Reshape to [seq_length, batch_size]\n",
        "        pos = pos.transpose(0, 1)  # Transpose to [batch_size, seq_length]\n",
        "\n",
        "        pos_embeddings = self.pos_embed(pos)  # This should now be [batch_size, seq_length, d_model]\n",
        "\n",
        "        x = x + pos_embeddings\n",
        "\n",
        "        return self.dropout(x)\n",
        "\n",
        "class CADEmbedding(nn.Module):\n",
        "    \"\"\"Embedding: positional embed + command embed + parameter embed + group embed (optional)\"\"\"\n",
        "    def __init__(self, n_commands=7, d_model=64, n_args=16, args_dim=257, seq_len=60):\n",
        "        super(CADEmbedding, self).__init__()\n",
        "        self.command_embed = nn.Embedding(n_commands, d_model)\n",
        "        self.arg_embed = nn.Embedding(args_dim, d_model, padding_idx=0)\n",
        "        self.embed_fcn = nn.Linear(d_model * n_args, d_model)\n",
        "        self.pos_encoding = PositionalEncodingLUT(d_model, max_len=seq_len+2)\n",
        "\n",
        "    def forward(self, commands, args):\n",
        "        S, N = commands.shape\n",
        "        src = self.command_embed(commands.long()) + \\\n",
        "              self.embed_fcn(self.arg_embed((args + 1).long()).view(S, N, -1))\n",
        "        src = self.pos_encoding(src)\n",
        "        return src\n",
        "\n",
        "class FusionModule(nn.Module):\n",
        "    def __init__(self, latent_size, command_embedding_size=64, hidden_size=128, output_size=64):\n",
        "        super(FusionModule, self).__init__()\n",
        "        self.fc1 = nn.Linear(latent_size + command_embedding_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, latent, command_embedding):\n",
        "        command_embedding = command_embedding.unsqueeze(1)\n",
        "        combined = torch.cat((latent, command_embedding), dim=2)\n",
        "        x = self.fc1(combined)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "from torch import Tensor\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "\n",
        "class CausalTransformerDecoderLayer(nn.TransformerDecoderLayer):\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        if self.training:\n",
        "            return super().forward(tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\n",
        "        else:\n",
        "            tgt_last_tok = tgt[-1:, :, :]\n",
        "            tgt_last_tok = self.self_attn(tgt_last_tok, tgt, tgt)[0] + tgt_last_tok\n",
        "            tgt_last_tok = self.norm1(tgt_last_tok)\n",
        "            if memory is not None:\n",
        "                tgt_last_tok = self.multihead_attn(tgt_last_tok, memory, memory)[0] + tgt_last_tok\n",
        "                tgt_last_tok = self.norm2(tgt_last_tok)\n",
        "            tgt_last_tok = self.linear2(self.dropout(self.activation(self.linear1(tgt_last_tok)))) + tgt_last_tok\n",
        "            tgt_last_tok = self.norm3(tgt_last_tok)\n",
        "            return tgt_last_tok\n",
        "\n",
        "\n",
        "class CausalTransformerDecoder(nn.TransformerDecoder):\n",
        "    def forward(self, tgt, memory=None, cache=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        # print(\"tgt in TransformerDecoder \", tgt.shape)\n",
        "        # print(\"memory in TransformerDecoder \", memory.shape)\n",
        "        if self.training:\n",
        "            if cache is not None:\n",
        "                raise ValueError(\"cache parameter should be None in training mode\")\n",
        "            for mod in self.layers:\n",
        "                tgt = mod(tgt, memory, tgt_mask=None, memory_mask=memory_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
        "            return tgt\n",
        "        else:\n",
        "            new_token_cache = []\n",
        "            for i, mod in enumerate(self.layers):\n",
        "                tgt = mod(tgt, memory)\n",
        "                new_token_cache.append(tgt)\n",
        "                if cache is not None:\n",
        "                    tgt = torch.cat([cache[i], tgt], dim=0)\n",
        "            if cache is not None:\n",
        "                new_cache = torch.cat([cache, torch.stack(new_token_cache, dim=0)], dim=1)\n",
        "            else:\n",
        "                new_cache = torch.stack(new_token_cache, dim=0)\n",
        "            # print(\"tgt[-1:] from the transformer decoder block \", tgt[-1:])\n",
        "            # print(\"new_cache from the transformer decoder block \", new_cache)\n",
        "            return tgt[-1:], new_cache\n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _generate_causal_mask(sz, device):\n",
        "        mask = torch.full((sz, sz), float('-inf'))\n",
        "        mask = torch.triu(mask, diagonal=1)\n",
        "        return mask.to(device)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _generate_causal_mask(sz, device):\n",
        "        \"\"\"\n",
        "        Generates a causal mask to hide future tokens for autoregressive tasks.\n",
        "        \"\"\"\n",
        "        mask = torch.full((sz, sz), float('-inf'))\n",
        "        mask = torch.triu(mask, diagonal=1)\n",
        "        return mask.to(device)\n",
        "\n",
        "\n",
        "class CADDecoder(nn.Module):\n",
        "    def __init__(self, d_model=64, nhead=8, num_decoder_layers=4, dim_feedforward=2048, dropout=0.1, activation=\"relu\", num_commands=7, max_seq_len=5000, num_parameters=16, param_cat=257):\n",
        "        super(CADDecoder, self).__init__()\n",
        "        self.param_cat = param_cat\n",
        "        # Embedding\n",
        "        self.cad_command_embedding = CADEmbedding(d_model=d_model, n_commands=num_commands, n_args=num_parameters, seq_len=max_seq_len)\n",
        "\n",
        "        # Fusion module\n",
        "        self.fusion_module = FusionModule(latent_size=128)\n",
        "\n",
        "        # Decoder\n",
        "        decoder_layer = CausalTransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, activation=activation)\n",
        "        self.transformer_decoder = CausalTransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "\n",
        "        # Final output\n",
        "        self.output_layer1 = nn.Linear(d_model, num_commands)  # t_i\n",
        "        self.output_layer2 = nn.Linear(d_model, num_parameters * param_cat)  # p_i\n",
        "\n",
        "    def forward(self, node_embeddings, graph_embeddings, command_seq, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        commands_count = command_seq.shape[1]\n",
        "        commands = command_seq[:, :, 0]\n",
        "        args = command_seq[:, :, 1:]\n",
        "        construct_embed = self.cad_command_embedding(commands, args)\n",
        "\n",
        "        # Fusion module\n",
        "        fusion_outputs = []\n",
        "        for i in range(commands_count):\n",
        "            fusion_output = self.fusion_module(graph_embeddings, construct_embed[:, i, :])\n",
        "            fusion_outputs.append(fusion_output)\n",
        "        fusion_outputs = torch.cat(fusion_outputs, dim=1)  # [batch_size, commands_count, d_model]\n",
        "\n",
        "        # Reshape node embeddings to [number of nodes, batch size, embedding dimension]\n",
        "        batch_size, num_nodes, embed_dim = node_embeddings.shape\n",
        "        node_embeddings = node_embeddings.transpose(0, 1)  # [number of nodes, batch size, embedding dimension]\n",
        "\n",
        "        all_decoder_outputs = []\n",
        "\n",
        "        # Autoregressive decoding\n",
        "        for t in range(commands_count):\n",
        "            if t == 0:\n",
        "                current_input = fusion_outputs[:, :1, :]  # [batch_size, 1, d_model]\n",
        "            else:\n",
        "                current_input = torch.cat((fusion_outputs[:, :t, :], last_output), dim=1)\n",
        "\n",
        "            current_input = current_input.transpose(0, 1)  # [sequence_length, batch_size, d_model]\n",
        "\n",
        "            decoder_output = self.transformer_decoder(\n",
        "                tgt=current_input,\n",
        "                memory=node_embeddings,\n",
        "                memory_mask=memory_mask,\n",
        "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                memory_key_padding_mask=memory_key_padding_mask\n",
        "            )\n",
        "\n",
        "            decoder_output = decoder_output.transpose(0, 1)  # [batch_size, sequence_length, d_model]\n",
        "\n",
        "            # Get the last output token\n",
        "            last_output = decoder_output[:, -1, :].unsqueeze(1)  # [batch_size, 1, d_model]\n",
        "\n",
        "            all_decoder_outputs.append(last_output)\n",
        "\n",
        "        all_decoder_outputs = torch.cat(all_decoder_outputs, dim=1)  # [batch_size, commands_count, d_model]\n",
        "\n",
        "        # Final output layers\n",
        "        output1 = self.output_layer1(all_decoder_outputs)\n",
        "        output1 = F.softmax(output1[:, -1:, :], dim=-1)  # t_i\n",
        "\n",
        "        output2 = self.output_layer2(all_decoder_outputs)\n",
        "        output2 = output2.view(batch_size, commands_count, -1, self.param_cat)[:, -1, :, :]  # Reshape and select last\n",
        "        output2 = F.softmax(output2, dim=2)  # p_i\n",
        "        return output1, output2\n",
        "\n",
        "\n",
        "class CADParser(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CADParser, self).__init__()\n",
        "\n",
        "        # encoder\n",
        "        self.cad_encoder = CADEncoder()\n",
        "\n",
        "        # decoder\n",
        "        self.cad_decoder = CADDecoder()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sflWNhu6J_D4",
        "outputId": "35c4b3a0-3f9e-4611-866a-7225ac73a0df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "graphs, label_dict = dgl.load_graphs(\"/content/drive/My Drive/DeepCADDataset/all_graphs.bin\")\n",
        "npz = np.load(\"/content/drive/My Drive/DeepCADDataset/all_npz.npz\")\n",
        "# Prepare data\n",
        "max_nodes = max(graph.number_of_nodes() for graph in graphs)\n",
        "print(\"Maximum number of nodes in any graph:\", max_nodes)\n",
        "\n",
        "# Filter graphs and corresponding npz entries\n",
        "filtered_graphs = []\n",
        "filtered_npz_keys = []\n",
        "for i in range(len(graphs)):\n",
        "    if graphs[i].number_of_nodes() <= 20:\n",
        "        filtered_graphs.append(graphs[i])\n",
        "        filtered_npz_keys.append(list(npz.keys())[i])\n",
        "\n",
        "# Count the number of filtered graphs\n",
        "count = len(filtered_graphs)\n",
        "print(\"Number of graphs with <= 20 nodes:\", count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lrc58J9PROEO",
        "outputId": "e751ec33-827e-41b8-8248-4e67a62579fc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum number of nodes in any graph: 66\n",
            "Number of graphs with <= 20 nodes: 6703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Assuming BaseDataset and CADParser are already defined\n",
        "\n",
        "# Load data\n",
        "graphs, label_dict = dgl.load_graphs(\"/content/drive/My Drive/DeepCADDataset/all_graphs.bin\")\n",
        "npz = np.load(\"/content/drive/My Drive/DeepCADDataset/all_npz.npz\")\n",
        "\n",
        "# Set seed for reproducibility\n",
        "seednumber = 2024\n",
        "torch.manual_seed(seednumber)\n",
        "torch.cuda.manual_seed(seednumber)\n",
        "np.random.seed(seednumber)\n",
        "\n",
        "# Prepare data\n",
        "max_nodes = max(graph.number_of_nodes() for graph in graphs)\n",
        "print(\"Maximum number of nodes in any graph:\", max_nodes)\n",
        "\n",
        "Y = [npz[key] for key in npz.keys()]\n",
        "X = graphs\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrTC4YLNaBCW",
        "outputId": "53d6779b-87b1-46ce-a3d8-287a5d45e9df"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum number of nodes in any graph: 66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_X = []\n",
        "filtered_Y = []\n",
        "for i in range(len(X)):\n",
        "    if X[i].number_of_nodes() <= 20:\n",
        "        filtered_X.append(X[i])\n",
        "        filtered_Y.append(Y[i])\n",
        "\n",
        "# Count the number of filtered graphs\n",
        "count = len(filtered_X)\n",
        "print(\"Number of graphs with <= 20 nodes:\", count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAjbNl50aKbr",
        "outputId": "0d2970d5-bc4e-45a3-ccb6-12f298f9fc8a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of graphs with <= 20 nodes: 6703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(filtered_X, filtered_Y, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "oO0lY__gaDks"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4nZtymaIfvY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "from torch import nn, optim\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define your model class and dataset class\n",
        "# from your_module import CADParser, BaseDataset\n",
        "\n",
        "# Set device and other configurations\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def load_checkpoint(model, optimizer, scheduler, checkpoint_path):\n",
        "    if os.path.isfile(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        loss_list = checkpoint.get('loss_list', [])\n",
        "        print(f\"Checkpoint loaded. Resuming from epoch {start_epoch}\")\n",
        "        return start_epoch, loss_list\n",
        "    else:\n",
        "        print(\"No checkpoint found at specified path. Starting from scratch.\")\n",
        "        return 0, []\n",
        "\n",
        "# Define hyperparameters\n",
        "num_epochs = 100\n",
        "initial_learning_rate = 5e-2\n",
        "batch_size = 96\n",
        "warmup_epochs = 10\n",
        "torch.set_printoptions(threshold=10_000)\n",
        "\n",
        "# Initialize dataset, dataloader, model, criterion, optimizer, scheduler\n",
        "dataset = BaseDataset(X_train, Y_train)\n",
        "data_loader = dataset.get_dataloader(batch_size)\n",
        "model = CADParser().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=initial_learning_rate)\n",
        "scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: 0.9 ** (epoch // 30) * min((epoch + 1) / warmup_epochs, 1))\n",
        "scaler = GradScaler()  # For mixed precision training\n",
        "\n",
        "# Load checkpoint if exists\n",
        "checkpoint_path = \"/content/drive/MyDrive/CADParser_Checkpoint/franky_model_epoch_61.pth\"\n",
        "if checkpoint_path != \"\":\n",
        "    start_epoch, loss_list = load_checkpoint(model, optimizer, scheduler, checkpoint_path)\n",
        "\n",
        "def print_memory_usage(tag=\"\"):\n",
        "    print(f\"{tag} - Allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MiB, Reserved: {torch.cuda.memory_reserved()/1024**2:.2f} MiB\")\n",
        "\n",
        "# Start training\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "        graphs, sequences = batch['graph'].to(device), batch['labels'].to(device)\n",
        "        num_nodes_per_graph = batch['num_nodes']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        batch_loss = 0\n",
        "\n",
        "        node_embeddings, graph_embeddings = model.cad_encoder(graphs)\n",
        "        assert node_embeddings.requires_grad, \"Node embeddings detached!\"\n",
        "        assert graph_embeddings.requires_grad, \"Graph embeddings detached!\"\n",
        "\n",
        "        del graphs  # Release memory of graphs\n",
        "        torch.cuda.empty_cache()  # Clear unused memory\n",
        "\n",
        "        decoder_input_seq = sequences[:, 0:1, :].to(device)\n",
        "        batch_num_nodes = node_embeddings.shape[0]\n",
        "        padded_node_embeddings = torch.zeros(len(num_nodes_per_graph), 66, 64, device=device)\n",
        "\n",
        "        start_idx = 0\n",
        "        for i, num_nodes in enumerate(num_nodes_per_graph):\n",
        "            end_idx = start_idx + num_nodes\n",
        "            padded_node_embeddings[i, :num_nodes] = node_embeddings[start_idx:end_idx]\n",
        "            start_idx = end_idx\n",
        "\n",
        "        graph_embeddings = torch.unsqueeze(graph_embeddings, 1).to(device)\n",
        "        predicted = []\n",
        "        assert node_embeddings.requires_grad, \"Node embeddings detached!\"\n",
        "        assert graph_embeddings.requires_grad, \"Graph embeddings detached!\"\n",
        "        for t in range(1, sequences.size(1)):\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            gt_t = sequences[:, :t, :]\n",
        "            command_type_t = sequences[:, t, 0].long()\n",
        "            param_t = sequences[:, t, 1:]\n",
        "            param_t_mapped = param_t + 1\n",
        "            true_t_i = F.one_hot(command_type_t, num_classes=7).float()\n",
        "            true_p_i = F.one_hot(param_t_mapped.long(), num_classes=257).float()\n",
        "\n",
        "            with autocast():  # Use mixed precision\n",
        "                decoder_output_t_i, decoder_output_p_i = model.cad_decoder(padded_node_embeddings, graph_embeddings, decoder_input_seq)\n",
        "            t_i_loss = criterion(decoder_output_t_i.squeeze(), true_t_i)\n",
        "            p_i_loss = criterion(decoder_output_p_i, true_p_i)\n",
        "            torch.cuda.empty_cache()\n",
        "            loss = t_i_loss + p_i_loss\n",
        "            batch_loss += loss\n",
        "\n",
        "            _, command_type_pred_next = torch.max(decoder_output_t_i, dim=2, keepdim=True)\n",
        "            _, command_args_pred_next = torch.max(decoder_output_p_i, dim=2, keepdim=True)\n",
        "            command_args_pred_next = command_args_pred_next - 1\n",
        "            next_token_pred = torch.cat((command_type_pred_next, command_args_pred_next), dim=1)\n",
        "            next_token_pred = next_token_pred.transpose(1, 2).view(batch_size, -1, 17)\n",
        "            decoder_input_seq = torch.cat((decoder_input_seq, next_token_pred), dim=1)\n",
        "            predicted.append(next_token_pred)\n",
        "\n",
        "            # Free the memory used by previous iterations\n",
        "            del decoder_output_t_i, decoder_output_p_i, gt_t, command_type_t, param_t, param_t_mapped, true_t_i, true_p_i\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # Normalize and backpropagate the loss\n",
        "        batch_loss /= (sequences.size(1) - 1)\n",
        "        batch_loss.backward()\n",
        "\n",
        "        # Check gradients for vanishing or exploding\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.grad is None:\n",
        "                print(f\"Gradient for {name} is None\")\n",
        "            else:\n",
        "                grad_norm = param.grad.norm().item()\n",
        "                if grad_norm < 1e-5:\n",
        "                    print(f\"Warning: Vanishing gradient detected in {name}, norm: {grad_norm}\")\n",
        "                elif grad_norm > 1e+2:\n",
        "                    print(f\"Warning: Exploding gradient detected in {name}, norm: {grad_norm}\")\n",
        "\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        total_loss += batch_loss.item()\n",
        "        print(\"batch loss: \", batch_loss.item())\n",
        "\n",
        "        scheduler.step()\n",
        "        torch.cuda.empty_cache()  # Clear unused memory after each batch\n",
        "        print_memory_usage(f\"After processing batch {batch_idx}\")\n",
        "\n",
        "    print(f'----------------------EPOCH {epoch + 1}, TOTAL LOSS = {total_loss / len(data_loader)}')\n",
        "    if epoch % 10 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'loss': total_loss / len(data_loader),\n",
        "        }, f'/content/drive/MyDrive/CADParser_Checkpoint/franky_model_epoch_{epoch + 1}.pth')\n",
        "\n",
        "    loss_list.append(total_loss / len(data_loader))\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Plotting training loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(loss_list, label='Loss per Epoch')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('/content/drive/MyDrive/CADParser_Checkpoint/training_loss.png')\n",
        "plt.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(loss_list, label='Loss per Epoch')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('/content/drive/MyDrive/CADParser_Checkpoint')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "fB6xt62C7pVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Checkpoint\n",
        "import torch\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Load checkpoint\n",
        "def load_checkpoint(checkpoint_path, model, optimizer, scheduler):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "    return start_epoch, loss\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CADParser().to(device)\n",
        "\n",
        "num_epochs = 1\n",
        "initial_learning_rate = 1e-4\n",
        "batch_size = 12\n",
        "warmup_epochs = 10\n",
        "torch.set_printoptions(threshold=10_000)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "checkpoint_path = '/content/drive/MyDrive/CADParser_Checkpoint/franky_model_epoch_61.pth'\n",
        "optimizer = optim.AdamW(model.parameters(), lr=initial_learning_rate)\n",
        "scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: 0.9 ** (epoch // 30) * min((epoch + 1) / warmup_epochs, 1))\n",
        "start_epoch, _ = load_checkpoint(checkpoint_path, model, optimizer, scheduler)\n",
        "\n",
        "print(f\"Resuming from epoch {start_epoch}\")"
      ],
      "metadata": {
        "id": "AvrEd9LYXq9S"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}