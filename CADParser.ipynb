{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install dgl -f https://data.dgl.ai/wheels/cu121/repo.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzIIuR9ZxCI-",
        "outputId": "b1cd92c1-6db4-4bff-edda-e8e99a4a93a0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels/cu121/repo.html\n",
            "Collecting dgl\n",
            "  Downloading https://data.dgl.ai/wheels/cu121/dgl-2.1.0%2Bcu121-cp310-cp310-manylinux1_x86_64.whl (467.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.5/467.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.66.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: torchdata>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (0.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2024.2.2)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata>=0.5.0->dgl) (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata>=0.5.0->dgl) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata>=0.5.0->dgl)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->torchdata>=0.5.0->dgl) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->torchdata>=0.5.0->dgl) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, dgl\n",
            "Successfully installed dgl-2.1.0+cu121 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from scipy.spatial.transform import Rotation\n",
        "\n",
        "\n",
        "def bounding_box_uvgrid(inp: torch.Tensor):\n",
        "    pts = inp[..., :3].reshape((-1, 3))\n",
        "    mask = inp[..., 6].reshape(-1)\n",
        "    point_indices_inside_faces = mask == 1\n",
        "    pts = pts[point_indices_inside_faces, :]\n",
        "    return bounding_box_pointcloud(pts)\n",
        "\n",
        "\n",
        "def bounding_box_pointcloud(pts: torch.Tensor):\n",
        "    x = pts[:, 0]\n",
        "    y = pts[:, 1]\n",
        "    z = pts[:, 2]\n",
        "    box = [[x.min(), y.min(), z.min()], [x.max(), y.max(), z.max()]]\n",
        "    return torch.tensor(box)\n",
        "\n",
        "\n",
        "def center_and_scale_uvgrid(inp: torch.Tensor, return_center_scale=False):\n",
        "    bbox = bounding_box_uvgrid(inp)\n",
        "    diag = bbox[1] - bbox[0]\n",
        "    scale = 2.0 / max(diag[0], diag[1], diag[2])\n",
        "    center = 0.5 * (bbox[0] + bbox[1])\n",
        "    inp[..., :3] -= center\n",
        "    inp[..., :3] *= scale\n",
        "    if return_center_scale:\n",
        "        return inp, center, scale\n",
        "    return inp\n",
        "\n",
        "\n",
        "def get_random_rotation():\n",
        "    \"\"\"Get a random rotation in 90 degree increments along the canonical axes\"\"\"\n",
        "    axes = [\n",
        "        np.array([1, 0, 0]),\n",
        "        np.array([0, 1, 0]),\n",
        "        np.array([0, 0, 1]),\n",
        "    ]\n",
        "    angles = [0.0, 90.0, 180.0, 270.0]\n",
        "    axis = random.choice(axes)\n",
        "    angle_radians = np.radians(random.choice(angles))\n",
        "    return Rotation.from_rotvec(angle_radians * axis)\n",
        "\n",
        "\n",
        "def rotate_uvgrid(inp, rotation):\n",
        "    \"\"\"Rotate the node features in the graph by a given rotation\"\"\"\n",
        "    Rmat = torch.tensor(rotation.as_matrix()).float()\n",
        "    orig_size = inp[..., :3].size()\n",
        "    inp[..., :3] = torch.mm(inp[..., :3].view(-1, 3), Rmat).view(\n",
        "        orig_size\n",
        "    )  # Points\n",
        "    inp[..., 3:6] = torch.mm(inp[..., 3:6].view(-1, 3), Rmat).view(\n",
        "        orig_size\n",
        "    )  # Normals/tangents\n",
        "    return inp\n",
        "\n",
        "\n",
        "INVALID_FONTS = [\n",
        "    \"Bokor\",\n",
        "    \"Lao Muang Khong\",\n",
        "    \"Lao Sans Pro\",\n",
        "    \"MS Outlook\",\n",
        "    \"Catamaran Black\",\n",
        "    \"Dubai\",\n",
        "    \"HoloLens MDL2 Assets\",\n",
        "    \"Lao Muang Don\",\n",
        "    \"Oxanium Medium\",\n",
        "    \"Rounded Mplus 1c\",\n",
        "    \"Moul Pali\",\n",
        "    \"Noto Sans Tamil\",\n",
        "    \"Webdings\",\n",
        "    \"Armata\",\n",
        "    \"Koulen\",\n",
        "    \"Yinmar\",\n",
        "    \"Ponnala\",\n",
        "    \"Noto Sans Tamil\",\n",
        "    \"Chenla\",\n",
        "    \"Lohit Devanagari\",\n",
        "    \"Metal\",\n",
        "    \"MS Office Symbol\",\n",
        "    \"Cormorant Garamond Medium\",\n",
        "    \"Chiller\",\n",
        "    \"Give You Glory\",\n",
        "    \"Hind Vadodara Light\",\n",
        "    \"Libre Barcode 39 Extended\",\n",
        "    \"Myanmar Sans Pro\",\n",
        "    \"Scheherazade\",\n",
        "    \"Segoe MDL2 Assets\",\n",
        "    \"Siemreap\",\n",
        "    \"Signika SemiBold\" \"Taprom\",\n",
        "    \"Times New Roman TUR\",\n",
        "    \"Playfair Display SC Black\",\n",
        "    \"Poppins Thin\",\n",
        "    \"Raleway Dots\",\n",
        "    \"Raleway Thin\",\n",
        "    \"Segoe MDL2 Assets\",\n",
        "    \"Segoe MDL2 Assets\",\n",
        "    \"Spectral SC ExtraLight\",\n",
        "    \"Txt\",\n",
        "    \"Uchen\",\n",
        "    \"Yinmar\",\n",
        "    \"Almarai ExtraBold\",\n",
        "    \"Fasthand\",\n",
        "    \"Exo\",\n",
        "    \"Freckle Face\",\n",
        "    \"Montserrat Light\",\n",
        "    \"Inter\",\n",
        "    \"MS Reference Specialty\",\n",
        "    \"MS Outlook\",\n",
        "    \"Preah Vihear\",\n",
        "    \"Sitara\",\n",
        "    \"Barkerville Old Face\",\n",
        "    \"Bodoni MT\" \"Bokor\",\n",
        "    \"Fasthand\",\n",
        "    \"HoloLens MDL2 Assests\",\n",
        "    \"Libre Barcode 39\",\n",
        "    \"Lohit Tamil\",\n",
        "    \"Marlett\",\n",
        "    \"MS outlook\",\n",
        "    \"MS office Symbol Semilight\",\n",
        "    \"MS office symbol regular\",\n",
        "    \"Ms office symbol extralight\",\n",
        "    \"Ms Reference speciality\",\n",
        "    \"Segoe MDL2 Assets\",\n",
        "    \"Siemreap\",\n",
        "    \"Sitara\",\n",
        "    \"Symbol\",\n",
        "    \"Wingdings\",\n",
        "    \"Metal\",\n",
        "    \"Ponnala\",\n",
        "    \"Webdings\",\n",
        "    \"Souliyo Unicode\",\n",
        "    \"Aguafina Script\",\n",
        "    \"Yantramanav Black\",\n",
        "    # \"Yaldevi\",\n",
        "    # Taprom,\n",
        "    # \"Zhi Mang Xing\",\n",
        "    # \"Taviraj\",\n",
        "    # \"SeoulNamsan EB\",\n",
        "]\n",
        "\n",
        "\n",
        "def valid_font(filename):\n",
        "    for name in INVALID_FONTS:\n",
        "        if name.lower() in str(filename).lower():\n",
        "            return False\n",
        "    return True"
      ],
      "metadata": {
        "id": "ecmjwzSkXuKn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import FloatTensor\n",
        "import dgl\n",
        "from dgl.data.utils import load_graphs\n",
        "from tqdm import tqdm\n",
        "from abc import abstractmethod\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import dgl\n",
        "from torch import FloatTensor, stack\n",
        "\n",
        "class BaseDataset(Dataset):\n",
        "    @staticmethod\n",
        "    def num_classes():\n",
        "        pass\n",
        "\n",
        "    def __init__(self, X_train, Y_train):\n",
        "        \"\"\"\n",
        "        self.data is a list of dictionaries with keys graph and label\n",
        "        \"\"\"\n",
        "        assert len(X_train) == len(Y_train), \"The number of graphs must match the number of labels\"\n",
        "        self.data = [{\"graph\": graph, \"label\": label} for graph, label in zip(X_train, Y_train)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        return sample[\"graph\"], sample[\"label\"]  # Returns a tuple of the sample graph and its corresponding label\n",
        "\n",
        "    def _collate(self, batch):\n",
        "        graphs, labels = zip(*batch)\n",
        "        batched_graph = dgl.batch(graphs)\n",
        "        num_nodes_per_graph = [graph.number_of_nodes() for graph in graphs]\n",
        "\n",
        "        # Create a default padding command vector\n",
        "        pad_vector = torch.tensor([6] + [-1]*16, dtype=torch.float32)\n",
        "\n",
        "        # Prepare labels with padding\n",
        "        padded_labels = []\n",
        "        for label in labels:\n",
        "            label_length = label.shape[0]\n",
        "            if label_length < 60:\n",
        "                # Calculate how many padding vectors are needed\n",
        "                padding_count = 60 - label_length\n",
        "                # Create a tensor of padding vectors\n",
        "                padding = pad_vector.repeat(padding_count, 1)\n",
        "                # Concatenate the original label with the padding\n",
        "                padded_label = torch.cat([torch.tensor(label, dtype=torch.float32), padding], dim=0)\n",
        "            else:\n",
        "                padded_label = torch.tensor(label, dtype=torch.float32)\n",
        "            padded_labels.append(padded_label)\n",
        "\n",
        "        # Stack all the padded labels into a single tensor\n",
        "        padded_labels = stack(padded_labels)\n",
        "        return {\"graph\": batched_graph, \"labels\": padded_labels, \"num_nodes\": num_nodes_per_graph}\n",
        "        # return {\"graph\": batched_graph, \"labels\": padded_labels}\n",
        "\n",
        "\n",
        "    def get_dataloader(self, batch_size=128, shuffle=True, num_workers=0):\n",
        "        return DataLoader(\n",
        "            self,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle,\n",
        "            collate_fn=self._collate,\n",
        "            num_workers=num_workers,  # Can be set to non-zero on Linux\n",
        "            drop_last=True\n",
        "        )\n"
      ],
      "metadata": {
        "id": "y7DSBZ2eWU0T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7db1a64c-f065-4566-ca0b-bdf664459052"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from dgl.nn.pytorch.conv import NNConv\n",
        "from dgl.nn.pytorch.glob import MaxPooling"
      ],
      "metadata": {
        "id": "RLIY6EKQYAzE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Layers"
      ],
      "metadata": {
        "id": "iqwTN_X5i8F_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _conv1d(in_channels, out_channels, kernel_size=3, padding=0, bias=False):\n",
        "    \"\"\"\n",
        "    Helper function to create a 1D convolutional layer with batchnorm and LeakyReLU activation\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Input channels\n",
        "        out_channels (int): Output channels\n",
        "        kernel_size (int, optional): Size of the convolutional kernel. Defaults to 3.\n",
        "        padding (int, optional): Padding size on each side. Defaults to 0.\n",
        "        bias (bool, optional): Whether bias is used. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        nn.Sequential: Sequential contained the Conv1d, BatchNorm1d and LeakyReLU layers\n",
        "    \"\"\"\n",
        "    return nn.Sequential(\n",
        "        nn.Conv1d(\n",
        "            in_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=bias\n",
        "        ),\n",
        "        nn.BatchNorm1d(out_channels),\n",
        "        nn.LeakyReLU(),\n",
        "    )\n",
        "\n",
        "\n",
        "def _conv2d(in_channels, out_channels, kernel_size, padding=0, bias=False):\n",
        "    \"\"\"\n",
        "    Helper function to create a 2D convolutional layer with batchnorm and LeakyReLU activation\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Input channels\n",
        "        out_channels (int): Output channels\n",
        "        kernel_size (int, optional): Size of the convolutional kernel. Defaults to 3.\n",
        "        padding (int, optional): Padding size on each side. Defaults to 0.\n",
        "        bias (bool, optional): Whether bias is used. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        nn.Sequential: Sequential contained the Conv2d, BatchNorm2d and LeakyReLU layers\n",
        "    \"\"\"\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=padding,\n",
        "            bias=bias,\n",
        "        ),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.LeakyReLU(),\n",
        "    )\n",
        "\n",
        "\n",
        "def _fc(in_features, out_features, bias=False):\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(in_features, out_features, bias=bias),\n",
        "        nn.BatchNorm1d(out_features),\n",
        "        nn.LeakyReLU(),\n",
        "    )"
      ],
      "metadata": {
        "id": "6PZVEEGri69Z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MLP Modules"
      ],
      "metadata": {
        "id": "cJkyB_cJjHAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class _MLP(nn.Module):\n",
        "    \"\"\"\"\"\"\n",
        "\n",
        "    def __init__(self, num_layers, input_dim, hidden_dim, output_dim):\n",
        "        \"\"\"\n",
        "        MLP with linear output\n",
        "        Args:\n",
        "            num_layers (int): The number of linear layers in the MLP\n",
        "            input_dim (int): Input feature dimension\n",
        "            hidden_dim (int): Hidden feature dimensions for all hidden layers\n",
        "            output_dim (int): Output feature dimension\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the given number of layers is <1\n",
        "        \"\"\"\n",
        "        super(_MLP, self).__init__()\n",
        "        self.linear_or_not = True  # default is linear model\n",
        "        self.num_layers = num_layers\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        if num_layers < 1:\n",
        "            raise ValueError(\"Number of layers should be positive!\")\n",
        "        elif num_layers == 1:\n",
        "            # Linear model\n",
        "            self.linear = nn.Linear(input_dim, output_dim)\n",
        "        else:\n",
        "            # Multi-layer model\n",
        "            self.linear_or_not = False\n",
        "            self.linears = torch.nn.ModuleList()\n",
        "            self.batch_norms = torch.nn.ModuleList()\n",
        "\n",
        "            self.linears.append(nn.Linear(input_dim, hidden_dim))\n",
        "            for layer in range(num_layers - 2):\n",
        "                self.linears.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "            self.linears.append(nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "            # TODO: this could move inside the above loop\n",
        "            for layer in range(num_layers - 1):\n",
        "                self.batch_norms.append(nn.BatchNorm1d((hidden_dim)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.linear_or_not:\n",
        "            # If linear model\n",
        "            return self.linear(x)\n",
        "        else:\n",
        "            # If MLP\n",
        "            h = x\n",
        "            for i in range(self.num_layers - 1):\n",
        "                h = F.relu(self.batch_norms[i](self.linears[i](h)))\n",
        "            return self.linears[-1](h)"
      ],
      "metadata": {
        "id": "_0HwNvTsjGCP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#UV-Net Curve Encoder and Surface Encoder"
      ],
      "metadata": {
        "id": "avV6r5l9jUcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UVNetCurveEncoder(nn.Module):\n",
        "    def __init__(self, in_channels=6, output_dims=64):\n",
        "        \"\"\"\n",
        "        This is the 1D convolutional network that extracts features from the B-rep edge\n",
        "        geometry described as 1D UV-grids (see Section 3.2, Curve & surface convolution\n",
        "        in paper)\n",
        "\n",
        "        Args:\n",
        "            in_channels (int, optional): Number of channels in the edge UV-grids. By default\n",
        "                                         we expect 3 channels for point coordinates and 3 for\n",
        "                                         curve tangents. Defaults to 6.\n",
        "            output_dims (int, optional): Output curve embedding dimension. Defaults to 64.\n",
        "        \"\"\"\n",
        "        super(UVNetCurveEncoder, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.conv1 = _conv1d(in_channels, 64, kernel_size=3, padding=1, bias=False)\n",
        "        self.conv2 = _conv1d(64, 128, kernel_size=3, padding=1, bias=False)\n",
        "        self.conv3 = _conv1d(128, 256, kernel_size=3, padding=1, bias=False)\n",
        "        self.final_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = _fc(256, output_dims, bias=False)\n",
        "\n",
        "        for m in self.modules():\n",
        "            self.weights_init(m)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, (nn.Linear, nn.Conv1d)):\n",
        "            torch.nn.init.kaiming_uniform_(m.weight.data)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.size(1) == self.in_channels\n",
        "        batch_size = x.size(0)\n",
        "        x = x.float()\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.final_pool(x)\n",
        "        x = x.view(batch_size, -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UVNetSurfaceEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=7,\n",
        "        output_dims=64,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        This is the 2D convolutional network that extracts features from the B-rep face\n",
        "        geometry described as 2D UV-grids (see Section 3.2, Curve & surface convolution\n",
        "        in paper)\n",
        "\n",
        "        Args:\n",
        "            in_channels (int, optional): Number of channels in the edge UV-grids. By default\n",
        "                                         we expect 3 channels for point coordinates and 3 for\n",
        "                                         surface normals and 1 for the trimming mask. Defaults\n",
        "                                         to 7.\n",
        "            output_dims (int, optional): Output surface embedding dimension. Defaults to 64.\n",
        "        \"\"\"\n",
        "        super(UVNetSurfaceEncoder, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.conv1 = _conv2d(in_channels, 64, 3, padding=1, bias=False)\n",
        "        self.conv2 = _conv2d(64, 128, 3, padding=1, bias=False)\n",
        "        self.conv3 = _conv2d(128, 256, 3, padding=1, bias=False)\n",
        "        self.final_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = _fc(256, output_dims, bias=False)\n",
        "        for m in self.modules():\n",
        "            self.weights_init(m)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
        "            torch.nn.init.kaiming_uniform_(m.weight.data)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.size(1) == self.in_channels\n",
        "        batch_size = x.size(0)\n",
        "        x = x.float()\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.final_pool(x)\n",
        "        x = x.view(batch_size, -1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "HnLy3iw8jSGQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Encoder"
      ],
      "metadata": {
        "id": "QT8BzVuwjhuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class _EdgeConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        edge_feats,\n",
        "        out_feats,\n",
        "        node_feats,\n",
        "        num_mlp_layers=2,\n",
        "        hidden_mlp_dim=64,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        This module implements Eq. 2 from the paper where the edge features are\n",
        "        updated using the node features at the endpoints.\n",
        "\n",
        "        Args:\n",
        "            edge_feats (int): Input edge feature dimension\n",
        "            out_feats (int): Output feature deimension\n",
        "            node_feats (int): Input node feature dimension\n",
        "            num_mlp_layers (int, optional): Number of layers used in the MLP. Defaults to 2.\n",
        "            hidden_mlp_dim (int, optional): Hidden feature dimension in the MLP. Defaults to 64.\n",
        "        \"\"\"\n",
        "        super(_EdgeConv, self).__init__()\n",
        "        self.proj = _MLP(1, node_feats, hidden_mlp_dim, edge_feats)\n",
        "        self.mlp = _MLP(num_mlp_layers, edge_feats, hidden_mlp_dim, out_feats)\n",
        "        self.batchnorm = nn.BatchNorm1d(out_feats)\n",
        "        self.eps = torch.nn.Parameter(torch.FloatTensor([0.0]))\n",
        "\n",
        "    def forward(self, graph, nfeat, efeat):\n",
        "        src, dst = graph.edges()\n",
        "        proj1, proj2 = self.proj(nfeat[src]), self.proj(nfeat[dst])\n",
        "        agg = proj1 + proj2\n",
        "        h = self.mlp((1 + self.eps) * efeat + agg)\n",
        "        h = F.leaky_relu(self.batchnorm(h))\n",
        "        return h\n",
        "\n",
        "\n",
        "class _NodeConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        node_feats,\n",
        "        out_feats,\n",
        "        edge_feats,\n",
        "        num_mlp_layers=2,\n",
        "        hidden_mlp_dim=64,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        This module implements Eq. 1 from the paper where the node features are\n",
        "        updated using the neighboring node and edge features.\n",
        "\n",
        "        Args:\n",
        "            node_feats (int): Input edge feature dimension\n",
        "            out_feats (int): Output feature deimension\n",
        "            node_feats (int): Input node feature dimension\n",
        "            num_mlp_layers (int, optional): Number of layers used in the MLP. Defaults to 2.\n",
        "            hidden_mlp_dim (int, optional): Hidden feature dimension in the MLP. Defaults to 64.\n",
        "        \"\"\"\n",
        "        super(_NodeConv, self).__init__()\n",
        "        self.gconv = NNConv(\n",
        "            in_feats=node_feats,\n",
        "            out_feats=out_feats,\n",
        "            edge_func=nn.Linear(edge_feats, node_feats * out_feats),\n",
        "            aggregator_type=\"sum\",\n",
        "            bias=False,\n",
        "        )\n",
        "        self.batchnorm = nn.BatchNorm1d(out_feats)\n",
        "        self.mlp = _MLP(num_mlp_layers, node_feats, hidden_mlp_dim, out_feats)\n",
        "        self.eps = torch.nn.Parameter(torch.FloatTensor([0.0]))\n",
        "\n",
        "    def forward(self, graph, nfeat, efeat):\n",
        "        h = (1 + self.eps) * nfeat\n",
        "        h = self.gconv(graph, h, efeat)\n",
        "        h = self.mlp(h)\n",
        "        h = F.leaky_relu(self.batchnorm(h))\n",
        "        return h\n",
        "\n",
        "\n",
        "class UVNetGraphEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        input_edge_dim,\n",
        "        output_dim,\n",
        "        hidden_dim=64,\n",
        "        learn_eps=True,\n",
        "        num_layers=3,\n",
        "        num_mlp_layers=2,\n",
        "    ):\n",
        "        super(UVNetGraphEncoder, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.learn_eps = learn_eps\n",
        "\n",
        "        # List of layers for node and edge feature message passing\n",
        "        self.node_conv_layers = torch.nn.ModuleList()\n",
        "        self.edge_conv_layers = torch.nn.ModuleList()\n",
        "\n",
        "        for layer in range(self.num_layers - 1):\n",
        "            node_feats = input_dim if layer == 0 else hidden_dim\n",
        "            edge_feats = input_edge_dim if layer == 0 else hidden_dim\n",
        "            self.node_conv_layers.append(\n",
        "                _NodeConv(\n",
        "                    node_feats=node_feats,\n",
        "                    out_feats=hidden_dim,\n",
        "                    edge_feats=edge_feats,\n",
        "                    num_mlp_layers=num_mlp_layers,\n",
        "                    hidden_mlp_dim=hidden_dim,\n",
        "                ),\n",
        "            )\n",
        "            self.edge_conv_layers.append(\n",
        "                _EdgeConv(\n",
        "                    edge_feats=edge_feats,\n",
        "                    out_feats=hidden_dim,\n",
        "                    node_feats=node_feats,\n",
        "                    num_mlp_layers=num_mlp_layers,\n",
        "                    hidden_mlp_dim=hidden_dim,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Linear function for graph poolings of output of each layer\n",
        "        # which maps the output of different layers into a prediction score\n",
        "        self.linears_prediction = torch.nn.ModuleList()\n",
        "\n",
        "        for layer in range(num_layers):\n",
        "            if layer == 0:\n",
        "                self.linears_prediction.append(nn.Linear(input_dim, output_dim))\n",
        "            else:\n",
        "                self.linears_prediction.append(nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "        self.drop1 = nn.Dropout(0.3)\n",
        "        self.drop = nn.Dropout(0.5)\n",
        "        self.pool = MaxPooling()\n",
        "\n",
        "    def forward(self, g, h, efeat):\n",
        "        hidden_rep = [h]\n",
        "        he = efeat\n",
        "\n",
        "        for i in range(self.num_layers - 1):\n",
        "            # Update node features\n",
        "            h = self.node_conv_layers[i](g, h, he)\n",
        "            # Update edge features\n",
        "            he = self.edge_conv_layers[i](g, h, he)\n",
        "            hidden_rep.append(h)\n",
        "        print(f'hidden_rep is {type(hidden_rep)}')\n",
        "        # Use the node embeddings from the last layer\n",
        "        node_embeddings = hidden_rep[-1]\n",
        "        # print(f'node_embeddings_0 = {node_embeddings.shape}')\n",
        "        node_embeddings = self.drop1(node_embeddings)\n",
        "        # print(f'node_embeddings_1 = {node_embeddings.shape}')\n",
        "\n",
        "        # Optional: Perform pooling to get a graph-level representation\n",
        "        graph_representation = 0\n",
        "        for i, h in enumerate(hidden_rep):\n",
        "            pooled_h = self.pool(g, h)\n",
        "            graph_representation += self.drop(self.linears_prediction[i](pooled_h))\n",
        "\n",
        "        print(f'node_embeddings_2.shape = {node_embeddings.shape}')\n",
        "        print(f'graph_representation.shape = {graph_representation.shape}')\n",
        "\n",
        "        return node_embeddings, graph_representation"
      ],
      "metadata": {
        "id": "NUP_JKC8jeyn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Encoder"
      ],
      "metadata": {
        "id": "2EHAEpDXi09u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CADEncoder(nn.Module):\n",
        "  def __init__(self, crv_emb_dim=64, srf_emb_dim=64, graph_emb_dim=128, dropout=0.3):\n",
        "    super(CADEncoder, self).__init__()\n",
        "    self.curv_encoder = UVNetCurveEncoder(in_channels=10, output_dims=crv_emb_dim) # in_channels originally 6\n",
        "    self.surf_encoder = UVNetSurfaceEncoder(in_channels=10, output_dims=srf_emb_dim)\n",
        "    self.graph_encoder = UVNetGraphEncoder(srf_emb_dim, crv_emb_dim, graph_emb_dim)\n",
        "\n",
        "  def forward(self, batched_graph):\n",
        "    input_crv_feat = batched_graph.edata[\"x\"]\n",
        "    input_srf_feat = batched_graph.ndata[\"x\"]\n",
        "    hidden_crv_feat = self.curv_encoder(input_crv_feat)\n",
        "    hidden_srf_feat = self.surf_encoder(input_srf_feat)\n",
        "    node_emb, graph_emb = self.graph_encoder(batched_graph, hidden_srf_feat, hidden_crv_feat)\n",
        "    return node_emb, graph_emb"
      ],
      "metadata": {
        "id": "xNNA04oRYQeb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
        "import math"
      ],
      "metadata": {
        "id": "lDIyR0NijynS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Embedding"
      ],
      "metadata": {
        "id": "TkL1vEFmj_Wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncodingLUT(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=250):\n",
        "        super(PositionalEncodingLUT, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(0, max_len, dtype=torch.long).unsqueeze(1)\n",
        "        self.register_buffer('position', position)\n",
        "\n",
        "        self.pos_embed = nn.Embedding(max_len, d_model)\n",
        "\n",
        "        self._init_embeddings()\n",
        "\n",
        "    def _init_embeddings(self):\n",
        "        nn.init.kaiming_normal_(self.pos_embed.weight, mode=\"fan_in\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, _ = x.shape\n",
        "\n",
        "        # Get positional encodings for the sequence length\n",
        "        pos = self.position[:seq_length]  # This will have shape [seq_length, 1]\n",
        "\n",
        "        # Expand positional encodings to cover the whole batch\n",
        "        pos = pos.expand(-1, batch_size).contiguous()  # Reshape to [seq_length, batch_size]\n",
        "        pos = pos.transpose(0, 1)  # Transpose to [batch_size, seq_length]\n",
        "\n",
        "        # Retrieve positional embeddings\n",
        "        pos_embeddings = self.pos_embed(pos)  # This should now be [batch_size, seq_length, d_model]\n",
        "\n",
        "        # Element-wise addition of embeddings to input x\n",
        "        x = x + pos_embeddings\n",
        "\n",
        "        # Apply dropout and return\n",
        "        return self.dropout(x)\n",
        "\n",
        "class CADEmbedding(nn.Module):\n",
        "    \"\"\"Embedding: positional embed + command embed + parameter embed + group embed (optional)\n",
        "\n",
        "    Note: d_model could also be 257\n",
        "    \"\"\"\n",
        "    # I think the d_model should be 64 dim\n",
        "    # n_args should be 16\n",
        "    # Shouldn't the embedding be that command embed + param embed + positional embed?\n",
        "    def __init__(self, n_commands=7, d_model=64, n_args=16, args_dim=257, seq_len=60, use_group=False, group_len=None):\n",
        "        super(CADEmbedding, self).__init__()\n",
        "\n",
        "        self.command_embed = nn.Embedding(n_commands, d_model)\n",
        "\n",
        "        # args_dim = args_dim + 1\n",
        "        self.arg_embed = nn.Embedding(args_dim, d_model, padding_idx=0)\n",
        "        self.embed_fcn = nn.Linear(d_model * n_args, d_model)\n",
        "\n",
        "        # use_group: additional embedding for each sketch-extrusion pair\n",
        "        # self.use_group = use_group\n",
        "        # if use_group:\n",
        "        #     if group_len is None:\n",
        "        #         group_len = cfg.max_num_groups\n",
        "        #     self.group_embed = nn.Embedding(group_len + 2, cfg.d_model)\n",
        "\n",
        "        self.pos_encoding = PositionalEncodingLUT(d_model, max_len=seq_len+2)\n",
        "\n",
        "    def forward(self, commands, args, groups=None):\n",
        "        S, N = commands.shape\n",
        "\n",
        "        # print(f'S = {S}')\n",
        "        # print(f'N = {N}')\n",
        "        src = self.command_embed(commands.long()) + \\\n",
        "              self.embed_fcn(self.arg_embed((args + 1).long()).view(S, N, -1))  # shift due to -1 PAD_VAL\n",
        "\n",
        "        # if self.use_group:\n",
        "        #     src = src + self.group_embed(groups.long())\n",
        "\n",
        "        src = self.pos_encoding(src)\n",
        "\n",
        "        return src\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# class CADCommandEmbedding(nn.Module):\n",
        "#     def __init__(self, d_E=512, n_commands=12, n_params=19, n_args, seq_len=100):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.d_E = d_E\n",
        "#         self.n_params = n_params\n",
        "\n",
        "#         # Command type embedding\n",
        "#         self.command_type_embed = nn.Embedding(n_commands, d_E)\n",
        "\n",
        "#         # Command parameters matrix and embedding layer\n",
        "#         self.param_matrix = nn.Linear(257, d_E)\n",
        "#         self.param_embed = nn.Linear(d_E * n_params, d_E)\n",
        "\n",
        "#         # Index (positional) embedding\n",
        "#         self.index_embed = nn.Embedding(seq_len, d_E)\n",
        "\n",
        "#     def forward(self, command_types, command_params, indices):\n",
        "#         # Embed command types using one-hot vectors\n",
        "#         command_type_embeddings = self.command_type_embed(command_types)\n",
        "\n",
        "#         # Process and embed command parameters\n",
        "#         # Assuming command_params i s a batch of one-hot encoded matrices for parameters\n",
        "#         flat_params = self.param_matrix(command_params.view(-1, 257))\n",
        "#         flat_params = flat_params.view(-1, self.n_params * self.d_E)\n",
        "#         command_param_embeddings = self.param_embed(flat_params)\n",
        "\n",
        "#         # Embed index (position)\n",
        "#         index_embeddings = self.index_embed(indices)\n",
        "\n",
        "#         # Combine all embeddings\n",
        "#         embeddings = command_type_embeddings + command_param_embeddings + index_embeddings\n",
        "#         return embeddings"
      ],
      "metadata": {
        "id": "YG5xqOtljzRZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fusion Module"
      ],
      "metadata": {
        "id": "GOCSqKYWkCYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class FusionModule(nn.Module):\n",
        "#     def __init__(self, latent_size, num_commands, command_embedding_size=257, hidden_size=256, output_size=128):\n",
        "#         \"\"\"\n",
        "#         Args:\n",
        "#             latent_size: dimension of z, the latent vector produced by encoder\n",
        "#             command_embedding_size: dimension of command embedding, size 257 in the paper\n",
        "#             hidden_size: perhaps the paper mentioned what its exact value is somewhere...\n",
        "#             output_size: input dimension into the standard transformer decoder\n",
        "#         \"\"\"\n",
        "#         super(FusionModule, self).__init__()\n",
        "#         self.fc1 = nn.Linear(latent_size + command_embedding_size * num_commands, hidden_size)\n",
        "#         self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "#     def forward(self, latent, command_embeddings):\n",
        "#         combined = torch.cat((latent, command_embeddings), dim=1)\n",
        "#         x = self.fc1(combined)\n",
        "#         x = F.relu(x)\n",
        "#         x = self.fc2(x)\n",
        "#         x = F.relu(x)\n",
        "#         return x\n",
        "\n",
        "# class FusionModule(nn.Module):\n",
        "#     def __init__(self, latent_size, command_embedding_size=257, N_c=60, hidden_size=256, output_size=128):\n",
        "#         super(FusionModule, self).__init__()\n",
        "#         self.latent_size = latent_size\n",
        "#         self.command_embedding_size = command_embedding_size\n",
        "#         self.N_c = N_c\n",
        "#         self.hidden_size = hidden_size\n",
        "#         self.output_size = output_size\n",
        "\n",
        "\n",
        "#         # Initial dummy parameters to be replaced on the first forward pass\n",
        "#         self.fc1 = nn.Linear(latent_size + command_embedding_size, hidden_size)  # Placeholder\n",
        "#         self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "#     def forward(self, latent, command_embeddings):\n",
        "#         command_embeddings_flat = command_embeddings.view(command_embeddings.size(0), -1)\n",
        "#         combined = torch.cat((latent, command_embeddings_flat), dim=1)\n",
        "\n",
        "#         # required_size = self.latent_size + self.command_embedding_size * self.N_c\n",
        "\n",
        "#         current_size = combined.size(1)\n",
        "#         if current_size < required_size:\n",
        "#             padding_size = required_size - current_size\n",
        "#             combined = F.pad(combined, (0, padding_size))\n",
        "\n",
        "#         x = self.fc1(combined)\n",
        "#         x = F.relu(x)\n",
        "#         x = self.fc2(x)\n",
        "#         x = F.relu(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "class FusionModule(nn.Module):\n",
        "    # THe command_embedding size, hidden size, and output size needs to be changed.\n",
        "    def __init__(self, latent_size, command_embedding_size=64, N_c=60, hidden_size=128, output_size=64):\n",
        "        super(FusionModule, self).__init__()\n",
        "        self.latent_size = latent_size\n",
        "        self.command_embedding_size = command_embedding_size\n",
        "        self.N_c = N_c\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "\n",
        "        # Initial dummy parameters to be replaced on the first forward pass\n",
        "        # THis needs to be changed\n",
        "        self.fc1 = nn.Linear(latent_size + command_embedding_size, hidden_size)  # Placeholder\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, latent, command_embedding):\n",
        "        # command_embeddings_flat = command_embeddings.view(command_embeddings.size(0), -1)\n",
        "        print('command embedding shape =', command_embedding.shape)\n",
        "        command_embedding = torch.unsqueeze(command_embedding, 1)\n",
        "        combined = torch.cat((latent, command_embedding), dim=2)\n",
        "\n",
        "        # required_size = self.latent_size + self.command_embedding_size * self.N_c\n",
        "\n",
        "        # current_size = combined.size(1)\n",
        "        # if current_size < required_size:\n",
        "        #     padding_size = required_size - current_size\n",
        "        #     combined = F.pad(combined, (0, padding_size))\n",
        "\n",
        "        x = self.fc1(combined)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "mKHlq6Ijj4BF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Decoder"
      ],
      "metadata": {
        "id": "GbkyVQMikDwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import Tensor\n",
        "from typing import Optional\n",
        "\n",
        "class CausalTransformerDecoder(nn.TransformerDecoder):\n",
        "    \"\"\"Implementation of a transformer decoder based on torch implementation but\n",
        "    more efficient. The difference is that it doesn't need to recompute the\n",
        "    embeddings of all the past decoded tokens but instead uses a cache to\n",
        "    store them. This makes use of the fact that the attention of a decoder is\n",
        "    causal, so new predicted tokens don't affect the old tokens' embedding bc\n",
        "    the corresponding attention cells are masked.\n",
        "    The complexity goes from seq_len^3 to seq_len^2.\n",
        "\n",
        "    This only happens in eval mode.\n",
        "    In training mode, teacher forcing makes these optimizations unnecessary. Hence the\n",
        "    Decoder acts like a regular nn.TransformerDecoder (except that the attention tgt\n",
        "    masks are handled for you).\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        tgt: Tensor,\n",
        "        memory: Optional[Tensor] = None,\n",
        "        cache: Optional[Tensor] = None,\n",
        "        memory_mask: Optional[Tensor] = None,\n",
        "        tgt_key_padding_mask: Optional[Tensor] = None,\n",
        "        memory_key_padding_mask: Optional[Tensor] = None,\n",
        "    ) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tgt (Tensor): current_len_output x bsz x hidden_dim\n",
        "            memory (Tensor): len_encoded_seq x bsz x hidden_dim\n",
        "            cache (Optional[Tensor]):\n",
        "                n_layers x (current_len_output - 1) x bsz x hidden_dim\n",
        "                If current_len_output == 1, nothing is cached yet, so cache\n",
        "                should be None. Same if the module is in training mode.\n",
        "            others (Optional[Tensor]): see official documentations\n",
        "        Returns:\n",
        "            output (Tensor): current_len_output x bsz x hidden_dim\n",
        "            cache (Optional[Tensor]): n_layers x current_len_output x bsz x hidden_dim\n",
        "                Only returns it when module is in eval mode (no caching in training)\n",
        "        \"\"\"\n",
        "\n",
        "        output = tgt\n",
        "        print(f'output.shape = {output.shape}')\n",
        "\n",
        "        if self.training:\n",
        "            if cache is not None:\n",
        "                raise ValueError(\"cache parameter should be None in training mode\")\n",
        "            for mod in self.layers:\n",
        "                output = mod(\n",
        "                    output,\n",
        "                    memory,\n",
        "                    memory_mask=memory_mask,\n",
        "                    tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                    memory_key_padding_mask=memory_key_padding_mask,\n",
        "                )\n",
        "\n",
        "            return output\n",
        "\n",
        "        new_token_cache = []\n",
        "\n",
        "        for i, mod in enumerate(self.layers):\n",
        "            output = mod(output, memory)\n",
        "            new_token_cache.append(output)\n",
        "            if cache is not None:\n",
        "                output = torch.cat([cache[i], output], dim=0)\n",
        "\n",
        "        if cache is not None:\n",
        "            new_cache = torch.cat([cache, torch.stack(new_token_cache, dim=0)], dim=1)\n",
        "        else:\n",
        "            new_cache = torch.stack(new_token_cache, dim=0)\n",
        "\n",
        "        return output, new_cache\n",
        "\n",
        "\n",
        "class CausalTransformerDecoderLayer(nn.TransformerDecoderLayer):\n",
        "    def __init__(self, d_model, nhead=6, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
        "        super().__init__(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            activation=activation,\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        tgt: torch.Tensor,\n",
        "        memory: Optional[torch.Tensor] = None,\n",
        "        tgt_mask: Optional[torch.Tensor] = None,\n",
        "        memory_mask: Optional[torch.Tensor] = None,\n",
        "        tgt_key_padding_mask: Optional[torch.Tensor] = None,\n",
        "        memory_key_padding_mask: Optional[torch.Tensor] = None,\n",
        "    ) -> torch.Tensor:\n",
        "        if self.training:\n",
        "            # In training mode, follow the standard procedure including masking\n",
        "            return super().forward(\n",
        "                tgt,\n",
        "                memory,\n",
        "                tgt_mask=self._generate_causal_mask(tgt.size(0), tgt.device),\n",
        "                memory_mask=memory_mask,\n",
        "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                memory_key_padding_mask=memory_key_padding_mask,\n",
        "            )\n",
        "        else:\n",
        "            # In evaluation mode, proceed with the autoregressive manner\n",
        "\n",
        "            # Handle the last token from the sequence only\n",
        "            tgt_last_tok = tgt[-1:, :, :]\n",
        "\n",
        "            # Perform self-attention on the last token\n",
        "            tgt_last_tok = self.self_attn(\n",
        "                tgt_last_tok,\n",
        "                tgt,\n",
        "                tgt,\n",
        "                attn_mask=None,  # no need for mask since we're only concerned with the last token\n",
        "                key_padding_mask=tgt_key_padding_mask,\n",
        "            )[0] + tgt_last_tok\n",
        "            tgt_last_tok = self.norm1(tgt_last_tok)\n",
        "\n",
        "            # Perform cross-attention with the memory (encoder's output)\n",
        "            if memory is not None:\n",
        "                tgt_last_tok = self.multihead_attn(\n",
        "                    tgt_last_tok,\n",
        "                    memory,\n",
        "                    memory,\n",
        "                    attn_mask=memory_mask,\n",
        "                    key_padding_mask=memory_key_padding_mask,\n",
        "                )[0] + tgt_last_tok\n",
        "                tgt_last_tok = self.norm2(tgt_last_tok)\n",
        "\n",
        "            # Pass through the final feed-forward network\n",
        "            tgt_last_tok = self.linear2(self.dropout(self.activation(self.linear1(tgt_last_tok)))) + tgt_last_tok\n",
        "            tgt_last_tok = self.norm3(tgt_last_tok)\n",
        "\n",
        "            return tgt_last_tok\n",
        "\n",
        "    @staticmethod\n",
        "    def _generate_causal_mask(sz, device):\n",
        "        \"\"\"\n",
        "        Generates a causal mask to hide future tokens for autoregressive tasks.\n",
        "        \"\"\"\n",
        "        mask = torch.full((sz, sz), float('-inf'))\n",
        "        mask_cond = torch.triu(mask, diagonal=1)\n",
        "        return mask_cond.to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "1UVkhDRrsLLs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CADDecoder(nn.Module):\n",
        "    def __init__(self, d_model=128, nhead=8, num_decoder_layers=4,\n",
        "                 dim_feedforward=2048, dropout=0.1, activation=\"relu\",\n",
        "                 num_commands=7, max_seq_len=5000, num_parameters=16, param_cat=257, num_embeddings=128):\n",
        "        super(CADDecoder, self).__init__()\n",
        "\n",
        "        # embedding\n",
        "        # self.cad_command_embedding = CADEmbedding(d_model, num_commands, max_seq_len, num_parameters, num_embeddings)\n",
        "        self.param_cat = param_cat\n",
        "        self.cad_command_embedding = CADEmbedding()\n",
        "\n",
        "        # fusion module\n",
        "        self.fusion_module = FusionModule(d_model) # not too sure about the dimensions, needs checking\n",
        "\n",
        "        # decoder\n",
        "        decoder_layer = CausalTransformerDecoderLayer(d_model=64, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, activation=activation)\n",
        "        self.transformer_decoder = CausalTransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "\n",
        "        # final output\n",
        "        # self.output_layer1 = nn.Linear(d_model, num_parameters + 1)\n",
        "\n",
        "        # implementation with softmax\n",
        "        self.output_layer1 = nn.Linear(d_model, num_commands) # t_i\n",
        "        self.output_layer2 = nn.Linear(d_model, num_parameters * param_cat) # p_i\n",
        "        # self.output_layer2 = nn.Linear(d_model, num_parameters) # p_i\n",
        "\n",
        "    def forward(self, node_embeddings, graph_embeddings, command_seq,\n",
        "                tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            latent: 2D latent vector from Encoder\n",
        "            command_types: tensor of command type indices\n",
        "            command_params: tensor of command parameter indices\n",
        "            command_indices: tensor of command indices in the sequence\n",
        "        \"\"\"\n",
        "\n",
        "        # embedding\n",
        "        commands_count = command_seq.shape[1]\n",
        "        commands = command_seq[:, :, 0]\n",
        "        args = command_seq[:, :, 1:]\n",
        "        # print(\"commands shape\", commands.shape)\n",
        "        # print(\"commands\",commands)\n",
        "        # print(\"args\", args.shape)\n",
        "        # print(\"args\",args)\n",
        "        construct_embed = self.cad_command_embedding(commands, args)\n",
        "        # print(\"construct_embd\",construct_embed.shape)\n",
        "\n",
        "        fusion_outputs = None\n",
        "        # fusion module\n",
        "        for i in range(commands_count):\n",
        "            fusion_output = self.fusion_module(graph_embeddings, construct_embed[:,i,:])\n",
        "\n",
        "            if fusion_outputs is None:\n",
        "                fusion_outputs = fusion_output.unsqueeze(1)  # Add an extra dimension for concatenation\n",
        "            else:\n",
        "                fusion_outputs = torch.cat((fusion_outputs, fusion_output.unsqueeze(1)), dim=1)\n",
        "\n",
        "        # fusion module\n",
        "        # fusion_output = self.fusion_module(latent, construct_embed)\n",
        "\n",
        "        # decoder\n",
        "        print(f'fusion_output.shape = {fusion_output.shape}')\n",
        "        print(f'node_embeddings.shape = {node_embeddings.shape}')\n",
        "        decoder_output = self.transformer_decoder(tgt=node_embeddings,\n",
        "                                          memory=fusion_output,\n",
        "                                          memory_mask=memory_mask,\n",
        "                                          tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                                          memory_key_padding_mask=memory_key_padding_mask)\n",
        "\n",
        "        # final output\n",
        "        # output1 = self.output_layer1(decoder_output)\n",
        "        # output1 = F.relu(output1)\n",
        "\n",
        "        # return output1\n",
        "\n",
        "\n",
        "        # softmax implementation\n",
        "        output1 = self.output_layer1(decoder_output)\n",
        "        output1 = F.softmax(output1) # t_i\n",
        "\n",
        "        output2 = self.output_layer2(decoder_output)\n",
        "        output2 = output2.view(-1, 16, self.param_cat)\n",
        "        output2 = F.softmax(output2, dim=2) # p_i\n",
        "\n",
        "        return output1, output2\n",
        "\n",
        "\n",
        "# class CADDecoder(nn.Module):\n",
        "#     def __init__(self, d_model=128, nhead=8, num_decoder_layers=4,\n",
        "#                  dim_feedforward=2048, dropout=0.1, activation=\"relu\",\n",
        "#                  num_commands=6, max_seq_len=5000, num_parameters=16, num_embeddings=128):\n",
        "#         super(CADDecoder, self).__init__()\n",
        "\n",
        "#         # embedding\n",
        "#         # self.cad_command_embedding = CADEmbedding(d_model, num_commands, max_seq_len, num_parameters, num_embeddings)\n",
        "#         self.cad_command_embedding = CADEmbedding()\n",
        "\n",
        "#         # fusion module\n",
        "#         self.fusion_module = FusionModule(d_model) # not too sure about the dimensions, needs checking\n",
        "\n",
        "#         # decoder\n",
        "#         decoder_layer = CausalTransformerDecoderLayer(d_model=128, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, activation=activation)\n",
        "#         self.transformer_decoder = CausalTransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "\n",
        "#         # final output\n",
        "#         # self.output_layer1 = nn.Linear(d_model, num_parameters + 1)\n",
        "\n",
        "#         # implementation with softmax\n",
        "#         self.output_layer1 = nn.Linear(d_model, num_commands) # t_i\n",
        "#         self.output_layer2 = nn.Linear(d_model, num_parameters) # p_i\n",
        "\n",
        "#     def forward(self, latent, command_seq,\n",
        "#                 tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "#         \"\"\"\n",
        "#         Args:\n",
        "#             latent: 2D latent vector from Encoder\n",
        "#             command_types: tensor of command type indices\n",
        "#             command_params: tensor of command parameter indices\n",
        "#             command_indices: tensor of command indices in the sequence\n",
        "#         \"\"\"\n",
        "\n",
        "#         # embedding\n",
        "#         commands_count = command_seq.shape[1]\n",
        "#         commands = command_seq[:, :, 0]\n",
        "#         args = command_seq[:, :, 1:]\n",
        "#         construct_embed = self.cad_command_embedding(commands, args)\n",
        "#         fusion_outputs = None\n",
        "#         # fusion module\n",
        "#         for i in range(commands_count):\n",
        "#             fusion_output = self.fusion_module(latent, construct_embed[:,i,:])\n",
        "\n",
        "#             if fusion_outputs is None:\n",
        "#                 fusion_outputs = fusion_output.unsqueeze(1)  # Add an extra dimension for concatenation\n",
        "#             else:\n",
        "#                 fusion_outputs = torch.cat((fusion_outputs, fusion_output.unsqueeze(1)), dim=1)\n",
        "\n",
        "#         print(f'fusion_outputs.shape = {fusion_outputs.shape}')\n",
        "#         # decoder\n",
        "#         decoder_output = self.transformer_decoder(tgt=fusion_outputs,\n",
        "#                                           memory=latent,\n",
        "#                                           memory_mask=memory_mask,\n",
        "#                                           tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "#                                           memory_key_padding_mask=memory_key_padding_mask)\n",
        "\n",
        "#         # final output\n",
        "#         # output1 = self.output_layer1(decoder_output)\n",
        "#         # output1 = F.relu(output1)\n",
        "\n",
        "#         # return output1\n",
        "\n",
        "\n",
        "#         # softmax implementation\n",
        "#         output1 = self.output_layer1(decoder_output)\n",
        "#         output1 = F.softmax(output1) # t_i\n",
        "\n",
        "#         output2 = self.output_layer2(decoder_output)\n",
        "#         output2 = F.softmax(output2) # p_i\n",
        "\n",
        "#         return output1, output2"
      ],
      "metadata": {
        "id": "ZWKOsU2Dj5lZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CadParser"
      ],
      "metadata": {
        "id": "XBx_j4lGNL6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Model"
      ],
      "metadata": {
        "id": "HYv8XmX7mRqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CADParserModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CADParserModel, self).__init__()\n",
        "\n",
        "        # encoder\n",
        "        self.cad_encoder = CADEncoder()\n",
        "\n",
        "        # decoder\n",
        "        self.cad_decoder = CADDecoder()\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, graph_input, label_input):\n",
        "        # encoder\n",
        "        latent = self.cad_encoder(graph_input)\n",
        "        # print(\"LATENT VECTOR\", latent.shape)\n",
        "        print(latent)\n",
        "        # decoder\n",
        "        output1, output2 = self.cad_decoder(latent, label_input)\n",
        "        return output1, output2"
      ],
      "metadata": {
        "id": "MCaKR6Y4mRby"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "4U893YmZmS2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "print(dgl.__version__)\n",
        "print(torch.__version__)\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Um7MfbneKyWj",
        "outputId": "3a990659-3e45-4669-f125-9991f7f6b592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n",
            "2.2.1+cu121\n",
            "Thu May 16 21:46:19 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "J7Vd3V0ijv4k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88280b4e-ce0f-4488-bccb-e539636841ca"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "import numpy as np\n",
        "\n",
        "# graphs, label_dict = dgl.load_graphs(\"/content/drive/My Drive/DeepCADDataset/all_graphs.bin\")\n",
        "# print(graphs)\n",
        "# print(label_dict)\n",
        "\n",
        "graphs, label_dict = dgl.load_graphs(\"/content/drive/My Drive/DeepCADDataset/all_graphs.bin\")\n",
        "print(label_dict)\n",
        "print(len(graphs))\n",
        "print(graphs[0])\n",
        "npz = np.load(\"/content/drive/My Drive/DeepCADDataset/all_npz.npz\")\n",
        "print(\"type: \",type(npz))\n",
        "print(\"length: \",len(npz))\n",
        "print(npz)\n",
        "print(npz['vec_0'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFOnO9QGnd8d",
        "outputId": "a51a1a9e-ff5c-42c5-b3c3-a3d93430c877"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{}\n",
            "7059\n",
            "Graph(num_nodes=6, num_edges=24,\n",
            "      ndata_schemes={'x': Scheme(shape=(10, 10, 7), dtype=torch.float64)}\n",
            "      edata_schemes={'x': Scheme(shape=(10, 6), dtype=torch.float64)})\n",
            "type:  <class 'numpy.lib.npyio.NpzFile'>\n",
            "length:  7059\n",
            "NpzFile '/content/drive/My Drive/DeepCADDataset/all_npz.npz' with keys: vec_0, vec_1, vec_2, vec_3, vec_4...\n",
            "[[  4  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1]\n",
            " [  0 223 128  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1]\n",
            " [  0 223 223  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1]\n",
            " [  0 128 223  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1]\n",
            " [  0 128 128  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1]\n",
            " [  5  -1  -1  -1  -1  -1 192 192  64  32 128 151   3 161 128   0   0]\n",
            " [  3  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "\n",
        "# Assuming 'graphs' is your list of DGLGraph objects\n",
        "max_nodes = 0\n",
        "\n",
        "for graph in graphs:\n",
        "    num_nodes = graph.number_of_nodes()\n",
        "    if num_nodes > max_nodes:\n",
        "        max_nodes = num_nodes\n",
        "\n",
        "print(\"Maximum number of nodes in any graph:\", max_nodes) # counting max nodes for padding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMirkSO3NrH5",
        "outputId": "333b8d7d-1a48-42f1-de58-1fcbb3647e22"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum number of nodes in any graph: 66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = []\n",
        "\n",
        "# Iterate over the sorted keys to maintain the order\n",
        "for key in npz.keys():\n",
        "    # print(key)\n",
        "    Y.append(npz[key])"
      ],
      "metadata": {
        "id": "HKmRp0pbntI5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = graphs"
      ],
      "metadata": {
        "id": "6FrhQ6L3zbx3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=42) # train size = 6353"
      ],
      "metadata": {
        "id": "hwHqc42DztQ5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPWMe3Pmu9aJ",
        "outputId": "7663bf26-0697-42a8-fdec-d5baaf14598d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seednumber=2024\n",
        "torch.manual_seed(seednumber)\n",
        "torch.cuda.manual_seed(seednumber)\n",
        "np.random.seed(seednumber)"
      ],
      "metadata": {
        "id": "yeqTx7CPXNSr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "torch.set_printoptions(threshold=10_000)\n",
        "# Path where checkpoints are stored\n",
        "checkpoint_dir = '/content/drive/My Drive/CADParser_Checkpoint_kevin_v1'\n",
        "loss_log_file_path = os.path.join(checkpoint_dir, 'loss_log.txt')\n",
        "\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "num_epochs = 100\n",
        "initial_learning_rate = 1e-3\n",
        "batch_size = 96  # Adjust according to your GPU memory\n",
        "warmup_epochs = 10\n",
        "root_dir = \"\"\n",
        "\n",
        "# Initialize the dataset and data loader\n",
        "dataset = BaseDataset(X_train, Y_train)\n",
        "data_loader = dataset.get_dataloader()\n",
        "\n",
        "# Initialize the model\n",
        "model = CADParserModel().to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=initial_learning_rate)\n",
        "\n",
        "# Gradual warmup and learning rate decay\n",
        "scheduler = LambdaLR(\n",
        "    optimizer,\n",
        "    lr_lambda=lambda epoch: 0.9**(epoch // 30) * min((epoch + 1) / warmup_epochs, 1)\n",
        ")\n",
        "\n",
        "# Function to find the latest checkpoint file\n",
        "def find_latest_checkpoint(checkpoint_dir):\n",
        "    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth')]\n",
        "    if checkpoint_files:\n",
        "        latest_file = max(checkpoint_files, key=lambda x: int(x.strip('model_epoch_').strip('.pth')))\n",
        "        return os.path.join(checkpoint_dir, latest_file)\n",
        "    return None\n",
        "start_epoch = 0\n",
        "\n",
        "# Load the latest checkpoint if it exists\n",
        "# latest_checkpoint = find_latest_checkpoint(checkpoint_dir)\n",
        "\n",
        "# if latest_checkpoint:\n",
        "#     print(f\"Loading checkpoint '{latest_checkpoint}'\")\n",
        "#     checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
        "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
        "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "#     scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "#     start_epoch = checkpoint['epoch']\n",
        "#     print(f\"Resuming training from epoch {start_epoch}\")\n",
        "\n",
        "loss_list = []"
      ],
      "metadata": {
        "id": "AW4zghe_yZrq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_all_node_counts = []\n",
        "for batch in data_loader:\n",
        "    num_nodes_batch = batch['num_nodes']\n",
        "    X_all_node_counts.append(num_nodes_batch)"
      ],
      "metadata": {
        "id": "5xqAkATEQWt5"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI9_JdCaTUpH",
        "outputId": "50334fbc-1197-41a9-f62e-d572227cc8b1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    print_out = True\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "        print(f\"type(batch['graph']) = {type(batch['graph'])}\")\n",
        "        print('batch_idx =', batch_idx)\n",
        "        graphs, sequences = batch['graph'].to(device), batch['labels'].to(device)\n",
        "        num_nodes_per_graph = batch['num_nodes']\n",
        "        # for label in sequences[4]:\n",
        "            # print(f'label = {label}')\n",
        "\n",
        "        # optimizer.zero_grad()\n",
        "        # outputs1, outputs2 = model(graphs)\n",
        "        # print(f'outputs1 = {outputs1}')\n",
        "        # print(f'outputs2 = {outputs2}')\n",
        "        # print(f'outputs1.shape = {outputs1.shape}')\n",
        "        # print(f'outputs2.shape = {outputs2.shape}')\n",
        "        # print('sequences =', sequences)\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        batch_loss = 0\n",
        "        # print('graphs.shape =', graphs)\n",
        "        # print('sequences.shape =', sequences.shape)\n",
        "        node_embeddings, graph_embeddings = model.cad_encoder(graphs) # z_l, z_g\n",
        "        decoder_input_seq = sequences[:, 0:1, :].to(device)  # Start with the first vector (START token)\n",
        "\n",
        "        # process node embeddings\n",
        "        batch_num_nodes = node_embeddings.shape[0]\n",
        "        padded_node_embeddings = torch.zeros(len(num_nodes_per_graph), 66, 64).to(device) # 66 is the max node contained by single graph\n",
        "\n",
        "        # Fill the padded_node_embeddings tensor\n",
        "        start_idx = 0\n",
        "        for i, num_nodes in enumerate(num_nodes_per_graph):\n",
        "            end_idx = start_idx + num_nodes\n",
        "            padded_node_embeddings[i, :num_nodes] = node_embeddings[start_idx:end_idx].to(device)\n",
        "            start_idx = end_idx\n",
        "        print(padded_node_embeddings.shape)\n",
        "\n",
        "        # turn graph_embeddings into 3D\n",
        "        graph_embeddings = torch.unsqueeze(graph_embeddings, 1).to(device)\n",
        "        print('graph_embeddings shape =', graph_embeddings.shape)\n",
        "\n",
        "        for t in range(1, sequences.size(1)):\n",
        "            # print(f't = {t}')\n",
        "            # print('node_embeddings =', node_embeddings.shape)\n",
        "            # print('graph_embeddings =', graph_embeddings.shape)\n",
        "            decoder_output_t_i, decoder_output_p_i = model.cad_decoder(padded_node_embeddings, graph_embeddings, decoder_input_seq).to(device)\n",
        "            gt_t = sequences[:, t, :] # ground truth seq in raw 17-len vector form\n",
        "\n",
        "            command_type_t = sequences[:, t, 0] # get command type\n",
        "            command_type_t = command_type_t.long()\n",
        "            true_t_i = F.one_hot(command_type_t, num_classes=7)\n",
        "            # print(f'true_t_i.shape = {true_t_i.shape}')\n",
        "\n",
        "            param_t = sequences[:, t, 1:]# get parameters\n",
        "            param_t_mapped = param_t + 1\n",
        "            true_p_i = F.one_hot(param_t_mapped.long(), num_classes=257)\n",
        "            # print(f'true_p_i.shape = {true_p_i.shape}')\n",
        "            # print(f'decoder_output_p_i.shape = {decoder_output_p_i.shape}')\n",
        "\n",
        "            # print(f'decoder_output_t_i.shape = {decoder_output_t_i.shape}')\n",
        "            # print(f'true_t_i.shape = {true_t_i.shape}')\n",
        "            # t_i_loss = -(true_t_i.float() * torch.log(true_t_i)).sum(dim=1).mean()\n",
        "            # print(f't_i_loss = {t_i_loss}')\n",
        "            t_i_loss = criterion(decoder_output_t_i, true_t_i.float())\n",
        "            p_i_loss = criterion(decoder_output_p_i, true_p_i.float())\n",
        "            # print(f'p_i_loss = {p_i_loss}')\n",
        "\n",
        "            loss = t_i_loss + p_i_loss # compare decoder output with the true next output\n",
        "            # print(f'decoder_output_t_i.shape = {decoder_output_t_i.shape}')\n",
        "\n",
        "            # convert softmax distribution back into valid token\n",
        "            _, command_type_pred_next = torch.max(decoder_output_t_i, dim=1, keepdim=True)\n",
        "            command_args_pred_next = torch.argmax(decoder_output_p_i, dim=2)\n",
        "            # print(f'command_args_pred_next.shape = {command_args_pred_next.shape}')\n",
        "            command_args_pred_next = command_args_pred_next - 1\n",
        "            next_token_pred = torch.cat((command_type_pred_next, command_args_pred_next), dim=1) # next_token_pred shape should be [128, 17]\n",
        "\n",
        "            next_token_pred = next_token_pred.unsqueeze(1) # reshaping next_token_pred to size [128, 1, 17]\n",
        "            decoder_input_seq = torch.cat((decoder_input_seq, next_token_pred), dim=1)\n",
        "            # print(f'decoder_input_seq.shape = {decoder_input_seq.shape}')\n",
        "            # print(f'decoder_input_seq[0] = {decoder_input_seq[0]}')\n",
        "            batch_loss += loss\n",
        "\n",
        "        if print_out:\n",
        "            print('batch_idx:', batch_idx)\n",
        "            print('pred:', decoder_input_seq[0])\n",
        "            print('true:', sequences[0])\n",
        "            print_out = False\n",
        "\n",
        "        # Normalize loss by seq length\n",
        "        batch_loss /= (sequences.size(1) - 1)\n",
        "        batch_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        total_loss += batch_loss.item()\n",
        "\n",
        "        scheduler.step()  # Update the learning rate\n",
        "        # print(f'batch_idx = {batch_idx}')\n",
        "        # print(f'epoch {epoch+1}, training batch_loss = {batch_loss}')\n",
        "\n",
        "\n",
        "\n",
        "    loss_list.append(total_loss / 49)\n",
        "    with open(loss_log_file_path, 'a') as f:\n",
        "        f.write(str(total_loss / 49))\n",
        "    print(f'----------------------EPOCH {epoch+1}, TOTAL LOSS = {total_loss / 49}')\n",
        "        # if epoch % 10 == 0:\n",
        "        #     print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(data_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Save checkpoint at the end of each epoch\n",
        "    checkpoint = {\n",
        "        'epoch': epoch + 1,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'loss': loss_list,\n",
        "    }\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        torch.save(checkpoint, f'{checkpoint_dir}/model_epoch_{epoch+1}.pth')\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a plot of the losses\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(loss_list, label='Loss per Epoch')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Save the plot to a PNG file\n",
        "plt.savefig('/content/drive/My Drive/CADParser_Checkpoint_kevin_v1/loss_plot.png')\n",
        "plt.close()  # Close the plot explicitly after saving to free up memory\n",
        "\n"
      ],
      "metadata": {
        "id": "ctYARkuhNP0B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4f9c6d1c-5665-47a2-8e4e-7e49e51a4c8c",
        "collapsed": true
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type(batch['graph']) = <class 'dgl.heterograph.DGLGraph'>\n",
            "batch_idx = 0\n",
            "hidden_rep is <class 'list'>\n",
            "node_embeddings_2.shape = torch.Size([1129, 64])\n",
            "graph_representation.shape = torch.Size([128, 128])\n",
            "torch.Size([128, 66, 64])\n",
            "graph_embeddings shape = torch.Size([128, 1, 128])\n",
            "command embedding shape = torch.Size([128, 64])\n",
            "fusion_output.shape = torch.Size([128, 1, 64])\n",
            "output.shape = torch.Size([128, 1, 64])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "shape '[128, 8, 8]' is invalid for input of size 540672",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-27ad9381b8c9>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;31m# print('node_embeddings =', node_embeddings.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# print('graph_embeddings =', graph_embeddings.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mdecoder_output_t_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_output_p_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcad_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_node_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mgt_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# ground truth seq in raw 17-len vector form\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-b1590af5ec75>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, node_embeddings, graph_embeddings, command_seq, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'fusion_output.shape = {fusion_output.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         decoder_output = self.transformer_decoder(tgt=fusion_output,\n\u001b[0m\u001b[1;32m     64\u001b[0m                                           \u001b[0mmemory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                                           \u001b[0mmemory_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-51293c75aa1e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, cache, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cache parameter should be None in training mode\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 output = mod(\n\u001b[0m\u001b[1;32m     51\u001b[0m                     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                     \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-51293c75aa1e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# In training mode, follow the standard procedure including masking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             return super().forward(\n\u001b[0m\u001b[1;32m     98\u001b[0m                 \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_is_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mha_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_is_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_mha_block\u001b[0;34m(self, x, mem, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    872\u001b[0m     def _mha_block(self, x: Tensor, mem: Tensor,\n\u001b[1;32m    873\u001b[0m                    attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: bool = False) -> Tensor:\n\u001b[0;32m--> 874\u001b[0;31m         x = self.multihead_attn(x, mem, mem,\n\u001b[0m\u001b[1;32m    875\u001b[0m                                 \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m                                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                 is_causal=is_causal)\n\u001b[1;32m   1240\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1242\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5380\u001b[0m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstatic_k\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5382\u001b[0;31m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5383\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5384\u001b[0m         \u001b[0;31m# TODO finish disentangling control flow so we don't do in-projections when statics are passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[128, 8, 8]' is invalid for input of size 540672"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter = 0\n",
        "for batch_id, batch in enumerate(data_loader):\n",
        "    iter += 1\n",
        "\n",
        "print(iter)"
      ],
      "metadata": {
        "id": "ygPASx8zrRbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "checkpoint_dir = '/content/drive/My Drive/CADParser_Checkpoint'\n",
        "\n",
        "dataset = BaseDataset(X_train, Y_train)\n",
        "data_loader = dataset.get_dataloader()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = CADParserModel().to(device)\n",
        "\n",
        "def find_latest_checkpoint(checkpoint_dir):\n",
        "    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth')]\n",
        "    if checkpoint_files:\n",
        "        latest_file = max(checkpoint_files, key=lambda x: int(x.strip('model_epoch_').strip('.pth')))\n",
        "        return os.path.join(checkpoint_dir, latest_file)\n",
        "    return None\n",
        "\n",
        "# Load the latest checkpoint if it exists\n",
        "latest_checkpoint = find_latest_checkpoint(checkpoint_dir)\n",
        "start_epoch = 0\n",
        "if latest_checkpoint:\n",
        "    print(f\"Loading checkpoint '{latest_checkpoint}'\")\n",
        "    checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    # scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    print(f\"loading epoch {start_epoch}\")"
      ],
      "metadata": {
        "id": "Y9QnFYEpQLw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_vector_total = np.empty([0, 128])"
      ],
      "metadata": {
        "id": "v19SbKagxEf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_idx, batch in enumerate(data_loader):\n",
        "    graphs, sequences = batch['graph'].to(device), batch['labels'].to(device)\n",
        "    latent_vector = model.cad_encoder(graphs)\n",
        "    latent_vector = latent_vector.cpu().detach().numpy()\n",
        "    latent_vector_total = np.concatenate((latent_vector_total, latent_vector), axis=0)"
      ],
      "metadata": {
        "id": "_wGsZPpSxCCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming latent_vector is your tensor of shape (128, 128)\n",
        " # Make sure it's a numpy array\n",
        "\n",
        "\n",
        "# Initialize t-SNE\n",
        "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
        "tsne_results = tsne.fit_transform(latent_vector_total)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(tsne_results[:, 0], tsne_results[:, 1])\n",
        "plt.colorbar()\n",
        "plt.xlabel('t-SNE feature 0')\n",
        "plt.ylabel('t-SNE feature 1')\n",
        "plt.title('t-SNE Visualization of Latent Vectors')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "i1z1fiI9sAzL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}